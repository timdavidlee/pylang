{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# import torch related libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "# import general libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import _pickle as pickle\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the local directory for the imported model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/paperspace/data/wikitext/wikitext-2/wiki.train.tokens'),\n",
       " PosixPath('/home/paperspace/data/wikitext/wikitext-2/mode117.pth'),\n",
       " PosixPath('/home/paperspace/data/wikitext/wikitext-2/wiki.valid.tokens'),\n",
       " PosixPath('/home/paperspace/data/wikitext/wikitext-2/sample.txt'),\n",
       " PosixPath('/home/paperspace/data/wikitext/wikitext-2/wiki.test.tokens')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_LM=Path(\"/home/paperspace/data/wikitext/wikitext-2\")\n",
    "list(PATH_LM.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained Word to ID mapping (from Language model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('UNK', 0), ('<eos>', 1), ('=', 2), ('Valkyria', 3), ('Chronicles', 4), ('III', 5), ('Senjō', 6), ('no', 7), ('3', 8), (':', 9)]\n",
      "['UNK', '<eos>', '=', 'Valkyria', 'Chronicles', 'III', 'Senjō', 'no', '3', ':']\n"
     ]
    }
   ],
   "source": [
    "# load pretrained dictionary mapping\n",
    "with open('dict17.pkl', 'rb') as f:\n",
    "    pretrn_word2idx, pretrn_idx2word = pickle.load(f)\n",
    "print(list(pretrn_word2idx.items())[:10])\n",
    "print(list(pretrn_idx2word)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Sub/ Obj dataset that we will leverage pretrained\n",
    "\n",
    "```\n",
    "quote.tok.gt9.5000\n",
    "plot.tok.gt9.5000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/paperspace/data/rotten_imdb/glove.6B.100d.txt'),\n",
       " PosixPath('/home/paperspace/data/rotten_imdb/quote.tok.gt9.5000'),\n",
       " PosixPath('/home/paperspace/data/rotten_imdb/glove.6B.50d.txt'),\n",
       " PosixPath('/home/paperspace/data/rotten_imdb/plot.tok.gt9.5000'),\n",
       " PosixPath('/home/paperspace/data/rotten_imdb/glove.6B.200d.txt'),\n",
       " PosixPath('/home/paperspace/data/rotten_imdb/rotten_imdb.tar.gz'),\n",
       " PosixPath('/home/paperspace/data/rotten_imdb/subjdata.README.1.0'),\n",
       " PosixPath('/home/paperspace/data/rotten_imdb/glove.6B.300d.txt'),\n",
       " PosixPath('/home/paperspace/data/rotten_imdb/rotten_imdb.tar')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "PATH = Path(\"/home/paperspace/data/rotten_imdb/\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Every dataset is lower cased except for TREC\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def read_file(path):\n",
    "    \"\"\" Read file returns a shuttled list.\"\"\"\n",
    "    with open(path, encoding = \"ISO-8859-1\") as f:\n",
    "        content = np.array(f.readlines())\n",
    "    return content\n",
    "\n",
    "def get_vocab(list_of_content):\n",
    "    \"\"\"Computes Dict of counts of words.\n",
    "    \n",
    "    Computes the number of times a word is on a document.\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(float)\n",
    "    for content in list_of_content:\n",
    "        for line in content:\n",
    "            line = clean_str(line.strip())\n",
    "            words = set(line.split())\n",
    "            for word in words:\n",
    "                vocab[word] += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Data Prep XY\n",
    "# ======================================================\n",
    "\n",
    "def make_XY():\n",
    "    \"\"\"\n",
    "    Load the subjective / objective dataset\n",
    "    \"\"\"\n",
    "    sub_content = read_file(PATH/\"quote.tok.gt9.5000\")\n",
    "    obj_content = read_file(PATH/\"plot.tok.gt9.5000\")\n",
    "    \n",
    "    # cleans the blanks and turns into numpy arrays\n",
    "    sub_content = np.array([clean_str(line.strip()) for line in sub_content])\n",
    "    obj_content = np.array([clean_str(line.strip()) for line in obj_content])\n",
    "    \n",
    "    # creates target Dataset\n",
    "    sub_y = np.zeros(len(sub_content))\n",
    "    obj_y = np.ones(len(obj_content))\n",
    "    \n",
    "    # combine into features / targets\n",
    "    X = np.append(sub_content, obj_content)\n",
    "    y = np.append(sub_y, obj_y)\n",
    "    return X,y\n",
    "\n",
    "\n",
    "def make_train_val(X,y):\n",
    "    \"\"\"\n",
    "    Train and test split\n",
    "    \"\"\"\n",
    "    X_tr, X_vl, y_tr, y_vl = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X_tr, X_vl, y_tr, y_vl\n",
    "\n",
    "\n",
    "def encode_sentence(s, word2idx, N=35):\n",
    "    \"\"\"\n",
    "    Takes in a long text and encodes it with dictionary\n",
    "    then makes vectors of sized N.\n",
    "    \"\"\"\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([word2idx.get(w, 0) for w in s.split() + [\"<eos>\"]])\n",
    "    l = min(N, len(enc1))\n",
    "    enc[:l] = enc1[:l]\n",
    "    return enc\n",
    "\n",
    "def encode_sent_array(list_of_sentences, word2idx, N=35):\n",
    "    return np.vstack([encode_sentence(sent, word2idx, N) for sent in list_of_sentences])\n",
    "\n",
    "# ======================================================\n",
    "# LM Model for reference\n",
    "# ======================================================\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Container module with an encoder, a recurrent module, and a decoder.\n",
    "    \n",
    "    ntoken: number of tokens\n",
    "    ninp: number of inputs\n",
    "    nhid: number of hidden units\n",
    "    nlayers: number of layers\n",
    "    dropout: % dropout\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        self.init_weights()\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range = 0.1\n",
    "        self.encoder.weight.data.uniform_(-init_range, init_range)\n",
    "        self.decoder.bias.data.fill_(0.0)\n",
    "        self.decoder.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\"\n",
    "        input: current input\n",
    "        hidden: hidden state from the previous step\n",
    "        \"\"\"\n",
    "        # pulls the embeddings for the input submitted\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        \n",
    "        # then applies the RNN against the embedding layer\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        \"\"\"\n",
    "        Initialize the hidden weights\n",
    "        \"\"\"\n",
    "        weight = next(self.parameters()).data\n",
    "        return Variable(weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "\n",
    "# ======================================================\n",
    "# Our RNN Classifier that imports the RNN model\n",
    "# ======================================================\n",
    "\n",
    "def load_model(m, p):\n",
    "    m.load_state_dict(torch.load(p))\n",
    "\n",
    "class NetLM(nn.Module):\n",
    "    def __init__(self, model_path, ntokens, nemb, nhid, nlayers, bsz, bidir=False):\n",
    "        super(NetLM, self).__init__()\n",
    "        \n",
    "        # if bidirectional is applied, (forward and backward)\n",
    "        # otherwise its a single forward pass\n",
    "        self.ndir = 2 if bidir else 1\n",
    "        self.nlayers = nlayers  \n",
    "        self.nemb = nemb\n",
    "        self.bsz = bsz\n",
    "        self.LM = RNNModel(ntokens, nemb, nhid, nlayers).cuda()\n",
    "        load_model(self.LM, model_path)\n",
    "            \n",
    "        self.nhid = nhid\n",
    "        \n",
    "        # freeze the RNN        \n",
    "        for param in self.LM.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.linear1 = nn.Linear(nhid*3, 100) # binary classification\n",
    "        self.linear2 = nn.Linear(100, 1)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.bn = nn.BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True)\n",
    "   \n",
    "    def forward(self, input, hidden):\n",
    "        bz = input.shape[1]\n",
    "        emb = self.LM.drop(self.LM.encoder(input))\n",
    "        output, hidden = self.LM.rnn(emb, hidden)\n",
    "        \n",
    "        # create concat pooling [ output, output average, output max]\n",
    "        out_avg = F.adaptive_avg_pool1d(output.permute(1,2,0), (1,)).view(bz,-1)\n",
    "        out_max = F.adaptive_max_pool1d(output.permute(1,2,0), (1,)).view(bz,-1)\n",
    "        out = torch.cat([output[-1], out_avg, out_max], dim=1)\n",
    "        \n",
    "        out = self.drop(F.relu(self.linear1(out)))\n",
    "        out = self.bn(out)\n",
    "        return F.sigmoid(self.linear2(out)), hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # variable of size [num_layers*num_directions, b_sz, hidden_sz]\n",
    "        return Variable(torch.zeros(self.ndir * self.nlayers, batch_size, self.nhid)).cuda()\n",
    "    \n",
    "# ====================================================================\n",
    "# Training functions\n",
    "# ====================================================================\n",
    "\n",
    "def train_epocs(model, x_train, y_train, x_test, y_test, epochs=10, lr=0.01):\n",
    "    parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    \n",
    "    # assume the data input size is the following\n",
    "    # [bptt, batch_size, embedding size]\n",
    "    # will need to rearrange some of the dimensions\n",
    "\n",
    "    b_sz = x_train.shape[0]\n",
    "    print(b_sz)\n",
    "    model.train()\n",
    "    hidden = model.init_hidden(b_sz)\n",
    "    for i in range(epochs):\n",
    "        # wrap the data in variables\n",
    "        x = Variable(torch.from_numpy(x_train)).long().cuda()\n",
    "        x = x.permute(1,0)\n",
    "        y = Variable(torch.from_numpy(y_train)).float().cuda().unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # wrap hidden states for the model\n",
    "        hidden = Variable(hidden.data)\n",
    "        \n",
    "        # pass our phrase through the model\n",
    "        # get the updated hidden state\n",
    "        y_hat, hidden = model(x, hidden)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        y_pred = y_hat > 0.5\n",
    "        correct = (y_pred.float() == y).float().sum()\n",
    "        accuracy = correct/y_pred.shape[0]\n",
    "        print('loss: %.4f, acc: %.4f' % (loss.data[0], accuracy.data[0]))\n",
    "    test_metrics(model, x_test, y_test, hidden)\n",
    "\n",
    "\n",
    "def test_metrics(m, x_test, y_test, hidden):\n",
    "    m.eval()\n",
    "    b_sz = x_test.shape[0]\n",
    "    hidden = model.init_hidden(b_sz)\n",
    "    x = Variable(torch.from_numpy(x_test)).long().cuda()\n",
    "    x = x.permute(1,0)\n",
    "    y = Variable(torch.from_numpy(y_test)).float().cuda().unsqueeze(1)\n",
    "    y_hat, hidden = m(x, hidden)\n",
    "    loss = F.binary_cross_entropy(y_hat, y)\n",
    "    y_pred = y_hat > 0.5\n",
    "    correct = (y_pred.float() == y).float().sum()\n",
    "    accuracy = correct/y_pred.shape[0]\n",
    "    print(\"test loss %.3f and accuracy %.3f\" % (loss.data[0], accuracy.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your dataset\n",
    "X,y = make_XY()\n",
    "\n",
    "# train test split\n",
    "X_tr, X_vl, y_tr, y_vl = make_train_val(X,y)\n",
    "\n",
    "# encode into numeric\n",
    "X_tr_enc = encode_sent_array(X_tr, pretrn_word2idx)\n",
    "X_vl_enc = encode_sent_array(X_vl, pretrn_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will god let her fall or give her a new path \\?\n",
      "[ 301 5011 7292  362 6357  311 2194  362   28  579 9429    0    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0]\n",
      "will god let her fall or give her a new path UNK <eos> UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n"
     ]
    }
   ],
   "source": [
    "# sample encoding / decoding\n",
    "print(X_tr[0])\n",
    "print(X_tr_enc[0])\n",
    "print(' '.join([pretrn_idx2word[idx] for idx in X_tr_enc[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup RNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = dict(nemb = 300,\n",
    "                    nhid = 300,\n",
    "                    nlayers = 2,\n",
    "                    ntokens = 33279,\n",
    "                    bsz = X_tr_enc.shape[0]\n",
    "                   )\n",
    "\n",
    "model = NetLM(PATH_LM/'mode117.pth', **model_params).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "loss: 0.7079, acc: 0.5060\n",
      "loss: 0.6099, acc: 0.6738\n",
      "loss: 0.5697, acc: 0.7213\n",
      "loss: 0.5504, acc: 0.7293\n",
      "loss: 0.5356, acc: 0.7393\n",
      "loss: 0.5253, acc: 0.7409\n",
      "loss: 0.5193, acc: 0.7433\n",
      "loss: 0.5196, acc: 0.7433\n",
      "loss: 0.5179, acc: 0.7399\n",
      "loss: 0.5119, acc: 0.7433\n",
      "loss: 0.5101, acc: 0.7454\n",
      "loss: 0.5046, acc: 0.7490\n",
      "loss: 0.5043, acc: 0.7501\n",
      "loss: 0.5026, acc: 0.7530\n",
      "loss: 0.4972, acc: 0.7565\n",
      "loss: 0.4929, acc: 0.7585\n",
      "loss: 0.4917, acc: 0.7584\n",
      "loss: 0.4877, acc: 0.7605\n",
      "loss: 0.4879, acc: 0.7630\n",
      "loss: 0.4848, acc: 0.7591\n",
      "loss: 0.4821, acc: 0.7586\n",
      "loss: 0.4801, acc: 0.7639\n",
      "loss: 0.4759, acc: 0.7675\n",
      "loss: 0.4705, acc: 0.7715\n",
      "loss: 0.4682, acc: 0.7783\n",
      "loss: 0.4616, acc: 0.7755\n",
      "loss: 0.4582, acc: 0.7798\n",
      "loss: 0.4535, acc: 0.7849\n",
      "loss: 0.4480, acc: 0.7846\n",
      "loss: 0.4446, acc: 0.7893\n",
      "test loss 0.490 and accuracy 0.765\n",
      "8000\n",
      "loss: 0.4838, acc: 0.7659\n",
      "loss: 0.4449, acc: 0.7906\n",
      "loss: 0.4392, acc: 0.7910\n",
      "loss: 0.4374, acc: 0.7966\n",
      "loss: 0.4359, acc: 0.7956\n",
      "loss: 0.4347, acc: 0.7998\n",
      "loss: 0.4359, acc: 0.7933\n",
      "loss: 0.4387, acc: 0.7944\n",
      "loss: 0.4372, acc: 0.7891\n",
      "loss: 0.4385, acc: 0.7883\n",
      "loss: 0.4365, acc: 0.7936\n",
      "loss: 0.4384, acc: 0.7950\n",
      "loss: 0.4336, acc: 0.7968\n",
      "loss: 0.4378, acc: 0.7934\n",
      "loss: 0.4338, acc: 0.7968\n",
      "loss: 0.4367, acc: 0.7971\n",
      "loss: 0.4306, acc: 0.7993\n",
      "loss: 0.4315, acc: 0.8016\n",
      "loss: 0.4304, acc: 0.7964\n",
      "loss: 0.4315, acc: 0.7979\n",
      "loss: 0.4312, acc: 0.7989\n",
      "loss: 0.4324, acc: 0.7976\n",
      "loss: 0.4342, acc: 0.7958\n",
      "loss: 0.4280, acc: 0.7979\n",
      "loss: 0.4302, acc: 0.7973\n",
      "loss: 0.4309, acc: 0.7938\n",
      "loss: 0.4315, acc: 0.7956\n",
      "loss: 0.4331, acc: 0.7978\n",
      "loss: 0.4256, acc: 0.8024\n",
      "loss: 0.4318, acc: 0.7951\n",
      "test loss 0.452 and accuracy 0.808\n",
      "8000\n",
      "loss: 0.4596, acc: 0.7836\n",
      "loss: 0.4300, acc: 0.8006\n",
      "loss: 0.4261, acc: 0.8043\n",
      "loss: 0.4234, acc: 0.8050\n",
      "loss: 0.4231, acc: 0.7999\n",
      "loss: 0.4199, acc: 0.8056\n",
      "loss: 0.4225, acc: 0.8040\n",
      "loss: 0.4277, acc: 0.7990\n",
      "loss: 0.4216, acc: 0.8086\n",
      "loss: 0.4220, acc: 0.8024\n",
      "loss: 0.4244, acc: 0.8003\n",
      "loss: 0.4243, acc: 0.8018\n",
      "loss: 0.4257, acc: 0.8001\n",
      "loss: 0.4258, acc: 0.8014\n",
      "loss: 0.4264, acc: 0.8035\n",
      "loss: 0.4250, acc: 0.8044\n",
      "loss: 0.4273, acc: 0.8008\n",
      "loss: 0.4269, acc: 0.8018\n",
      "loss: 0.4285, acc: 0.8008\n",
      "loss: 0.4257, acc: 0.8053\n",
      "loss: 0.4236, acc: 0.8021\n",
      "loss: 0.4227, acc: 0.8016\n",
      "loss: 0.4250, acc: 0.8038\n",
      "loss: 0.4264, acc: 0.8000\n",
      "loss: 0.4233, acc: 0.8033\n",
      "loss: 0.4285, acc: 0.7984\n",
      "loss: 0.4295, acc: 0.7995\n",
      "loss: 0.4315, acc: 0.8038\n",
      "loss: 0.4275, acc: 0.8005\n",
      "loss: 0.4320, acc: 0.8009\n",
      "test loss 0.446 and accuracy 0.810\n",
      "8000\n",
      "loss: 0.4552, acc: 0.7845\n",
      "loss: 0.4901, acc: 0.7614\n",
      "loss: 0.4666, acc: 0.7745\n",
      "loss: 0.4713, acc: 0.7698\n",
      "loss: 0.4546, acc: 0.7834\n",
      "loss: 0.4562, acc: 0.7816\n",
      "loss: 0.4514, acc: 0.7839\n",
      "loss: 0.4456, acc: 0.7868\n",
      "loss: 0.4430, acc: 0.7886\n",
      "loss: 0.4371, acc: 0.7938\n",
      "loss: 0.4322, acc: 0.7921\n",
      "loss: 0.4326, acc: 0.7934\n",
      "loss: 0.4380, acc: 0.7940\n",
      "loss: 0.4363, acc: 0.7973\n",
      "loss: 0.4315, acc: 0.7948\n",
      "loss: 0.4222, acc: 0.8014\n",
      "loss: 0.4188, acc: 0.8051\n",
      "loss: 0.4262, acc: 0.7978\n",
      "loss: 0.4192, acc: 0.8081\n",
      "loss: 0.4206, acc: 0.8049\n",
      "loss: 0.4146, acc: 0.8119\n",
      "loss: 0.4126, acc: 0.8059\n",
      "loss: 0.4137, acc: 0.8081\n",
      "loss: 0.4105, acc: 0.8086\n",
      "loss: 0.4083, acc: 0.8078\n",
      "loss: 0.4065, acc: 0.8088\n",
      "loss: 0.4026, acc: 0.8131\n",
      "loss: 0.4033, acc: 0.8138\n",
      "loss: 0.3966, acc: 0.8153\n",
      "loss: 0.3954, acc: 0.8189\n",
      "test loss 0.439 and accuracy 0.810\n",
      "8000\n",
      "loss: 0.4297, acc: 0.7978\n",
      "loss: 0.3974, acc: 0.8185\n",
      "loss: 0.3932, acc: 0.8213\n",
      "loss: 0.3896, acc: 0.8253\n",
      "loss: 0.3916, acc: 0.8196\n",
      "loss: 0.3889, acc: 0.8223\n",
      "loss: 0.3910, acc: 0.8230\n",
      "loss: 0.3862, acc: 0.8224\n",
      "loss: 0.3861, acc: 0.8253\n",
      "loss: 0.3869, acc: 0.8226\n",
      "loss: 0.3872, acc: 0.8211\n",
      "loss: 0.3881, acc: 0.8216\n",
      "loss: 0.3879, acc: 0.8213\n",
      "loss: 0.3792, acc: 0.8311\n",
      "loss: 0.3817, acc: 0.8240\n",
      "loss: 0.3878, acc: 0.8206\n",
      "loss: 0.3819, acc: 0.8325\n",
      "loss: 0.3800, acc: 0.8251\n",
      "loss: 0.3842, acc: 0.8253\n",
      "loss: 0.3844, acc: 0.8224\n",
      "loss: 0.3848, acc: 0.8235\n",
      "loss: 0.3881, acc: 0.8234\n",
      "loss: 0.3829, acc: 0.8261\n",
      "loss: 0.3829, acc: 0.8248\n",
      "loss: 0.3844, acc: 0.8259\n",
      "loss: 0.3834, acc: 0.8233\n",
      "loss: 0.3826, acc: 0.8249\n",
      "loss: 0.3845, acc: 0.8231\n",
      "loss: 0.3830, acc: 0.8236\n",
      "loss: 0.3801, acc: 0.8249\n",
      "test loss 0.436 and accuracy 0.800\n",
      "8000\n",
      "loss: 0.4218, acc: 0.8028\n",
      "loss: 0.3962, acc: 0.8169\n",
      "loss: 0.3879, acc: 0.8205\n",
      "loss: 0.3834, acc: 0.8240\n",
      "loss: 0.3858, acc: 0.8204\n",
      "loss: 0.3792, acc: 0.8251\n",
      "loss: 0.3745, acc: 0.8270\n",
      "loss: 0.3754, acc: 0.8318\n",
      "loss: 0.3743, acc: 0.8276\n",
      "loss: 0.3762, acc: 0.8295\n",
      "loss: 0.3794, acc: 0.8295\n",
      "loss: 0.3773, acc: 0.8273\n",
      "loss: 0.3803, acc: 0.8281\n",
      "loss: 0.3803, acc: 0.8235\n",
      "loss: 0.3778, acc: 0.8309\n",
      "loss: 0.3760, acc: 0.8280\n",
      "loss: 0.3800, acc: 0.8233\n",
      "loss: 0.3801, acc: 0.8280\n",
      "loss: 0.3765, acc: 0.8270\n",
      "loss: 0.3824, acc: 0.8274\n",
      "loss: 0.3838, acc: 0.8229\n",
      "loss: 0.3817, acc: 0.8264\n",
      "loss: 0.3809, acc: 0.8291\n",
      "loss: 0.3798, acc: 0.8275\n",
      "loss: 0.3827, acc: 0.8271\n",
      "loss: 0.3828, acc: 0.8255\n",
      "loss: 0.3827, acc: 0.8249\n",
      "loss: 0.3844, acc: 0.8239\n",
      "loss: 0.3806, acc: 0.8284\n",
      "loss: 0.3805, acc: 0.8263\n",
      "test loss 0.439 and accuracy 0.792\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, X_tr_enc, y_tr, X_vl_enc, y_vl,  epochs=30, lr=0.01)\n",
    "train_epocs(model, X_tr_enc, y_tr, X_vl_enc, y_vl,  epochs=30, lr=0.001)\n",
    "train_epocs(model, X_tr_enc, y_tr, X_vl_enc, y_vl,  epochs=30, lr=0.0001)\n",
    "train_epocs(model, X_tr_enc, y_tr, X_vl_enc, y_vl,  epochs=30, lr=0.01)\n",
    "train_epocs(model, X_tr_enc, y_tr, X_vl_enc, y_vl,  epochs=30, lr=0.001)\n",
    "train_epocs(model, X_tr_enc, y_tr, X_vl_enc, y_vl,  epochs=30, lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast AI notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class LanguageModelData\n",
    "    def get_model(self, opt_fn, emb_sz, n_hid, n_layers, **kwargs):\n",
    "            \"\"\" Method returns a RNN_Learner object, that wraps an instance of the RNN_Encoder module.\n",
    "\n",
    "            Args:\n",
    "                opt_fn (Optimizer): the torch optimizer function to use\n",
    "                emb_sz (int): embedding size\n",
    "                n_hid (int): number of hidden inputs\n",
    "                n_layers (int): number of hidden layers\n",
    "                kwargs: other arguments\n",
    "\n",
    "            Returns:\n",
    "                An instance of the RNN_Learner class.\n",
    "\n",
    "            \"\"\"\n",
    "            m = get_language_model(self.nt, emb_sz, n_hid, n_layers, self.pad_idx, **kwargs)\n",
    "            model = SingleModel(to_gpu(m))\n",
    "            return RNN_Learner(self, model, opt_fn=opt_fn)\n",
    "    \n",
    "class RNN_Learner(Learner):\n",
    "    def __init__(self, data, models, **kwargs):\n",
    "        super().__init__(data, models, **kwargs)\n",
    "\n",
    "    def _get_crit(self, data): return F.cross_entropy\n",
    "\n",
    "    def save_encoder(self, name): save_model(self.model[0], self.get_model_path(name))\n",
    "\n",
    "    def load_encoder(self, name): load_model(self.model[0], self.get_model_path(name))\n",
    "    \n",
    "\n",
    "def get_language_model(n_tok, emb_sz, nhid, nlayers, pad_token,\n",
    "                 dropout=0.4, dropouth=0.3, dropouti=0.5, dropoute=0.1, wdrop=0.5, tie_weights=True):\n",
    "    \"\"\"Returns a SequentialRNN model.\n",
    "\n",
    "    A RNN_Encoder layer is instantiated using the parameters provided.\n",
    "\n",
    "    This is followed by the creation of a LinearDecoder layer.\n",
    "\n",
    "    Also by default (i.e. tie_weights = True), the embedding matrix used in the RNN_Encoder\n",
    "    is used to  instantiate the weights for the LinearDecoder layer.\n",
    "\n",
    "    The SequentialRNN layer is the native torch's Sequential wrapper that puts the RNN_Encoder and\n",
    "    LinearDecoder layers sequentially in the model.\n",
    "\n",
    "    Args:\n",
    "        n_tok (int): number of unique vocabulary words (or tokens) in the source dataset\n",
    "        emb_sz (int): the embedding size to use to encode each token\n",
    "        nhid (int): number of hidden activation per LSTM layer\n",
    "        nlayers (int): number of LSTM layers to use in the architecture\n",
    "        pad_token (int): the int value used for padding text.\n",
    "        dropouth (float): dropout to apply to the activations going from one LSTM layer to another\n",
    "        dropouti (float): dropout to apply to the input layer.\n",
    "        dropoute (float): dropout to apply to the embedding layer.\n",
    "        wdrop (float): dropout used for a LSTM's internal (or hidden) recurrent weights.\n",
    "        tie_weights (bool): decide if the weights of the embedding matrix in the RNN encoder should be tied to the\n",
    "            weights of the LinearDecoder layer.\n",
    "    Returns:\n",
    "        A SequentialRNN model\n",
    "    \"\"\"\n",
    "\n",
    "    rnn_enc = RNN_Encoder(n_tok, emb_sz, nhid=nhid, nlayers=nlayers, pad_token=pad_token,\n",
    "                 dropouth=dropouth, dropouti=dropouti, dropoute=dropoute, wdrop=wdrop)\n",
    "    enc = rnn_enc.encoder if tie_weights else None\n",
    "    return SequentialRNN(rnn_enc, LinearDecoder(n_tok, emb_sz, dropout, tie_encoder=enc))\n",
    "\n",
    "\n",
    "class RNN_Encoder(nn.Module):\n",
    "\n",
    "    \"\"\"A custom RNN encoder network that uses\n",
    "        - an embedding matrix to encode input,\n",
    "        - a stack of LSTM layers to drive the network, and\n",
    "        - variational dropouts in the embedding and LSTM layers\n",
    "\n",
    "        The architecture for this network was inspired by the work done in\n",
    "        \"Regularizing and Optimizing LSTM Language Models\".\n",
    "        (https://arxiv.org/pdf/1708.02182.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, ntoken, emb_sz, nhid, nlayers, pad_token, bidir=False,\n",
    "                 dropouth=0.3, dropouti=0.65, dropoute=0.1, wdrop=0.5):\n",
    "        \"\"\" Default constructor for the RNN_Encoder class\n",
    "\n",
    "            Args:\n",
    "                bs (int): batch size of input data\n",
    "                ntoken (int): number of vocabulary (or tokens) in the source dataset\n",
    "                emb_sz (int): the embedding size to use to encode each token\n",
    "                nhid (int): number of hidden activation per LSTM layer\n",
    "                nlayers (int): number of LSTM layers to use in the architecture\n",
    "                pad_token (int): the int value used for padding text.\n",
    "                dropouth (float): dropout to apply to the activations going from one LSTM layer to another\n",
    "                dropouti (float): dropout to apply to the input layer.\n",
    "                dropoute (float): dropout to apply to the embedding layer.\n",
    "                wdrop (float): dropout used for a LSTM's internal (or hidden) recurrent weights.\n",
    "\n",
    "            Returns:\n",
    "                None\n",
    "          \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.ndir = 2 if bidir else 1\n",
    "        self.bs = 1\n",
    "        self.encoder = nn.Embedding(ntoken, emb_sz, padding_idx=pad_token)\n",
    "        self.encoder_with_dropout = EmbeddingDropout(self.encoder)\n",
    "        self.rnns = [nn.LSTM(emb_sz if l == 0 else nhid, (nhid if l != nlayers - 1 else emb_sz)//self.ndir,\n",
    "             1, bidirectional=bidir) for l in range(nlayers)]\n",
    "        if wdrop: self.rnns = [WeightDrop(rnn, wdrop) for rnn in self.rnns]\n",
    "        self.rnns = torch.nn.ModuleList(self.rnns)\n",
    "        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "\n",
    "        self.emb_sz,self.nhid,self.nlayers,self.dropoute = emb_sz,nhid,nlayers,dropoute\n",
    "        self.dropouti = LockedDropout(dropouti)\n",
    "        self.dropouths = nn.ModuleList([LockedDropout(dropouth) for l in range(nlayers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\" Invoked during the forward propagation of the RNN_Encoder module.\n",
    "        Args:\n",
    "            input (Tensor): input of shape (sentence length x batch_size)\n",
    "\n",
    "        Returns:\n",
    "            raw_outputs (tuple(list (Tensor), list(Tensor)): list of tensors evaluated from each RNN layer without using\n",
    "            dropouth, list of tensors evaluated from each RNN layer using dropouth,\n",
    "        \"\"\"\n",
    "        sl,bs = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        with set_grad_enabled(self.training):\n",
    "            emb = self.encoder_with_dropout(input, dropout=self.dropoute if self.training else 0)\n",
    "            emb = self.dropouti(emb)\n",
    "            raw_output = emb\n",
    "            new_hidden,raw_outputs,outputs = [],[],[]\n",
    "            for l, (rnn,drop) in enumerate(zip(self.rnns, self.dropouths)):\n",
    "                current_input = raw_output\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "                new_hidden.append(new_h)\n",
    "                raw_outputs.append(raw_output)\n",
    "                if l != self.nlayers - 1: raw_output = drop(raw_output)\n",
    "                outputs.append(raw_output)\n",
    "\n",
    "            self.hidden = repackage_var(new_hidden)\n",
    "        return raw_outputs, outputs\n",
    "\n",
    "    def one_hidden(self, l):\n",
    "        nh = (self.nhid if l != self.nlayers - 1 else self.emb_sz)//self.ndir\n",
    "        if IS_TORCH_04: return Variable(self.weights.new(self.ndir, self.bs, nh).zero_())\n",
    "        else: return Variable(self.weights.new(self.ndir, self.bs, nh).zero_(), volatile=not self.training)\n",
    "\n",
    "    def reset(self):\n",
    "        self.weights = next(self.parameters()).data\n",
    "        self.hidden = [(self.one_hidden(l), self.one_hidden(l)) for l in range(self.nlayers)]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "def get_rnn_classifer(bptt, max_seq, n_class, n_tok, emb_sz, n_hid, n_layers, pad_token, layers, drops, bidir=False,\n",
    "                      dropouth=0.3, dropouti=0.5, dropoute=0.1, wdrop=0.5):\n",
    "    rnn_enc = MultiBatchRNN(bptt, max_seq, n_tok, emb_sz, n_hid, n_layers, pad_token=pad_token, bidir=bidir,\n",
    "                      dropouth=dropouth, dropouti=dropouti, dropoute=dropoute, wdrop=wdrop)\n",
    "    return SequentialRNN(rnn_enc, PoolingLinearClassifier(layers, drops))\n",
    "\n",
    "\n",
    "class LinearBlock(nn.Module):\n",
    "    def __init__(self, ni, nf, drop):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(ni, nf)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.bn = nn.BatchNorm1d(ni)\n",
    "\n",
    "    def forward(self, x): return self.lin(self.drop(self.bn(x)))\n",
    "\n",
    "    \n",
    "class PoolingLinearClassifier(nn.Module):\n",
    "    def __init__(self, layers, drops):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            LinearBlock(layers[i], layers[i + 1], drops[i]) for i in range(len(layers) - 1)])\n",
    "\n",
    "    def pool(self, x, bs, is_max):\n",
    "        f = F.adaptive_max_pool1d if is_max else F.adaptive_avg_pool1d\n",
    "        return f(x.permute(1,2,0), (1,)).view(bs,-1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = outputs[-1]\n",
    "        sl,bs,_ = output.size()\n",
    "        avgpool = self.pool(output, bs, False)\n",
    "        mxpool = self.pool(output, bs, True)\n",
    "        x = torch.cat([output[-1], mxpool, avgpool], 1)\n",
    "        for l in self.layers:\n",
    "            l_x = l(x)\n",
    "            x = F.relu(l_x)\n",
    "        return l_x, raw_outputs, outputs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
