{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "import _pickle as pickle\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Unzip the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-05-16 22:53:46--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.100.181\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.100.181|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4475746 (4.3M) [application/zip]\n",
      "Saving to: ‘/home/paperspace/data/wikitext/wikitext-2-v1.zip’\n",
      "\n",
      "wikitext-2-v1.zip   100%[===================>]   4.27M  2.19MB/s    in 1.9s    \n",
      "\n",
      "2018-05-16 22:53:48 (2.19 MB/s) - ‘/home/paperspace/data/wikitext/wikitext-2-v1.zip’ saved [4475746/4475746]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip -P ~/data/wikitext/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /home/paperspace/data/wikitext/wikitext-2-v1.zip\n",
      "   creating: /home/paperspace/data/wikitext/wikitext-2/\n",
      "  inflating: /home/paperspace/data/wikitext/wikitext-2/wiki.test.tokens  \n",
      "  inflating: /home/paperspace/data/wikitext/wikitext-2/wiki.valid.tokens  \n",
      "  inflating: /home/paperspace/data/wikitext/wikitext-2/wiki.train.tokens  \n"
     ]
    }
   ],
   "source": [
    "!unzip /home/paperspace/data/wikitext/wikitext-2-v1.zip -d /home/paperspace/data/wikitext/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a peek at the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \r\n",
      " = Homarus gammarus = \r\n",
      " \r\n",
      " Homarus gammarus , known as the European lobster or common lobster , is a species of <unk> lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts of the Black Sea . It is closely related to the American lobster , H. americanus . It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ) , and bears a conspicuous pair of claws . In life , the lobsters are blue , only becoming \" lobster red \" on cooking . Mating occurs in the summer , producing eggs which are carried by the females for up to a year before hatching into <unk> larvae . Homarus gammarus is a highly esteemed food , and is widely caught using lobster pots , mostly around the British Isles . \r\n",
      " \r\n",
      " = = Description = = \r\n",
      " \r\n",
      " Homarus gammarus is a large <unk> , with a body length up to 60 centimetres ( 24 in ) and weighing up to 5 – 6 kilograms ( 11 – 13 lb ) , although the lobsters caught in lobster pots are usually 23 – 38 cm ( 9 – 15 in ) long and weigh 0 @.@ 7 – 2 @.@ 2 kg ( 1 @.@ 5 – 4 @.@ 9 lb ) . Like other crustaceans , lobsters have a hard <unk> which they must shed in order to grow , in a process called <unk> ( <unk> ) . This may occur several times a year for young lobsters , but decreases to once every 1 – 2 years for larger animals . \r\n",
      " The first pair of <unk> is armed with a large , asymmetrical pair of claws . The larger one is the \" <unk> \" , and has rounded <unk> used for crushing prey ; the other is the \" cutter \" , which has sharp inner edges , and is used for holding or tearing the prey . Usually , the left claw is the <unk> , and the right is the cutter . \r\n",
      " The <unk> is generally blue above , with spots that <unk> , and yellow below . The red colour associated with lobsters only appears after cooking . This occurs because , in life , the red pigment <unk> is bound to a protein complex , but the complex is broken up by the heat of cooking , releasing the red pigment . \r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "! cat /home/paperspace/data/wikitext/wikitext-2/wiki.valid.tokens | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/paperspace/data/wikitext/wikitext-2/wiki.train.tokens'),\n",
       " PosixPath('/home/paperspace/data/wikitext/wikitext-2/wiki.valid.tokens'),\n",
       " PosixPath('/home/paperspace/data/wikitext/wikitext-2/wiki.test.tokens')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH=Path(\"/home/paperspace/data/wikitext/wikitext-2\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up some helper objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# Helper Objects\n",
    "# ========================================================================\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializes both conversions of the dictionary \"\"\"\n",
    "        self.word2idx = {\"UNK\": 0}\n",
    "        self.idx2word = [\"UNK\"]\n",
    "\n",
    "    def add_word(self, word):\n",
    "        \"\"\" Adds a new word to the dictionary. gives it an auto incremented ID\"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Returns the number of tokens in the word\"\"\"\n",
    "        return len(self.idx2word)\n",
    "    \n",
    "    def to_pkl(self, filepath):\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump([self.word2idx, self.idx2word],f)\n",
    "        \n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path, ):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'wiki.train.tokens'), add=True)\n",
    "        self.valid = self.tokenize(os.path.join(path, 'wiki.valid.tokens'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'wiki.test.tokens'))\n",
    "        \n",
    "    def count_words(self, path, add=False):\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                if add:\n",
    "                    for word in words:\n",
    "                        self.dictionary.add_word(word)\n",
    "        return tokens\n",
    "\n",
    "    def tokenize(self, path, add=False):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        tokens = self.count_words(path, add)\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r') as f:\n",
    "            ids = torch.LongTensor(tokens)\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx.get(word, 0)\n",
    "                    token += 1\n",
    "        return ids\n",
    "\n",
    "# ========================================================================\n",
    "# Helper functions\n",
    "# ========================================================================\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    \"\"\"\n",
    "    Takes any dataset and returns a sub-batched version\n",
    "    [1,2,3,4,5,6,7,8,9,10], batchsize = 3 will give\n",
    "    (1,2,3) (4,5,6) (7,8,9)\n",
    "    \"\"\"\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    \n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    \n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.cuda()\n",
    "\n",
    "def get_batch(source, i, bptt, evaluation=False):\n",
    "    \"\"\"\n",
    "    Source: data array\n",
    "    i: index\n",
    "    bptt: back prop through time\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = Variable(source[i:i+seq_len], volatile=evaluation)\n",
    "    target = Variable(source[i+1:i+1+seq_len].view(-1))\n",
    "    return data, target\n",
    "\n",
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "\n",
    "def load_model(m, p): m.load_state_dict(torch.load(p))\n",
    "\n",
    "def LR_range_finder(model, train_data, criterion, lr_low=1e-3, lr_high=10, epochs=2):\n",
    "    losses = []\n",
    "    (train_data.size(0) - 1)//bptt + 1\n",
    "    iterations = epochs * ((train_data.size(0) - 1)//bptt + 1)\n",
    "    delta = (lr_high - lr_low)/(iterations-1)\n",
    "    losses = []\n",
    "    lrs = [lr_low + i*delta for i in range(iterations)]\n",
    "    model.train()\n",
    "    ind = 0\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    for i in range(epochs):\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "            lr = lrs[ind]\n",
    "            data, targets = get_batch(train_data, i, bptt)\n",
    "        \n",
    "            hidden = Variable(hidden.data) #.detach()\n",
    "            model.zero_grad()\n",
    "            output, hidden = model(data, hidden)\n",
    "            loss = criterion(output.view(-1, ntokens), targets)\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "            losses.append(loss.data[0])\n",
    "            ind += 1\n",
    "    return lrs, losses \n",
    "\n",
    "def get_triangular_lr2(lr_low, lr_high, iterations):\n",
    "    iter1 = int(0.35*iterations)\n",
    "    iter2 = int(0.85*iter1)\n",
    "    iter3 = iterations - iter1 - iter2\n",
    "    delta1 = (lr_high - lr_low)/iter1\n",
    "    delta2 = (lr_high - lr_low)/(iter1 -1)\n",
    "    lrs1 = [lr_low + i*delta1 for i in range(iter1)]\n",
    "    lrs2 = [lr_high - i*(delta1) for i in range(0, iter2)]\n",
    "    delta2 = (lrs2[-1] - lr_low)/(iter3)\n",
    "    lrs3 = [lrs2[-1] - i*(delta2) for i in range(1, iter3+1)]\n",
    "    return lrs1+lrs2+lrs3\n",
    "\n",
    "\n",
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for i in range(0, data_source.size(0) - 1, bptt):\n",
    "        data, targets = get_batch(data_source, i, bptt, evaluation=True)\n",
    "        output, hidden = model(data, hidden)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += len(data) * criterion(output_flat, targets).data\n",
    "        hidden = Variable(hidden.data) #.detach()\n",
    "    return total_loss[0] / len(data_source)\n",
    "\n",
    "\n",
    "# ========================================================================\n",
    "# Model functions\n",
    "# ========================================================================\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Container module with an encoder, a recurrent module, and a decoder.\n",
    "    \n",
    "    ntoken: number of tokens\n",
    "    ninp: number of inputs\n",
    "    nhid: number of hidden units\n",
    "    nlayers: number of layers\n",
    "    dropout: % dropout\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        self.init_weights()\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range = 0.1\n",
    "        self.encoder.weight.data.uniform_(-init_range, init_range)\n",
    "        self.decoder.bias.data.fill_(0.0)\n",
    "        self.decoder.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\"\n",
    "        input: current input\n",
    "        hidden: hidden state from the previous step\n",
    "        \"\"\"\n",
    "        # pulls the embeddings for the input submitted\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        \n",
    "        # then applies the RNN against the embedding layer\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        \"\"\"\n",
    "        Initialize the hidden weights\n",
    "        \"\"\"\n",
    "        weight = next(self.parameters()).data\n",
    "        return Variable(weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "    \n",
    "\n",
    "\n",
    "def train_triangular_policy(model, epochs=4, lr_low=1e-4, lr_high=4):\n",
    "    # Turn on training mode which enables dropout.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    iterations = epochs * ((train_data.size(0) - 1)//bptt + 1)\n",
    "    lrs = get_triangular_lr2(lr_low, lr_high, iterations)\n",
    "    idx = 0\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # initialize the model\n",
    "        # these are the weights from hidden layers\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        \n",
    "        for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "            lr = lrs[idx]\n",
    "            \n",
    "            # get a single batch of data (iterator)\n",
    "            data, targets = get_batch(train_data, i, bptt)\n",
    "            \n",
    "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "            # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "            hidden = Variable(hidden.data) #.detach()\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # run the data through the model\n",
    "            # this would be a small snippet of the phrase\n",
    "            output, hidden = model(data, hidden)\n",
    "            loss = criterion(output.view(-1, ntokens), targets)\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "            total_loss += len(data)*loss.data\n",
    "            idx += 1\n",
    "        # results after each epoch\n",
    "        val_loss = evaluate(val_data)\n",
    "        elapsed = time.time() - start_time\n",
    "        train_loss = total_loss[0]/len(train_data)\n",
    "        print('| epoch {:3d} | lr {:02.5f} | t_loss {:5.2f} | t_ppl {:5.2f} | v_loss {:5.2f} | v_ppl {:5.2f}'.format(\n",
    "             epoch, lr, train_loss, math.exp(train_loss), val_loss, math.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33279"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = Corpus(PATH)\n",
    "len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2319\n",
       "  137\n",
       "   28\n",
       " 5648\n",
       "    1\n",
       "[torch.LongTensor of size 5]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample tokenization using a corpus\n",
    "with open(PATH/'sample.txt','w') as f:\n",
    "    f.write('man met a dog')\n",
    "corpus.tokenize(PATH/'sample.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, batch_size)\n",
    "test_data = batchify(corpus.test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "nemb = 300\n",
    "nhid = 300\n",
    "bptt = 35\n",
    "clip = 0.25\n",
    "nlayers = 2\n",
    "ntokens = len(corpus.dictionary)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = RNNModel(ntokens, nemb, nhid, nlayers).cuda()\n",
    "lrs, losses = LR_range_finder(model, train_data, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xd4VGX2B/DvSSeBBFIIQoDQe5PQBURApbigK7ui7sq6Cro2cNcVV1QUVH42VBZdu66LyAqKKyhVqtJCDyWEFjqEGgikv78/pmR6ZjLlzp18P8+zD5k7d+aerHDmnXPf97yilAIREelfmNYBEBGRbzChExGFCCZ0IqIQwYRORBQimNCJiEIEEzoRUYhgQiciChFM6EREIYIJnYgoREQE8mLJyckqPT09kJckItK9zZs3n1VKpVR2XkATenp6OjIzMwN5SSIi3RORXHfOY8mFiChEMKETEYUIJnQiohDBhE5EFCKY0ImIQgQTOhFRiGBCJyIKEbpI6D/vPY0RM38Bt8sjInIu6BO6Ugr3f56J7UcvoskzP6K0rFzrkIiIglLQJ3QRwY7JN5sfL9hxUsNoiIiCV9AndACIj4nEwVeGIjxMMH7ONpSXs/RCRGRLFwkdAMLCBPf2aAQA2HH8ksbREBEFH90kdAAYldEQADBy5i8aR0JEFHx0ldDb1Y/XOgQioqClq4QuIuafi0s524WIyJKuErql7FOXtQ6BiCio6C6hPzGwBQBgdU6expEQEQUX3SX0e3oaZrrsOZmvcSRERMFFdwk9MTYKABcYERHZ0l1CjwjXXchERAHB7EhEFCIqTegi8qmInBGRLItjiSKyVERyjH/W8W+Y1urFxwTyckREuuDOCP1zALfaHJsIYLlSqgWA5cbHAdOtSSIAsJ0uEZGFShO6Umo1gPM2h0cA+ML48xcARvo4LpfaG1eMXi0uC+RliYiCWlVr6KlKqZMAYPyzrrMTRWSsiGSKSGZenm/mjifUiAQAXLpW4pP3IyIKBX6/KaqU+lAplaGUykhJSfHJezKhExHZq2pCPy0i1wGA8c8zvgupckzoRET2qprQ/wfgPuPP9wH43jfhuCfemNDzmdCJiMzcmbY4G8A6AK1E5JiI/BnANACDRSQHwGDj44DhCJ2IyF5EZScopUY7eWqgj2Nxm2mEPmXBbvOmF0RE1Z0uV4rWijZ8DuUXlmocCRFR8NBlQg8Lk8pPIiKqZnSZ0E3S6tTQOgQioqBRaQ09WHVtXAcxkbr+PCIi8indZsSo8DDuK0pEZEG/CT2CCZ2IyJJuE3pkeBiKmNCJiMx0m9CjI8JQXMaETkRkotuEzpILEZE1/Sb08DCUcIRORGSm24See74Ap/OLtA6DiCho6DahZx3PBwCUlXMbOiIiQMcJ/ZEBzQEARaXcho6ICNBxQl+86xQAYM6moxpHQkQUHHSb0E/nFwIAdp/I1zgSIqLgoNuEHhsVDgAoKGYLXSIiQMcJPc7YE/1KEWvoRESAjhP6Dc2TAQCNEtlCl4gI0HFCH929EQCgU1ptjSMhIgoOuk3o4cZdi8oV56ETEQE6TugRxoReyoVFREQAdJzQTSN0rhQlIjLQbUKPCDOEXlrGhE5EBOg4oYeHG0boLy3YrXEkRETBQbcJ3VRDJyIiA90m9HAmdCIiK/pN6MKETkRkSbcJPYwjdCIiK7pN6EREZM2rhC4iT4hIlojsEpHxvgqKiIg8V+WELiLtATwIoDuATgCGi0gLXwVGRESe8WaE3gbAeqXUVaVUKYBVAG73TVjuiTP2RCciIu8SehaAfiKSJCKxAIYCaGh7koiMFZFMEcnMy8vz4nL27r+hCQBAsUEXEVHVE7pSag+A/wOwFMAiANsB2G0fpJT6UCmVoZTKSElJqXKgjpwrKAYA5Bdy1yIiIq9uiiqlPlFKXa+U6gfgPIAc34Tlnq82HAEALNxxMpCXJSIKSt7Ocqlr/LMRgDsAzPZFUO4a3DYVAJBWh7sWERF5Ow99nojsBvADgEeUUhd8EJPbfpdhKNnXiY0K5GWJiIJShDcvVkr19VUgVRFu/Dgq401RIiJ9rxQNE25yQURkouuEbtrkggmdiEjnCV3BkMhzzxVoHAkRkfZ0ndC35F4EADw1dwdKyso1joaISFu6TugR4RUtdK8WlWkYCRGR9nSd0CPD2ROdiMhE5wld1+ETEfmUrjNiVISuwyci8ildZ8S+zS2afbH6QkTVnK4TesPEih4upy4VahgJEZH2dJ3QRSqG5a8vztYwEiIi7ek6oVtatue01iEQEWkqZBI6EVF1x4RORBQimNCJiEJESCX0Vft8uwk1EZGehFRCv+/TjVqHQESkmZBK6ERE1VnIJfTCEnZdJKLqKeQSeuvnFmkdAhGRJkIuoRMRVVe6T+izHuihdQhEREFB9wm9fYMEu2OsoxNRdaT7hJ5QI9LuWOvnFkEppUE0RETa0X1Cd+aD1Qe1DoGIKKBCNqGvzD6jdQhERAEVsgl9/cHzLLsQUbUSEgn9hubJDo9/uT43wJEQEWknJBJ6WJjjDUV3HLsU4EiIiLTjVUIXkQkisktEskRktojE+CowTzjJ55i7+VhgAyEi0lCVE7qINADwOIAMpVR7AOEA7vJVYJ4IEycZHUD6xIXYfSI/gNEQEWnD25JLBIAaIhIBIBbACe9D8pyzEbrJol2nAhMIEZGGqpzQlVLHAbwB4AiAkwAuKaWW+CowT4iLEToAvLs8B2tyuPkFEYU2b0oudQCMANAEQH0AcSJyr4PzxopIpohk5uX5J6lOGdG+0nMmzNnul2sTEQULb0ougwAcUkrlKaVKAHwLoLftSUqpD5VSGUqpjJSUFC8u51y9hBh85WGTroe+3Iwh76zxSzxERFrwJqEfAdBTRGLFUPMYCGCPb8LyXO/mydg3dYiLM6wXGS3adQp7TvJmKRGFDm9q6BsAzAWwBcBO43t96KO4qiS8krujxaXlyDrOuelEFJoivHmxUuoFAC/4KBavucrnZ68UY8qC3fhyfS5mjO4SuKCIiAIkJFaKmogIasfat9M12XDoHADg0NmCQIVERBQwIZXQAWDew3b3Zc32nb4CAHhr6b5AhUNEFDAhl9CbpdT06PxrxWWYMGcbTucX+ikiIqLACLmE7qm1+8/iu63H8ex3WeZj6w6cw/4zlzWMiojIc17dFA0FppF5YUkZikrLMHPFAby7PAcAcHjaMC1DIyLySLVP6JPmG0bma/efRatJizx+fWlZOQ6dLUCL1Fq+Do2IyCPVvuTirdeXZGPw9NU4mHdF61CIqJpjQnfhlR8rX/i6JfcCACDvcpG/wyEicimkE3rLVM9mvNj6cPVBfLL2ENInLkTOacc3SQWG1UzcvZSItBbSCX3JhP54+/edvXqPKQt2AwDu/Nc6LN51CuXlNqm7kl7sRESBEpIJfcFjN+CpW1oBAEZ2aeCT97x0rQTjvtyMez7e4JP3IyLytZCc5dK+QQLaN0jwy3uvO3jO/PPWIxew8dB5AIBizYWINBaSI3RbfVsk+/T9Js3fifSJC3H7e7+aj43+aD2ms6UAEWmoWiT0pLgon77ff9YfcXj8HeOCJEsrss/g4zUHfXp9IiJHQrLkYiuskj1HfelMfiHqxseYH//ps00AgLt7NEJsVLX4v5uINFItRuhDO1wXsGt1f2U5bpuxFmevWM9Lb/v8Yry5JNvla68UlaK4tNyf4RFRCKsWCX1A67oBvd7O45eQMXWZecqjyYyf9+PStRIMeWcN0icuRPrEhVbPt39hMf74KWfREFHVVIuEHh4mmPdwLwBAxzT/zH5x5JO1h+yOdXpxidVeprM25Fo9v/7geb/HRUShqVokdACIjzHsZNQyyJpoPftdFpRSePUnzfbXJqIQUW0SeovUWvji/u6YMqK91qHYOV9QjA9WVcyEyT51GXmXi/DIrC0oKCrVMDIi0pNqk9ABoH/LFNSICjc/rlsrGgDQr2WKViEBAGy7Cfz1m22YvmwfFu48iW+3HtcmKCLSnWqV0E2GdTTMevn+0T74dEwGwixmNX42plvA49l/xrr1btbxfFy6VgKArWKIyH2iArhmPSMjQ2VmZgbses6UlyuUKYXIcMPn2Z8+24gV2XkAKnYpmrUh12pbOi29dmdHjOqaBqnifPoHvtiEZXvOcAcmIp0Skc1KqYzKzquWI/SwMDEnc6Bi4dFrv+1oPnZPj8aoERlu91ot/H3uDnPPmKpYtueMD6MhomBVLRO6rTBjzSUhNtLq+MM3NtMiHIfyC0ux+0R+5ScSUbXFhI6KOrVt+SmYOig++O9MDH13DfbZbLRRWFKGczarUrcdvYjhM9Zgya5TgQyRiDTGhI6KkkswJXBntuRewFvGro7bj17ELW+vRtepywAYWgesP3gOD/47E1nH8zH2y81Wr523+VjA4yWiwGG3KABhxo+1MtsRehBuLDfx250AgKEd6mHEzF/Mx8vLFZ76Zjt+yrIeld//+Sbzz3/9Zjt+2zUtMIESUcBxhA6YZ4/YzgcP5hH7kXNXrR7PXLEfh84W2J33817rG6Irs13fIL14tRjpExfi642OWwQ7c+5KkV1DMiIKrCondBFpJSLbLP6XLyLjfRlcoPRskggAaJIUZ3U8iPM5XvzBuvFXZu4F7D3leCNrS4uyXNfVpywwtCD4j02Pmcp0nboMGcbSDxFpo8oJXSmVrZTqrJTqDKArgKsAvvNZZAF0b8/GWPv0AHSwadxluknat0Uy1j8zEK/d2dHRy63c1qm+X2K0dfziNavHq/blufW6yqayz9tiqLNnHc+HUgrPf5+FrOOXqhQjEQWWr0ouAwEcUEp5NqwLEiKCtDqxdsebphhG7KMyGqJeQgx+l9HQ6vlb29XDy7db94Z5Y1RHLJnQz3/Bemn2xqMoL1dYmX0GBUWluFrsvFfMxasl+Pe6XAyfsRbztx7Hqz/tsZsJBIClFqIg4auboncBmO2j9woaIzs3QHpSHDo3rG33XOakQUiKi8Ium7nh0RHhQdfR0daCnSfx+Oyt5sfJNaMwvGN9/LFXY6vzLHd6Gj9nGwBgVNeGaF63ptV5Z/KZ0ImCgdcjdBGJAvAbAN84eX6siGSKSGZenntlgWAhIujSqI7DJffJNaMhImjfIAH/urcrWqbWxOC2qebnvx7bM5CheuS/m45aPT57pRif/3oYN725qtLX/mXWZpfPP/nfbVh34JxX8RFR1fii5DIEwBal1GlHTyqlPlRKZSilMlJStO1q6C+3tq+HJRP646M/VrRa6Nk0yeVr5miY8NfuP+vWebbTOAFg32lDI7Hi0nKH5ZdvtxzHPR+v9y5AIqoSXyT00QjBcosrGY3reHT+sif7Y+fkm62OWbbxDVYfrj7o8Hh+YQlaTvoJTZ750eHzln1yiChwvKqhi0gsgMEAxvkmnOC3/fmbERPlXsL69i+9AcCu5qwX/1p1wOHxyd/vMv9suy8qABSVluPStRIk1Ii0e46I/MeroZRS6qpSKkkpVW3mtSXERiI6wr3R9fWN6uD6Rvaj+cXjg3cWjDvc2XSj04tLsCbH+T2T3HMFDks2rizccRKzPVzwRFSd8LuxBlrVq4XEuCirYzsn34y9U27VKCL/+MMnGx0ezz51Gf1fX4kPLEo6W49cwM5jrscFj3y1Bc98u9O8+Ud1kF9Ygt/8c63dJihEjjChayStTiwe6l/RnrdWTCRigqT/uj/9uv8sRhp70Ez7aS+uGPdMvf29X3HbP9eazyssKcM7y3Kw/qD9jJl8i4S+5cgFj0f6erJi7xnsOHYJ7yzP0ToU0gEmdA21TLWvrd/arh7euauzBtH4R/rEhVZ19jGfb8K1kjLz49VOVri2fm4Rpi/bh7s+tJ8xc7W4DNOX7sPPe0/jjvd+xee/HvZ53MEmlD+0yHfYbTHI/OsPXQEAT3y9zep4fEwE8gudr+oMdleKSlEzOsKuQc6Gg+dQLyHG/LikrLzSWTLvLN+HH3eeQvsG8QBg1yM+lFR120GqnjhCD1KmZGWi15kyJs8Y2/4Wl5VbHf9iXS7ueO9X8+PzBcUY9Jb1Aqf0iQvx73WHzY9/3GloMFZYYniv2RuPmjtNlpcrHD1v3YmSqLpgQg9S34zrjS3PDTY//uS+bhpG473ccwVuNfnq8cpyhzcAn7eYKmlSblGG2HXiEpRSGD5jLfq+tsI8ar90tQQXCooBAB+uPoDWz/1U1V/BI4fPFuByYfW5eUvBgSWXIHB7lwZ2x2pEhaNGVDgWje+L6xJq6H5O945jlzB8xtrKT/SAZVlZKWD+tuPYfdLQW+f4hWtomVoLnV5aAgB4sG8TfLTmEADDjJouxumkhSVluFJUiuSa0VWKobi0HC/8bxf2n7mMrx7saS4X3fjGSrRKrYXFPmrUxgo6uYMj9CDXul687pO5v1hu6HG5sNS6SZhN6dmUzAHDjBrTa+/+aL25j3uHyYtx5/uG8k95uUJxqXV5yJGery7H7I1HsOnwBZy4eA0zV+xHgXHmTrYPavusoJMnmNADaM3fB+CXiTdV+fXfPNTLh9GEln98txOv/rTX/PhPn21yuIOTyQjjFMktRy6aj10uLEVm7gUAwGNfb0XLSYbyTFFpmXnD7d0n8jHfuLCquLQc543lHABYuPMkXl+cjdcXZ5uPlZRV/qFA5CssuQRQw0T7nuue6JaeiGeHtsHLP+6xOv70ra1xd/dG5vICGQx4Y6XT51zNGMo5fRkLd5wEAPzz5xzM+Hk/ikrLMf33nTBhznYAQHpynN2US9NNWtMIHQDKyhX8ubzgTH4hFuw4iftvaOK/i5BucIQeAga2qYuEWMdlmS6NKnq5J9msTiXHTluUbt5Ysg9FxtKLKZkDwMiZv+Ctpfscvr7QSakm5/RlfLT6IEqrMGpfuOOkw9H+uP9sxksLdrv8NkLVBxO6hga3TUWPJomYMKil268Z0qEeoiLCMLzjdejZ1LAXam0XNfYv/9wD0+7oAADISPesS2So+8ii9YBlI7LXFu91dHql3jWu5vxh+wnzMcsbt4Onr8bLP+7B377Zbp55UxnLaei2G34Dhlk8AHDL26txmEm92mNC11CtmEjMGdcLjZLcL8Wk1YnFvqlD8M+7r8e/7++BZU/2R934GIfnzhjdBTWjIxBvTPjCW2xWLEtX0yzq7zsq6SnjCeVgfsr8bSfQZcpSAIY6/IzlOSi0WD0LAC98n2W3Uci4L51vLlJcWo73VzrujulIWbnCteKyyk904n/bTyB94kKcvVKEYxeu4u1l+7iaNQgwoetYVESY1YKjHx/vi4WP32B+bLthNRcdBp4pxzlLdl9tyMWbS/fhg1UHsTn3gnmu/hfrcjH6o/X4cedJp++dd7kIly3q9aYPj+fmZzlsazxlwW6M/9qw9eCk+Vlo8/wij5PwOeP+saYtDA+cuYKH/rMZby/LwYE8fkPQGhN6CGlbPx7t6ifYHXf1b7ZpcpwfI6Kn5+3AxavF+GGHfWIuL1e4ahyZXyspw2/f/9Vurr5pVawj3V5ehrzLFfV+pYDNuRfw5XrDXu1HzlWsmP11/1l8svYQ5m8zlINMbYhNfzfKy1WlXSzXHTiHrlOXWX1YLNp1ClnH843vxRG61pjQq4FezZJQKzoC4yy6O5rMf7QP5tpMh+zauA42PTsoUOGFtAU7TmLkzF+sNuU2Oeekjr7ZOHXSUwrAb9+vaKPQ7/UV5tk6d3+8wXz8fxY1/gXGbwBvL89BpxeXmEfgjpgWbVn67JfDVtcnbTGhh6DNkwZh+wsVW94lxkVh54u3oHPDihkvPzx6A779S2/Ex0SiQZ0aVq/v0rA2UmpF25VoHnLwgZAab1hhaTmbhqwdPue4t0y3l5eZfz52oeIcy6Tsibmbj9kde+SrLXbHLD9cnpm3AwDwkzGx237I7D6Rj4FvrkR+YQmiI1yni+Ez1lZpBg/5Duehh6AkF8vYX72jA9KT4tAhraI0Uy8+Bg/f2AwXrxZj9saj5uONE2Nx+NxVREeEmafuAcDfb22FhBqRSE+KQ8+mSThXUIS6tWIc1m3JPQsclGQcWZl9BkoZ5sH7QkFxmdXOUjdPX43D04aZH7+1NBsH8gqwMjsPUZUkdNNCK2c36YNdebnCd1uPY0Tn+ojQ6b64TOjVzOjujeyOiQievrW1eRqf6avznHG9sP3oRTRNqYlfD5zFiYuFhucVcE+PxubX162lz3/AwcDTsvOYzzb5PIb5W09YPV6+5zQy0hNRVq6w6bCh/PP47K0Y1TWtyte4UlSKqPAwREWEYfvRi1i7/yweGdDc7devyD6DH7afwFu/899eAbM2HsFz87Nw9kqRw/KkHujzY4j8wrbEkhofg5vb1UPzujXxx17plc6SGdqhnv+CC1GWc9b9pbzc9afGvC3HkGPR4fLPX2Ti0a+2IGPqUqsbpd84KOm4q/0LizH6I8NmJSNm/oLXF2fjQkEx1uTkudWF80+fbcK3Wyr2sv12yzE0eWYhikpdT708nV/odhnouflZAJzf29ADJnSyU9XJCu/d09W3gVQDe0/5f3OOUR+s8/g1h88VoJLPAceMH/pbjlzA6fxCq6c2517AyuyKxVFPzd2OP3yyEcNnrMV3W4/hpR924+Sla25dZtpPe6EUcPGq85k5l66VoMcry/HiD7vtnissKXN6A1gphUtXSyr9IAxGTOhkZrppalqBamtQm7oAgH4tUtx6v6wXb8GBV4ZWKZa6tarWzpbsVXXWTFX87ZsdKCgqxR3v/Yp+r63AC99bz4m3LBkt21OR3CfM2Y5PfzmEEf/8xa3rmHKtqy+Npn70P+89g+LScqv+9KP+tQ5dpy5z+LpzBcXo9NISvL3McWsHT5y8dA3Pzc8K2M1iJnQyy0hPxPYXbsbN7RyXTro2TsThacOsbqhWJjzMvdVMa/4+wOrxs8PauH0N8r2j590bKdtavS/PvOq2qLQcX6zL9ej1Z4zz6ts8twjDZ6xBQVEp5jks9Rgy+tkrjssjby3dh4+NLZMLikvxwL8z0WFyRfO6ncYyT7vnF9m91lTa+THrFHJOXzZvZF4VT8/biS/X52Kdg83O/YEJnax423v9iYEtqvS6homx6N0syfy4S0PHfWd6Nk1Eg9o1HD5HwcG0sKmqikvLca2kDFnH8zH4rVX46zcVTdFW78vD3R+tNyfy4TPWWC2gMnl3eY558/CLV0vMnTHfWZZjdV6Bi/YH+89cweDpq3H/Z5vw1YYjbt3vsF2cZSrbBGrNFRM6+dSEwe43GrP16ZiKbfYaJcXi8LRhGNw21Xzs8LRh+HpsL6ut5yj0mPrQA8CJS9Z1+D9+uhG/WvS4KVeGBVQH8ipu6m4/ehHOTF+2Dz/vPW117Jlvd2DLEedlqY2Hz+Mf3+3EYw4Wh1n6fttxdHpxidVN3kC322BCJ5+LizI0AHe2FPyxmxxPV4tx0Dj89Ts74qH+zbDibzeajzGfk62Bb67Ckl2nsGpfHkbMdF2Hv//zTKvHszcetdqo3FM5py/ju63HsCbnLADHK2oD9VeWCZ18TioZlngyaKkdG4WJQ1qjicVCmpoxhuUTy57sX5XwKESN/XIzZnlZ7qmMUgo7jxk2JL94tRizNuRi8PTVmDBnO0y3i7ScHcOFRaQ7n43phkVZp6w6TRIBwJLdpys/yQur9uVhzGebEBcVbld/X5FtqNM7yudZxy+hf0v3Zod5gyN08rvJt7W1ehzv4sbr7Ad74qcn+rp8v4aJsXiwX1OrY+MHVX4ztl39+ErPceW54W0rP4lC2tELhtk/jm6mmjpfljmoCb6+ODsgI3evErqI1BaRuSKyV0T2iAh3MSYz01/fMX2aYGRnQ2/2TmkJuK93uvmct3/fGe+O7mJ+3KtZEtpc53nibVjHfpOQB2z22XxpRHu7cyZ5MD2yVWotj+Oi0GJaTerK1AW7rfaVNSkNQEL3tuTyDoBFSqk7RSQKgHe7IFNIcFUjH9MnHZHhYRjVNQ3lChjZpYFX15o4pDW+33bC4cyXScPb4uiFq1i8y/A1vGa0/V/3aCc7OF+XEIMwERy/WDEfu5fFtEoiZ4pKy/Heyv24u0dj841SwLBLlL9VeYQuIvEA+gH4BACUUsVKKefzhajamDKyPZJrRiHWIll2bWyYV944yXBz8/VRnfDm7zp5fa2H+jfDT0/0NX8baGFTV393dBfc3aMRHr+pOVqmul9zX/Zkfyx9sp/VMXcXSRF9vOYQhr6zxupYSbn/V4t6U3JpCiAPwGcislVEPhYRbn9DGNmlATInDbZqQXpvz8ZY+bcbcX0j/2xUHWacWWO7EXZ0RDheub0Dnry5ldXsmzqxrhdQiQCxUfYj+sxJ1ht//KZT/Upr/lT9FJWWO11k5E/eJPQIANcDeF8p1QVAAYCJtieJyFgRyRSRzLy8PNunqZoQEZ/18Hbktk7XYUzvdDx9a2s3zq3v9DlTz2/ThtqmlgTxxqmSyTa95h++sVmVav5U/QSihu5NQj8G4JhSyrS31VwYErwVpdSHSqkMpVRGSor/p+1Q9RQdEY7Jv2mH2rFRLs87PG0YZljchLVbpWRq+mQczDdMjMXGfwzEmqdvsnuvfwxtbU7mjZNi0T09EYvG9610IwiqnoJ6hK6UOgXgqIi0Mh4aCMC+TyVREDKVX9o3sG40powZ3XJtVN34GKseN6b5xJ3SKrbdW/XUAPz3oV5oXS/e3JNmwiDrNgg9mybi9Ts7+u6XIF0J9hE6ADwGYJaI7ADQGcAr3odE5H/djLX25nVr4vC0YcieeqtVfVxczNX5/E/dsOJvN6JHU8ezXsb2NcyRv7en/e5Qrso9jiwa3xfP28x/581ZfQrqWS4AoJTaZiyndFRKjVRKBa7xMpEX3v59Fywa3xe1Ygwj7+iIcLv6uDMiYtWKwFbv5sk4PG2Yw71dYyLDYZuPk+Kcl4nSk+Jwv818+pypQ7D26QFOXlGhrUVt/3GbLpi7X7oFb/++M+7q1rDS9yHfCPqETqRXNaLC0bqe/c3MyvrQeMq0oAqoKNfvmzrEauMPR5ecfFtbzLz7eruGZU2S4xAWJkhzsJDK0pje6Zj3cG/z4ycHt0RyzYplrNWyAAAKYElEQVQPjtioCIzs0sCjfT3JO3oouRCFlO/+0huPDmjusxubb99VcQPW9M85IjzMpmxin9HH9GmCYR2vsztu2XUywvgeM0Z3wcy7K+YjdEpLwOTftENMpPXvsOnZQWhzXbzV8YaJFR8Mr9zewapdMflWWh3/9/Fncy4KOTNGd0G9hJgqvbZd/QS0q+/+jkwecTJA65iWgJ/3nkFMZBgKS9xffLJj8s0AKubLP/KV9fO23zZExOGc+bkP9UJ+YQluap2K9KRYLPWwwdXyv/bHwDdXefSa6shRe2hfY0KnkOPpjUd/e/+e6/HwrC3mGTS2ZozugtIyhfBwsVpda+nnv/ZH7nnrnXkcLXwCgPGDPNtkJCPdYg9Zi8+AQW1SsWyPIblPGdEODRNjrfYEBYCP/5iBZimed72sFR2By8Z+JxMGtcR0i/07a0SG44lBLXB9ozr4XRU2uK7OWHIh8rPezZIBGNoU2LqzaxrioiOQEBuJmtERCAsThDmYxdI0pSYGtKrr1vUGtHbvPEdMs3u6N0lEn+bWs3hubFUXh6cNMz9+4IYmGFSFEk1UeBh+2zXN/Di+hvUHU+3YSDzUvxm6N0nEo8Yaf4PaNcyblJNzTOhEfpYQG4nD04ZhYBvr5Jfz8hC89tvgmpdu/ixR1rNkHHl6SMWq3PYN4jFlZHuECTB1pH1XS1sv3NYWTVMMM4Vc3Yb+XUZDJNeMxpxxPfHxfd1cnOk925lAesSETqSRyPAwh6NxLbWqZ2gR/Oe+TdCjaRJucjHaj7To1bPgsb74Q8/GOPjqMNzbs7HL1wGGev4g4wdcVITz2nKjpFhkThpU6ayeyiS6mBpq8qQX++EGCyZ0IjKrHRuFw9OG4ZZ29QBUJHjLev2nYzLwRCWj2ffvvR4bnx2Ip25pZfdcP+NK2ycGtsCjA5pjVEaa3Tnusu1574xtvFNGtHN43rNDK/rjj+3XtNIPpmDDm6JE5NQTA1sgKS7Kqm/9Ta1TcVNr17Xz6Ihw1K0VjnH9miK/sAQfrDoIAFj91ADUjTcsuIqLjsDfHCR8TzRJca/h232909GqXi3c9eF6ADD34EmrUwPHLlT0vH+wX1OUKYVpP+2FAHaLwIIdR+hE5FRMZDge6Nu0yu0GIsLD8MyQilFvo6RYh9P3vnqwh12LA3e0rud6FynLFb09LVo1mOYbpcbHYPaDPXFfr8YVz1lMRvL1QjN/4widKESseupG5Jy+onUYDr0xqhPqu1gb0LtZMpqn1MRLC3a7Ve8GDM3OujZOdPhck+Q4HDpbYLUQy5IpaQsMO1FZ7kZlnl4qrm/YAsD033fChDnb3Yo3EJjQiUJE46Q4845QwebOrpXXyevGx2DaHR3cnnbZv6XhvIaJNXD0/DWr5+Y93BvHLlx19DIAgFL2XTVtCcS8cYoz0S5u6GqBJRciChp3dW+E1HjnI3nLfjQm/x1nvTd9ndhIJMZFoaNFe2NnHHXVjAwzpMWocEFYJRnSwVa2mmJCJyLd+P7RG8w/m0oj1yXUwKwHenj0Pq7y8B96NcbYfk3x0I3N0KC28/4ru168xWr17/xH+tidU8/Fh5M/MKETkW7UinFcJY7w8KatqSnZLe3r2T0XExmOfwxtg9gowyycmXdfj/0vD7E7Ly46wurbROeG9t8I7uoe2PbETOhEIa55Xc97rQSrGk563VjORnGnY2SD2jWwc/LNuL9PusvzoiPCMazjdU5n+XRLT0RiXBQ6OUjmQOD7CvGmKFEIm/9IHzRO9G6VZTCJDA/DuP5NzfPaTUz5PK1ODbx8ewenr5/1QA8cyDPMBDJtbuIOV9MXtzw32OHxZ4e2qVLjMm8woROFMEdlgFBkSrep8TFWLQls9WmejD7Nk/0Wx5d/7o4rhaXo3TwZ8RbloVvaBabPPBM6EemKaWaK5QyTeOMm3o38/G2kW3odbDrsfKfNvi1S7I5lvXgLYny0YUplmNCJSPdaptbCJ/dlWC0Q8rVlT/ZHvYQYtH9hsUevqxkduDTLhE5EIcG2PbGv6eHmMme5EJGuuCiRV3scoRORrozr3wznC4oxpne61qEEHSZ0ItKV+JhIvHpHcO30FCyY0ImIPDBnbE8cvXCt8hM1wIROROSBHk2T4FnnmMDh7QUiohDBhE5EFCKY0ImIQoRXNXQROQzgMoAyAKVKqQxfBEVERJ7zxU3RAUqpsz54HyIi8gJLLkREIcLbhK4ALBGRzSIy1hcBERFR1XhbcumjlDohInUBLBWRvUqp1ZYnGBP9WABo1KiRl5cjIiJnRPlo22oRmQzgilLqDRfn5AHIreIlkgFUt1o9f+fqgb9z6PP2922slLJvtm6jyiN0EYkDEKaUumz8+WYAL7l6jTsBubheZnWbRcPfuXrg7xz6AvX7elNySQXwnXGvvQgAXymlFvkkKiIi8liVE7pS6iCATj6MhYiIvKCnaYsfah2ABvg7Vw/8nUNfQH5fn90UJSIibelphE5ERC7oIqGLyK0iki0i+0Vkotbx+JOINBSRFSKyR0R2icgTWscUKCISLiJbRWSB1rEEgojUFpG5IrLX+N+7l9Yx+ZuITDD+vc4SkdkiEqN1TL4mIp+KyBkRybI4ligiS0Ukx/hnHX9cO+gTuoiEA5gJYAiAtgBGi0hbbaPyq1IAf1VKtQHQE8AjIf77WnoCwB6tgwigdwAsUkq1hmGCQUj/7iLSAMDjADKUUu0BhAO4S9uo/OJzALfaHJsIYLlSqgWA5cbHPhf0CR1AdwD7lVIHlVLFAL4GMELjmPxGKXVSKbXF+PNlGP6RN9A2Kv8TkTQAwwB8rHUsgSAi8QD6AfgEAJRSxUqpi9pGFRARAGqISASAWAAnNI7H54yr5c/bHB4B4Avjz18AGOmPa+shoTcAcNTi8TFUgwQHACKSDqALgA3aRhIQbwP4O4ByrQMJkKYA8gB8ZiwzfWxcoBeylFLHAbwB4AiAkwAuKaWWaBtVwKQqpU4ChkEbgLr+uIgeEro4OBbyU3NEpCaAeQDGK6XytY7Hn0RkOIAzSqnNWscSQBEArgfwvlKqC4AC+OlreLAw1o1HAGgCoD6AOBG5V9uoQoseEvoxAA0tHqchBL+mWRKRSBiS+Syl1LdaxxMAfQD8xrhhytcAbhKR/2gbkt8dA3BMKWX69jUXhgQfygYBOKSUylNKlQD4FkBvjWMKlNMich0AGP8844+L6CGhbwLQQkSaiEgUDDdR/qdxTH4jhl4KnwDYo5R6S+t4AkEp9YxSKk0plQ7Df9+flVIhPXJTSp0CcFREWhkPDQSwW8OQAuEIgJ4iEmv8ez4QIX4j2ML/ANxn/Pk+AN/74yK+2LHIr5RSpSLyKIDFMNwV/1QptUvjsPypD4A/ANgpItuMx/6hlPpRw5jIPx4DMMs4UDkI4E8ax+NXSqkNIjIXwBYYZnNtRQiuGBWR2QBuBJAsIscAvABgGoD/isifYfhgG+WXa3OlKBFRaNBDyYWIiNzAhE5EFCKY0ImIQgQTOhFRiGBCJyIKEUzoREQhggmdiChEMKETEYWI/web2aq3MAgkDAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lrs, losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 10\n",
    "nemb = 300\n",
    "nhid = 300\n",
    "nlayers = 2\n",
    "ntokens = len(corpus.dictionary)\n",
    "model = RNNModel(ntokens, nemb, nhid, nlayers).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | lr 8.28489 | t_loss  5.90 | t_ppl 364.66 | v_loss  5.68 | v_ppl 292.48\n",
      "| epoch   1 | lr 7.42878 | t_loss  5.65 | t_ppl 283.10 | v_loss  5.43 | v_ppl 227.92\n",
      "| epoch   2 | lr 4.63954 | t_loss  5.37 | t_ppl 214.31 | v_loss  5.23 | v_ppl 185.87\n",
      "| epoch   3 | lr 4.00000 | t_loss  5.22 | t_ppl 185.14 | v_loss  5.16 | v_ppl 173.31\n",
      "| epoch   0 | lr 8.28489 | t_loss  5.17 | t_ppl 175.33 | v_loss  5.21 | v_ppl 183.51\n",
      "| epoch   1 | lr 7.42878 | t_loss  5.17 | t_ppl 175.81 | v_loss  5.13 | v_ppl 169.35\n",
      "| epoch   2 | lr 4.63954 | t_loss  5.01 | t_ppl 150.21 | v_loss  5.04 | v_ppl 154.18\n",
      "| epoch   3 | lr 4.00000 | t_loss  4.91 | t_ppl 136.23 | v_loss  5.00 | v_ppl 147.98\n",
      "| epoch   0 | lr 6.28489 | t_loss  4.86 | t_ppl 128.80 | v_loss  5.04 | v_ppl 153.78\n",
      "| epoch   1 | lr 5.42878 | t_loss  4.90 | t_ppl 133.96 | v_loss  5.02 | v_ppl 151.62\n",
      "| epoch   2 | lr 2.63954 | t_loss  4.79 | t_ppl 120.11 | v_loss  4.93 | v_ppl 138.39\n",
      "| epoch   3 | lr 2.00000 | t_loss  4.72 | t_ppl 112.24 | v_loss  4.91 | v_ppl 136.05\n",
      "| epoch   0 | lr 6.28489 | t_loss  4.72 | t_ppl 111.99 | v_loss  4.99 | v_ppl 146.68\n",
      "| epoch   1 | lr 5.42878 | t_loss  4.78 | t_ppl 119.23 | v_loss  4.99 | v_ppl 147.45\n",
      "| epoch   2 | lr 2.63954 | t_loss  4.68 | t_ppl 107.97 | v_loss  4.91 | v_ppl 135.87\n",
      "| epoch   3 | lr 2.00000 | t_loss  4.62 | t_ppl 101.28 | v_loss  4.88 | v_ppl 132.18\n",
      "| epoch   0 | lr 4.57074 | t_loss  4.60 | t_ppl 99.27 | v_loss  4.93 | v_ppl 138.39\n",
      "| epoch   1 | lr 3.85731 | t_loss  4.64 | t_ppl 103.99 | v_loss  4.90 | v_ppl 134.46\n",
      "| epoch   2 | lr 1.53295 | t_loss  4.57 | t_ppl 96.90 | v_loss  4.86 | v_ppl 129.35\n",
      "| epoch   3 | lr 1.00000 | t_loss  4.53 | t_ppl 92.85 | v_loss  4.85 | v_ppl 127.80\n",
      "| epoch   0 | lr 4.57074 | t_loss  4.53 | t_ppl 92.85 | v_loss  4.92 | v_ppl 137.05\n",
      "| epoch   1 | lr 3.85731 | t_loss  4.59 | t_ppl 98.20 | v_loss  4.90 | v_ppl 134.51\n",
      "| epoch   2 | lr 1.53295 | t_loss  4.52 | t_ppl 91.78 | v_loss  4.85 | v_ppl 127.91\n",
      "| epoch   3 | lr 1.00000 | t_loss  4.48 | t_ppl 88.18 | v_loss  4.83 | v_ppl 125.59\n",
      "| epoch   0 | lr 2.99952 | t_loss  4.47 | t_ppl 86.97 | v_loss  4.87 | v_ppl 130.63\n",
      "| epoch   1 | lr 2.50012 | t_loss  4.50 | t_ppl 89.60 | v_loss  4.87 | v_ppl 130.91\n",
      "| epoch   2 | lr 0.87306 | t_loss  4.46 | t_ppl 86.28 | v_loss  4.83 | v_ppl 124.94\n",
      "| epoch   3 | lr 0.50000 | t_loss  4.44 | t_ppl 84.46 | v_loss  4.81 | v_ppl 123.20\n",
      "| epoch   0 | lr 2.99952 | t_loss  4.43 | t_ppl 83.79 | v_loss  4.86 | v_ppl 128.94\n",
      "| epoch   1 | lr 2.50012 | t_loss  4.46 | t_ppl 86.80 | v_loss  4.84 | v_ppl 126.99\n",
      "| epoch   2 | lr 0.87306 | t_loss  4.43 | t_ppl 83.60 | v_loss  4.81 | v_ppl 123.20\n",
      "| epoch   3 | lr 0.50000 | t_loss  4.40 | t_ppl 81.79 | v_loss  4.81 | v_ppl 122.15\n",
      "| epoch   0 | lr 1.45688 | t_loss  4.40 | t_ppl 81.17 | v_loss  4.83 | v_ppl 125.01\n",
      "| epoch   1 | lr 1.18578 | t_loss  4.40 | t_ppl 81.34 | v_loss  4.82 | v_ppl 124.37\n",
      "| epoch   2 | lr 0.30252 | t_loss  4.39 | t_ppl 80.98 | v_loss  4.79 | v_ppl 120.82\n",
      "| epoch   3 | lr 0.10000 | t_loss  4.40 | t_ppl 81.68 | v_loss  4.78 | v_ppl 118.78\n",
      "| epoch   0 | lr 1.45688 | t_loss  4.37 | t_ppl 79.40 | v_loss  4.82 | v_ppl 123.96\n",
      "| epoch   1 | lr 1.18578 | t_loss  4.38 | t_ppl 80.11 | v_loss  4.82 | v_ppl 123.81\n",
      "| epoch   2 | lr 0.30252 | t_loss  4.38 | t_ppl 79.60 | v_loss  4.79 | v_ppl 120.35\n",
      "| epoch   3 | lr 0.10000 | t_loss  4.39 | t_ppl 80.39 | v_loss  4.78 | v_ppl 118.57\n"
     ]
    }
   ],
   "source": [
    "runs = [\n",
    "    [4,10],\n",
    "    [4,10],\n",
    "    [2,8],\n",
    "    [2,8],\n",
    "    [1,6],\n",
    "    [1,6],\n",
    "    [0.5,4],\n",
    "    [0.5,4],\n",
    "    [0.1,2],\n",
    "    [0.1,2]    \n",
    "]\n",
    "for run in runs:\n",
    "    train_triangular_policy(model, epochs=4, lr_low=run[0], lr_high=run[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PATH/\"mode117.pth\"\n",
    "save_model(model, str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wiki.train.tokens', 'mode117.pth', 'wiki.valid.tokens', 'wiki.test.tokens']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.dictionary.to_pkl('dict17.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.3439606e-02,  2.0715311e-02,  9.1994025e-02, ...,\n",
       "        -6.1570562e-02,  5.3499438e-02, -4.3861378e-02],\n",
       "       [ 1.0782615e-02,  1.7795961e-01,  9.4854139e-04, ...,\n",
       "         1.5818091e-01, -7.0780493e-02,  7.3512267e-05],\n",
       "       [-4.4077042e-02,  1.1453146e-01,  1.2156373e-02, ...,\n",
       "         1.2476309e-01, -2.0462185e-02, -1.5805538e-01],\n",
       "       ...,\n",
       "       [ 2.7100815e-02,  5.4215021e-02, -1.7806140e-03, ...,\n",
       "         1.0721195e-01, -2.6213776e-04,  6.4145461e-02],\n",
       "       [-3.3080060e-02, -5.3587399e-02, -6.5688647e-02, ...,\n",
       "         7.5484984e-02,  1.7419914e-03,  2.6621601e-02],\n",
       "       [ 6.3480407e-02,  5.1375408e-02, -1.6410444e-02, ...,\n",
       "         5.1401567e-02, -2.6295036e-03,  1.9764256e-04]], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lm_emb_np_trained.pkl','wb') as f:\n",
    "    pickle.dump(emb.weight.cpu().data.numpy(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
