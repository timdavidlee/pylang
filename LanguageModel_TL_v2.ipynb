{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "import _pickle as pickle\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Unzip the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip -P ~/data/wikitext/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !unzip /home/paperspace/data/wikitext/wikitext-2-v1.zip -d /home/paperspace/data/wikitext/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a peek at the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \r\n",
      " = Homarus gammarus = \r\n",
      " \r\n",
      " Homarus gammarus , known as the European lobster or common lobster , is a species of <unk> lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts of the Black Sea . It is closely related to the American lobster , H. americanus . It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ) , and bears a conspicuous pair of claws . In life , the lobsters are blue , only becoming \" lobster red \" on cooking . Mating occurs in the summer , producing eggs which are carried by the females for up to a year before hatching into <unk> larvae . Homarus gammarus is a highly esteemed food , and is widely caught using lobster pots , mostly around the British Isles . \r\n",
      " \r\n",
      " = = Description = = \r\n",
      " \r\n",
      " Homarus gammarus is a large <unk> , with a body length up to 60 centimetres ( 24 in ) and weighing up to 5 – 6 kilograms ( 11 – 13 lb ) , although the lobsters caught in lobster pots are usually 23 – 38 cm ( 9 – 15 in ) long and weigh 0 @.@ 7 – 2 @.@ 2 kg ( 1 @.@ 5 – 4 @.@ 9 lb ) . Like other crustaceans , lobsters have a hard <unk> which they must shed in order to grow , in a process called <unk> ( <unk> ) . This may occur several times a year for young lobsters , but decreases to once every 1 – 2 years for larger animals . \r\n",
      " The first pair of <unk> is armed with a large , asymmetrical pair of claws . The larger one is the \" <unk> \" , and has rounded <unk> used for crushing prey ; the other is the \" cutter \" , which has sharp inner edges , and is used for holding or tearing the prey . Usually , the left claw is the <unk> , and the right is the cutter . \r\n",
      " The <unk> is generally blue above , with spots that <unk> , and yellow below . The red colour associated with lobsters only appears after cooking . This occurs because , in life , the red pigment <unk> is bound to a protein complex , but the complex is broken up by the heat of cooking , releasing the red pigment . \r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "! cat /home/paperspace/data/wikitext/wikitext-2/wiki.valid.tokens | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/paperspace/data/wikitext/wikitext-2/wiki.train.tokens'),\n",
       " PosixPath('/home/paperspace/data/wikitext/wikitext-2/mode117.pth'),\n",
       " PosixPath('/home/paperspace/data/wikitext/wikitext-2/wiki.valid.tokens'),\n",
       " PosixPath('/home/paperspace/data/wikitext/wikitext-2/sample.txt'),\n",
       " PosixPath('/home/paperspace/data/wikitext/wikitext-2/wiki.test.tokens')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH=Path(\"/home/paperspace/data/wikitext/wikitext-2\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up some helper objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# Pre-processing Objects\n",
    "# ========================================================================\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializes both conversions of the dictionary \"\"\"\n",
    "        self.word2idx = {\"UNK\": 0}\n",
    "        self.idx2word = [\"UNK\"]\n",
    "\n",
    "    def add_word(self, word):\n",
    "        \"\"\" Adds a new word to the dictionary. gives it an auto incremented ID\"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Returns the number of tokens in the word\"\"\"\n",
    "        return len(self.idx2word)\n",
    "    \n",
    "    def to_pkl(self, filepath):\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump([self.word2idx, self.idx2word],f)\n",
    "        \n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path, ):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'wiki.train.tokens'), add=True)\n",
    "        self.valid = self.tokenize(os.path.join(path, 'wiki.valid.tokens'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'wiki.test.tokens'))\n",
    "        \n",
    "    def count_words(self, path, add=False):\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                if add:\n",
    "                    for word in words:\n",
    "                        self.dictionary.add_word(word)\n",
    "        return tokens\n",
    "\n",
    "    def tokenize(self, path, add=False):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        tokens = self.count_words(path, add)\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r') as f:\n",
    "            ids = torch.LongTensor(tokens)\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx.get(word, 0)\n",
    "                    token += 1\n",
    "        return ids\n",
    "\n",
    "# ========================================================================\n",
    "# Helper functions\n",
    "# ========================================================================\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    \"\"\"\n",
    "    Takes any dataset and returns a sub-batched version\n",
    "    [1,2,3,4,5,6,7,8,9,10], batchsize = 3 will give\n",
    "    (1,2,3) (4,5,6) (7,8,9)\n",
    "    \"\"\"\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    \n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    \n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.cuda()\n",
    "\n",
    "def get_batch(source, i, bptt, evaluation=False):\n",
    "    \"\"\"\n",
    "    Source: data array\n",
    "    i: index\n",
    "    bptt: back prop through time\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = Variable(source[i:i+seq_len], volatile=evaluation)\n",
    "    target = Variable(source[i+1:i+1+seq_len].view(-1))\n",
    "    return data, target\n",
    "\n",
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "\n",
    "def load_model(m, p): m.load_state_dict(torch.load(p))\n",
    "\n",
    "def LR_range_finder(model, train_data, criterion, lr_low=1e-3, lr_high=10, epochs=2):\n",
    "    losses = []\n",
    "    (train_data.size(0) - 1)//bptt + 1\n",
    "    iterations = epochs * ((train_data.size(0) - 1)//bptt + 1)\n",
    "    delta = (lr_high - lr_low)/(iterations-1)\n",
    "    losses = []\n",
    "    lrs = [lr_low + i*delta for i in range(iterations)]\n",
    "    model.train()\n",
    "    ind = 0\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    for i in range(epochs):\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "            lr = lrs[ind]\n",
    "            data, targets = get_batch(train_data, i, bptt)\n",
    "        \n",
    "            hidden = Variable(hidden.data) #.detach()\n",
    "            model.zero_grad()\n",
    "            output, hidden = model(data, hidden)\n",
    "            loss = criterion(output.view(-1, ntokens), targets)\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "            losses.append(loss.data[0])\n",
    "            ind += 1\n",
    "    return lrs, losses \n",
    "\n",
    "def get_triangular_lr2(lr_low, lr_high, iterations):\n",
    "    iter1 = int(0.35*iterations)\n",
    "    iter2 = int(0.85*iter1)\n",
    "    iter3 = iterations - iter1 - iter2\n",
    "    delta1 = (lr_high - lr_low)/iter1\n",
    "    delta2 = (lr_high - lr_low)/(iter1 -1)\n",
    "    lrs1 = [lr_low + i*delta1 for i in range(iter1)]\n",
    "    lrs2 = [lr_high - i*(delta1) for i in range(0, iter2)]\n",
    "    delta2 = (lrs2[-1] - lr_low)/(iter3)\n",
    "    lrs3 = [lrs2[-1] - i*(delta2) for i in range(1, iter3+1)]\n",
    "    return lrs1+lrs2+lrs3\n",
    "\n",
    "\n",
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for i in range(0, data_source.size(0) - 1, bptt):\n",
    "        data, targets = get_batch(data_source, i, bptt, evaluation=True)\n",
    "        output, hidden = model(data, hidden)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += len(data) * criterion(output_flat, targets).data\n",
    "        hidden = Variable(hidden.data) #.detach()\n",
    "    return total_loss[0] / len(data_source)\n",
    "\n",
    "\n",
    "# ========================================================================\n",
    "# Model functions\n",
    "# ========================================================================\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Container module with an encoder, a recurrent module, and a decoder.\n",
    "    \n",
    "    ntoken: number of tokens\n",
    "    ninp: number of inputs (embedding size)\n",
    "    nhid: number of hidden units\n",
    "    nlayers: number of layers\n",
    "    dropout: % dropout\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, emb_sz, nhid, nlayers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.ndir = 2 # because bidir\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, emb_sz)\n",
    "        self.rnn1 = nn.GRU(emb_sz, nhid, nlayers, bidirectional=True, dropout=dropout)\n",
    "        self.rnn2 = nn.GRU(nhid*2, emb_sz, nlayers, bidirectional=True, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid*2, ntoken)\n",
    "        self.init_weights()\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range = 0.1\n",
    "        self.encoder.weight.data.uniform_(-init_range, init_range)\n",
    "        self.decoder.bias.data.fill_(0.0)\n",
    "        self.decoder.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\"\n",
    "        input: current input\n",
    "        hidden: hidden state from the previous step\n",
    "        \"\"\"\n",
    "        # pulls the embeddings for the input submitted\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        \n",
    "        # then applies the RNN against the embedding layer\n",
    "        output, hidden = self.rnn1(emb, hidden)\n",
    "        output, hidden = self.rnn2(output, hidden)\n",
    "        \n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        \"\"\"\n",
    "        Initialize the hidden weights\n",
    "        \"\"\"\n",
    "        weight = next(self.parameters()).data\n",
    "        return Variable(weight.new(self.nlayers*self.ndir, bsz, self.nhid).zero_())\n",
    "    \n",
    "\n",
    "\n",
    "def train_triangular_policy(model, epochs=4, lr_low=1e-4, lr_high=4): # on training mode which enables dropout.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    iterations = epochs * ((train_data.size(0) - 1)//bptt + 1)\n",
    "    lrs = get_triangular_lr2(lr_low, lr_high, iterations)\n",
    "    idx = 0\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # initialize the model\n",
    "        # these are the weights from hidden layers\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        \n",
    "        for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "            lr = lrs[idx]\n",
    "            \n",
    "            # get a single batch of data (iterator)\n",
    "            data, targets = get_batch(train_data, i, bptt)\n",
    "            \n",
    "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "            # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "            hidden = Variable(hidden.data) #.detach()\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # run the data through the model\n",
    "            # this would be a small snippet of the phrase\n",
    "            output, hidden = model(data, hidden)\n",
    "            loss = criterion(output.view(-1, ntokens), targets)\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "            total_loss += len(data)*loss.data\n",
    "            idx += 1\n",
    "        # results after each epoch\n",
    "        val_loss = evaluate(val_data)\n",
    "        elapsed = time.time() - start_time\n",
    "        train_loss = total_loss[0]/len(train_data)\n",
    "        print('| epoch {:3d} | lr {:02.5f} | t_loss {:5.2f} | t_ppl {:5.2f} | v_loss {:5.2f} | v_ppl {:5.2f}'.format(\n",
    "             epoch, lr, train_loss, math.exp(train_loss), val_loss, math.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33279"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = Corpus(PATH)\n",
    "len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2319\n",
       "  137\n",
       "   28\n",
       " 5648\n",
       "    1\n",
       "[torch.LongTensor of size 5]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample tokenization using a corpus\n",
    "with open(PATH/'sample.txt','w') as f:\n",
    "    f.write('man met a dog')\n",
    "corpus.tokenize(PATH/'sample.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, batch_size)\n",
    "test_data = batchify(corpus.test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "nemb = 300\n",
    "nhid = 300\n",
    "bptt = 35\n",
    "clip = 0.25\n",
    "nlayers = 2\n",
    "ntokens = len(corpus.dictionary)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = RNNModel(ntokens, nemb, nhid, nlayers).cuda()\n",
    "# lrs, losses = LR_range_finder(model, train_data, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([104431, 20]), torch.Size([12278, 20]))"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20, 300])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden = model.init_hidden(batch_size)\n",
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 20, 300])\n",
      "start torch.Size([35, 20, 300]) torch.Size([4, 20, 300])\n",
      "rnn1 output torch.Size([35, 20, 600]) torch.Size([4, 20, 300])\n",
      "rnn2 output torch.Size([35, 20, 600]) torch.Size([4, 20, 300])\n"
     ]
    }
   ],
   "source": [
    "hidden = model.init_hidden(batch_size)\n",
    "print(hidden.shape)\n",
    "\n",
    "data, targets = get_batch(train_data, 0, bptt)\n",
    "emb = model.drop(model.encoder(data))\n",
    "        \n",
    "# then applies the RNN against the embedding layer\n",
    "print('start', emb.shape, hidden.shape)\n",
    "tmp_out, tmp_hid = model.rnn1(emb, hidden)\n",
    "print('rnn1 output', tmp_out.shape, tmp_hid.shape)\n",
    "tmp_out, tmp_hid = model.rnn2(tmp_out, tmp_hid)\n",
    "print('rnn2 output',tmp_out.shape, tmp_hid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([35, 20]),\n",
       " torch.Size([35, 20, 300]),\n",
       " torch.Size([35, 20, 600]),\n",
       " torch.Size([4, 20, 300]))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden = model.init_hidden(batch_size)\n",
    "data, targets = get_batch(train_data, 0, bptt)\n",
    "output, hidden = model(data, hidden)\n",
    "data.shape, emb.shape, tmp_out.shape, tmp_hid.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([35, 20, 33279]), torch.Size([4, 20, 300]), torch.Size([700]))"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " output.shape,  hidden.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(lrs, losses)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 10\n",
    "nemb = 300\n",
    "nhid = 300\n",
    "nlayers = 2\n",
    "ntokens = len(corpus.dictionary)\n",
    "model = RNNModel(ntokens, nemb, nhid, nlayers).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | lr 6.14219 | t_loss  3.67 | t_ppl 39.24 | v_loss  1.60 | v_ppl  4.94\n",
      "| epoch   1 | lr 8.28510 | t_loss  1.55 | t_ppl  4.70 | v_loss  0.86 | v_ppl  2.36\n",
      "| epoch   2 | lr 9.57199 | t_loss  1.03 | t_ppl  2.80 | v_loss  0.61 | v_ppl  1.84\n",
      "| epoch   3 | lr 7.42908 | t_loss  0.77 | t_ppl  2.16 | v_loss  0.47 | v_ppl  1.60\n",
      "| epoch   4 | lr 5.28618 | t_loss  0.60 | t_ppl  1.83 | v_loss  0.39 | v_ppl  1.48\n",
      "| epoch   5 | lr 4.63910 | t_loss  0.50 | t_ppl  1.64 | v_loss  0.35 | v_ppl  1.41\n",
      "| epoch   6 | lr 4.31955 | t_loss  0.43 | t_ppl  1.54 | v_loss  0.32 | v_ppl  1.37\n",
      "| epoch   7 | lr 4.00000 | t_loss  0.38 | t_ppl  1.47 | v_loss  0.30 | v_ppl  1.35\n"
     ]
    }
   ],
   "source": [
    "runs = [\n",
    "    [4,10],\n",
    "    [4,10],\n",
    "    [2,8],\n",
    "    [2,8],\n",
    "    [1,6],\n",
    "    [1,6],\n",
    "    [0.5,4],\n",
    "    [0.5,4],\n",
    "    [0.1,2],\n",
    "    [0.1,2]    \n",
    "]\n",
    "\n",
    "runs=[\n",
    "    [4,10]\n",
    "]\n",
    "for run in runs:\n",
    "    train_triangular_policy(model, epochs=8, lr_low=run[0], lr_high=run[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PATH/\"mode117.pth\"\n",
    "p = PATH/\"mode118.pth\"\n",
    "save_model(model, str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wiki.train.tokens', 'mode117.pth', 'wiki.valid.tokens', 'wiki.test.tokens']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.dictionary.to_pkl('dict17.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.3439606e-02,  2.0715311e-02,  9.1994025e-02, ...,\n",
       "        -6.1570562e-02,  5.3499438e-02, -4.3861378e-02],\n",
       "       [ 1.0782615e-02,  1.7795961e-01,  9.4854139e-04, ...,\n",
       "         1.5818091e-01, -7.0780493e-02,  7.3512267e-05],\n",
       "       [-4.4077042e-02,  1.1453146e-01,  1.2156373e-02, ...,\n",
       "         1.2476309e-01, -2.0462185e-02, -1.5805538e-01],\n",
       "       ...,\n",
       "       [ 2.7100815e-02,  5.4215021e-02, -1.7806140e-03, ...,\n",
       "         1.0721195e-01, -2.6213776e-04,  6.4145461e-02],\n",
       "       [-3.3080060e-02, -5.3587399e-02, -6.5688647e-02, ...,\n",
       "         7.5484984e-02,  1.7419914e-03,  2.6621601e-02],\n",
       "       [ 6.3480407e-02,  5.1375408e-02, -1.6410444e-02, ...,\n",
       "         5.1401567e-02, -2.6295036e-03,  1.9764256e-04]], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lm_emb_np_trained.pkl','wb') as f:\n",
    "    pickle.dump(emb.weight.cpu().data.numpy(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
