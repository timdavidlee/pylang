{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subjectivity classification using a pretrained language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we implement the approched described in cite... Jeremy's paper In particular, we will classify sentences into \"subjective\" or \"objective\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subjectivity Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subjectivity dataset has 5000 subjective and 5000 objective processed sentences. To get the data:\n",
    "```\n",
    "wget http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten_imdb.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/data/yinterian/rotten_imdb/subjdata.README.1.0'),\n",
       " PosixPath('/data/yinterian/rotten_imdb/plot.tok.gt9.5000'),\n",
       " PosixPath('/data/yinterian/rotten_imdb/quote.tok.gt9.5000')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "PATH = Path(\"/data/yinterian/rotten_imdb/\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the readme file:\n",
    "- quote.tok.gt9.5000 contains 5000 subjective sentences (or snippets)\n",
    "- plot.tok.gt9.5000 contains 5000 objective sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the movie begins in the past where a young boy named sam attempts to save celebi from a hunter . \r\n",
      "emerging from the human psyche and showing characteristics of abstract expressionism , minimalism and russian constructivism , graffiti removal has secured its place in the history of modern art while being created by artists who are unconscious of their artistic achievements . \r\n",
      "spurning her mother's insistence that she get on with her life , mary is thrown out of the house , rejected by joe , and expelled from school as she grows larger with child . \r\n",
      "amitabh can't believe the board of directors and his mind is filled with revenge and what better revenge than robbing the bank himself , ironic as it may sound . \r\n",
      "she , among others excentricities , talks to a small rock , gertrude , like if she was alive . \r\n",
      "this gives the girls a fair chance of pulling the wool over their eyes using their sexiness to poach any last vestige of common sense the dons might have had . \r\n",
      "styled after vh1's \" behind the music , \" this mockumentary profiles the rise and fall of an internet startup , called icevan . com . \r\n",
      "being blue is not his only predicament ; he also lacks the ability to outwardly express his emotions . \r\n",
      "the killer's clues are a perversion of biblical punishments for sins : stoning , burning , decapitation . \r\n",
      "david is a painter with painter's block who takes a job as a waiter to get some inspiration . \r\n"
     ]
    }
   ],
   "source": [
    "! head /data/yinterian/rotten_imdb/plot.tok.gt9.5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# this is from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Every dataset is lower cased except for TREC\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    \"\"\" Read file returns a shuttled list.\n",
    "    \"\"\"\n",
    "    with open(path, encoding = \"ISO-8859-1\") as f:\n",
    "        content = np.array(f.readlines())\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict \n",
    "def get_vocab(list_of_content):\n",
    "    \"\"\"Computes Dict of counts of words.\n",
    "    \n",
    "    Computes the number of times a word is on a document.\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(float)\n",
    "    for content in list_of_content:\n",
    "        for line in content:\n",
    "            line = clean_str(line.strip())\n",
    "            words = set(line.split())\n",
    "            for word in words:\n",
    "                vocab[word] += 1\n",
    "    return vocab       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_content = read_file(PATH/\"quote.tok.gt9.5000\")\n",
    "obj_content = read_file(PATH/\"plot.tok.gt9.5000\")\n",
    "sub_content = np.array([clean_str(line.strip()) for line in sub_content])\n",
    "obj_content = np.array([clean_str(line.strip()) for line in obj_content])\n",
    "sub_y = np.zeros(len(sub_content))\n",
    "obj_y = np.ones(len(obj_content))\n",
    "X = np.append(sub_content, obj_content)\n",
    "y = np.append(sub_y, obj_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['will god let her fall or give her a new path \\\\?',\n",
       "        \"the director 's twitchy sketchbook style and adroit perspective shifts grow wearisome amid leaden pacing and indifferent craftsmanship \\\\( most notably wretched sound design \\\\)\",\n",
       "        \"welles groupie scholar peter bogdanovich took a long time to do it , but he 's finally provided his own broadside at publishing giant william randolph hearst\",\n",
       "        'based on the 1997 john king novel of the same name with a rather odd synopsis a first novel about a seasoned chelsea football club hooligan who represents a disaffected society operating by brutal rules',\n",
       "        'yet , beneath an upbeat appearance , she is struggling desperately with the emotional and physical scars left by the attack'],\n",
       "       dtype='<U679'), array([1., 0., 0., 1., 1.]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5], y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting vocab from training sets\n",
    "vocab = get_vocab([X_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laguage Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        self.init_weights()\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range = 0.1\n",
    "        self.encoder.weight.data.uniform_(-init_range, init_range)\n",
    "        self.decoder.bias.data.fill_(0.0)\n",
    "        self.decoder.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        return Variable(weight.new(self.nlayers, bsz, self.nhid).zero_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "nemb = 300\n",
    "nhid = 300\n",
    "nlayers = 2\n",
    "ntokens = 33279\n",
    "lang_model = RNNModel(ntokens, nemb, nhid, nlayers).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load pre-trained model\n",
    "model_path = \"/data/yinterian/wikitext-2/mode117.pth\"\n",
    "def load_model(m, p):\n",
    "    m.load_state_dict(torch.load(p))\n",
    "load_model(lang_model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel (\n",
       "  (drop): Dropout (p = 0.5)\n",
       "  (encoder): Embedding(33279, 300)\n",
       "  (rnn): GRU(300, 300, num_layers=2, dropout=0.5)\n",
       "  (decoder): Linear (300 -> 33279)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding data with original corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {\"UNK\": 0}\n",
    "        self.idx2word = [\"UNK\"]\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "\n",
    "        self.train = self.tokenize(os.path.join(path, 'wiki.train.tokens'), add=True)\n",
    "        self.valid = self.tokenize(os.path.join(path, 'wiki.valid.tokens'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'wiki.test.tokens'))\n",
    "        \n",
    "    def count_words(self, path, add=False):\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                if add:\n",
    "                    for word in words:\n",
    "                        self.dictionary.add_word(word)\n",
    "        return tokens\n",
    "\n",
    "    def tokenize(self, path, add=False):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        tokens = self.count_words(path, add)\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r') as f:\n",
    "            ids = torch.LongTensor(tokens)\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx.get(word, 0)\n",
    "                    token += 1\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_corpus = Corpus(\"/data/yinterian/wikitext-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK': 0,\n",
       " '<eos>': 1,\n",
       " '=': 2,\n",
       " 'Valkyria': 3,\n",
       " 'Chronicles': 4,\n",
       " 'III': 5,\n",
       " 'Senjō': 6,\n",
       " 'no': 7,\n",
       " '3': 8,\n",
       " ':': 9,\n",
       " '<unk>': 10,\n",
       " '(': 11,\n",
       " 'Japanese': 12,\n",
       " '戦場のヴァルキュリア3': 13,\n",
       " ',': 14,\n",
       " 'lit': 15,\n",
       " '.': 16,\n",
       " 'of': 17,\n",
       " 'the': 18,\n",
       " 'Battlefield': 19,\n",
       " ')': 20,\n",
       " 'commonly': 21,\n",
       " 'referred': 22,\n",
       " 'to': 23,\n",
       " 'as': 24,\n",
       " 'outside': 25,\n",
       " 'Japan': 26,\n",
       " 'is': 27,\n",
       " 'a': 28,\n",
       " 'tactical': 29,\n",
       " 'role': 30,\n",
       " '@-@': 31,\n",
       " 'playing': 32,\n",
       " 'video': 33,\n",
       " 'game': 34,\n",
       " 'developed': 35,\n",
       " 'by': 36,\n",
       " 'Sega': 37,\n",
       " 'and': 38,\n",
       " 'Media.Vision': 39,\n",
       " 'for': 40,\n",
       " 'PlayStation': 41,\n",
       " 'Portable': 42,\n",
       " 'Released': 43,\n",
       " 'in': 44,\n",
       " 'January': 45,\n",
       " '2011': 46,\n",
       " 'it': 47,\n",
       " 'third': 48,\n",
       " 'series': 49,\n",
       " 'same': 50,\n",
       " 'fusion': 51,\n",
       " 'real': 52,\n",
       " 'time': 53,\n",
       " 'gameplay': 54,\n",
       " 'its': 55,\n",
       " 'predecessors': 56,\n",
       " 'story': 57,\n",
       " 'runs': 58,\n",
       " 'parallel': 59,\n",
       " 'first': 60,\n",
       " 'follows': 61,\n",
       " '\"': 62,\n",
       " 'Nameless': 63,\n",
       " 'penal': 64,\n",
       " 'military': 65,\n",
       " 'unit': 66,\n",
       " 'serving': 67,\n",
       " 'nation': 68,\n",
       " 'Gallia': 69,\n",
       " 'during': 70,\n",
       " 'Second': 71,\n",
       " 'Europan': 72,\n",
       " 'War': 73,\n",
       " 'who': 74,\n",
       " 'perform': 75,\n",
       " 'secret': 76,\n",
       " 'black': 77,\n",
       " 'operations': 78,\n",
       " 'are': 79,\n",
       " 'pitted': 80,\n",
       " 'against': 81,\n",
       " 'Imperial': 82,\n",
       " 'Raven': 83,\n",
       " 'The': 84,\n",
       " 'began': 85,\n",
       " 'development': 86,\n",
       " '2010': 87,\n",
       " 'carrying': 88,\n",
       " 'over': 89,\n",
       " 'large': 90,\n",
       " 'portion': 91,\n",
       " 'work': 92,\n",
       " 'done': 93,\n",
       " 'on': 94,\n",
       " 'II': 95,\n",
       " 'While': 96,\n",
       " 'retained': 97,\n",
       " 'standard': 98,\n",
       " 'features': 99,\n",
       " 'also': 100,\n",
       " 'underwent': 101,\n",
       " 'multiple': 102,\n",
       " 'adjustments': 103,\n",
       " 'such': 104,\n",
       " 'making': 105,\n",
       " 'more': 106,\n",
       " 'newcomers': 107,\n",
       " 'Character': 108,\n",
       " 'designer': 109,\n",
       " 'Honjou': 110,\n",
       " 'composer': 111,\n",
       " 'Hitoshi': 112,\n",
       " 'Sakimoto': 113,\n",
       " 'both': 114,\n",
       " 'returned': 115,\n",
       " 'from': 116,\n",
       " 'previous': 117,\n",
       " 'entries': 118,\n",
       " 'along': 119,\n",
       " 'with': 120,\n",
       " 'director': 121,\n",
       " 'Takeshi': 122,\n",
       " 'Ozawa': 123,\n",
       " 'A': 124,\n",
       " 'team': 125,\n",
       " 'writers': 126,\n",
       " 'handled': 127,\n",
       " 'script': 128,\n",
       " \"'s\": 129,\n",
       " 'opening': 130,\n",
       " 'theme': 131,\n",
       " 'was': 132,\n",
       " 'sung': 133,\n",
       " 'May': 134,\n",
       " \"'n\": 135,\n",
       " 'It': 136,\n",
       " 'met': 137,\n",
       " 'positive': 138,\n",
       " 'sales': 139,\n",
       " 'praised': 140,\n",
       " 'western': 141,\n",
       " 'critics': 142,\n",
       " 'After': 143,\n",
       " 'release': 144,\n",
       " 'received': 145,\n",
       " 'downloadable': 146,\n",
       " 'content': 147,\n",
       " 'an': 148,\n",
       " 'expanded': 149,\n",
       " 'edition': 150,\n",
       " 'November': 151,\n",
       " 'that': 152,\n",
       " 'year': 153,\n",
       " 'adapted': 154,\n",
       " 'into': 155,\n",
       " 'manga': 156,\n",
       " 'original': 157,\n",
       " 'animation': 158,\n",
       " 'Due': 159,\n",
       " 'low': 160,\n",
       " 'not': 161,\n",
       " 'localized': 162,\n",
       " 'but': 163,\n",
       " 'fan': 164,\n",
       " 'translation': 165,\n",
       " 'compatible': 166,\n",
       " 'released': 167,\n",
       " '2014': 168,\n",
       " 'would': 169,\n",
       " 'return': 170,\n",
       " 'franchise': 171,\n",
       " 'Azure': 172,\n",
       " 'Revolution': 173,\n",
       " '4': 174,\n",
       " 'Gameplay': 175,\n",
       " 'As': 176,\n",
       " 'games': 177,\n",
       " 'where': 178,\n",
       " 'players': 179,\n",
       " 'take': 180,\n",
       " 'control': 181,\n",
       " 'part': 182,\n",
       " 'missions': 183,\n",
       " 'enemy': 184,\n",
       " 'forces': 185,\n",
       " 'Stories': 186,\n",
       " 'told': 187,\n",
       " 'through': 188,\n",
       " 'comic': 189,\n",
       " 'book': 190,\n",
       " 'like': 191,\n",
       " 'panels': 192,\n",
       " 'animated': 193,\n",
       " 'character': 194,\n",
       " 'portraits': 195,\n",
       " 'characters': 196,\n",
       " 'speaking': 197,\n",
       " 'partially': 198,\n",
       " 'voiced': 199,\n",
       " 'speech': 200,\n",
       " 'bubbles': 201,\n",
       " 'text': 202,\n",
       " 'player': 203,\n",
       " 'progresses': 204,\n",
       " 'linear': 205,\n",
       " 'gradually': 206,\n",
       " 'unlocked': 207,\n",
       " 'maps': 208,\n",
       " 'can': 209,\n",
       " 'be': 210,\n",
       " 'freely': 211,\n",
       " 'replayed': 212,\n",
       " 'they': 213,\n",
       " 'route': 214,\n",
       " 'each': 215,\n",
       " 'location': 216,\n",
       " 'map': 217,\n",
       " 'varies': 218,\n",
       " 'depending': 219,\n",
       " 'individual': 220,\n",
       " 'approach': 221,\n",
       " 'when': 222,\n",
       " 'one': 223,\n",
       " 'option': 224,\n",
       " 'selected': 225,\n",
       " 'other': 226,\n",
       " 'sealed': 227,\n",
       " 'off': 228,\n",
       " 'Outside': 229,\n",
       " 'rest': 230,\n",
       " 'camp': 231,\n",
       " 'units': 232,\n",
       " 'customized': 233,\n",
       " 'growth': 234,\n",
       " 'occurs': 235,\n",
       " 'Alongside': 236,\n",
       " 'main': 237,\n",
       " 'specific': 238,\n",
       " 'sub': 239,\n",
       " 'relating': 240,\n",
       " 'different': 241,\n",
       " 'squad': 242,\n",
       " 'members': 243,\n",
       " 'completion': 244,\n",
       " 'additional': 245,\n",
       " 'episodes': 246,\n",
       " 'some': 247,\n",
       " 'them': 248,\n",
       " 'having': 249,\n",
       " 'higher': 250,\n",
       " 'difficulty': 251,\n",
       " 'than': 252,\n",
       " 'those': 253,\n",
       " 'found': 254,\n",
       " 'There': 255,\n",
       " 'love': 256,\n",
       " 'simulation': 257,\n",
       " 'elements': 258,\n",
       " 'related': 259,\n",
       " 'two': 260,\n",
       " 'although': 261,\n",
       " 'very': 262,\n",
       " 'minor': 263,\n",
       " 'battle': 264,\n",
       " 'system': 265,\n",
       " 'carried': 266,\n",
       " 'directly': 267,\n",
       " 'During': 268,\n",
       " 'select': 269,\n",
       " 'using': 270,\n",
       " 'top': 271,\n",
       " 'down': 272,\n",
       " 'perspective': 273,\n",
       " 'battlefield': 274,\n",
       " 'once': 275,\n",
       " 'moves': 276,\n",
       " 'around': 277,\n",
       " 'person': 278,\n",
       " 'only': 279,\n",
       " 'act': 280,\n",
       " 'per': 281,\n",
       " 'turn': 282,\n",
       " 'granted': 283,\n",
       " 'turns': 284,\n",
       " 'at': 285,\n",
       " 'expense': 286,\n",
       " \"'\": 287,\n",
       " 'Each': 288,\n",
       " 'has': 289,\n",
       " 'field': 290,\n",
       " 'distance': 291,\n",
       " 'movement': 292,\n",
       " 'limited': 293,\n",
       " 'their': 294,\n",
       " 'Action': 295,\n",
       " 'Up': 296,\n",
       " 'nine': 297,\n",
       " 'assigned': 298,\n",
       " 'single': 299,\n",
       " 'mission': 300,\n",
       " 'will': 301,\n",
       " 'call': 302,\n",
       " 'out': 303,\n",
       " 'if': 304,\n",
       " 'something': 305,\n",
       " 'happens': 306,\n",
       " 'health': 307,\n",
       " 'points': 308,\n",
       " 'HP': 309,\n",
       " 'getting': 310,\n",
       " 'or': 311,\n",
       " 'being': 312,\n",
       " 'knocked': 313,\n",
       " 'attacks': 314,\n",
       " 'Potentials': 315,\n",
       " 'skills': 316,\n",
       " 'unique': 317,\n",
       " 'They': 318,\n",
       " 'divided': 319,\n",
       " 'Personal': 320,\n",
       " 'Potential': 321,\n",
       " 'which': 322,\n",
       " 'innate': 323,\n",
       " 'remain': 324,\n",
       " 'unaltered': 325,\n",
       " 'unless': 326,\n",
       " 'otherwise': 327,\n",
       " 'dictated': 328,\n",
       " 'either': 329,\n",
       " 'help': 330,\n",
       " 'impede': 331,\n",
       " 'Battle': 332,\n",
       " 'grown': 333,\n",
       " 'throughout': 334,\n",
       " 'always': 335,\n",
       " 'grant': 336,\n",
       " 'To': 337,\n",
       " 'learn': 338,\n",
       " 'Masters': 339,\n",
       " 'Table': 340,\n",
       " 'grid': 341,\n",
       " 'based': 342,\n",
       " 'skill': 343,\n",
       " 'table': 344,\n",
       " 'used': 345,\n",
       " 'acquire': 346,\n",
       " 'link': 347,\n",
       " 'Characters': 348,\n",
       " 'have': 349,\n",
       " 'Special': 350,\n",
       " 'temporary': 351,\n",
       " 'Kurt': 352,\n",
       " 'activate': 353,\n",
       " 'Direct': 354,\n",
       " 'Command': 355,\n",
       " 'move': 356,\n",
       " 'without': 357,\n",
       " 'his': 358,\n",
       " 'Point': 359,\n",
       " 'gauge': 360,\n",
       " 'shift': 361,\n",
       " 'her': 362,\n",
       " 'Form': 363,\n",
       " 'become': 364,\n",
       " 'while': 365,\n",
       " 'Imca': 366,\n",
       " 'target': 367,\n",
       " 'heavy': 368,\n",
       " 'weapon': 369,\n",
       " 'Troops': 370,\n",
       " 'five': 371,\n",
       " 'classes': 372,\n",
       " 'Scouts': 373,\n",
       " 'Engineers': 374,\n",
       " 'Armored': 375,\n",
       " 'Soldier': 376,\n",
       " 'switch': 377,\n",
       " 'changing': 378,\n",
       " 'Changing': 379,\n",
       " 'class': 380,\n",
       " 'does': 381,\n",
       " 'greatly': 382,\n",
       " 'affect': 383,\n",
       " 'stats': 384,\n",
       " 'gained': 385,\n",
       " 'With': 386,\n",
       " 'victory': 387,\n",
       " 'experience': 388,\n",
       " 'awarded': 389,\n",
       " 'distributed': 390,\n",
       " 'attributes': 391,\n",
       " 'shared': 392,\n",
       " 'entire': 393,\n",
       " 'feature': 394,\n",
       " 'differing': 395,\n",
       " 'early': 396,\n",
       " 'method': 397,\n",
       " 'distributing': 398,\n",
       " 'types': 399,\n",
       " 'Plot': 400,\n",
       " 'takes': 401,\n",
       " 'place': 402,\n",
       " 'Gallian': 403,\n",
       " 'Army': 404,\n",
       " 'Squad': 405,\n",
       " '422': 406,\n",
       " 'known': 407,\n",
       " 'composed': 408,\n",
       " 'criminals': 409,\n",
       " 'foreign': 410,\n",
       " 'offenders': 411,\n",
       " 'whose': 412,\n",
       " 'names': 413,\n",
       " 'erased': 414,\n",
       " 'records': 415,\n",
       " 'officially': 416,\n",
       " 'numbers': 417,\n",
       " 'most': 418,\n",
       " 'dangerous': 419,\n",
       " 'Regular': 420,\n",
       " 'Militia': 421,\n",
       " 'do': 422,\n",
       " 'nevertheless': 423,\n",
       " 'up': 424,\n",
       " 'task': 425,\n",
       " 'exemplified': 426,\n",
       " 'motto': 427,\n",
       " 'meaning': 428,\n",
       " 'Always': 429,\n",
       " 'Ready': 430,\n",
       " 'three': 431,\n",
       " 'Irving': 432,\n",
       " 'army': 433,\n",
       " 'officer': 434,\n",
       " 'falsely': 435,\n",
       " 'accused': 436,\n",
       " 'treason': 437,\n",
       " 'wishes': 438,\n",
       " 'redeem': 439,\n",
       " 'himself': 440,\n",
       " ';': 441,\n",
       " 'Ace': 442,\n",
       " 'female': 443,\n",
       " 'Darcsen': 444,\n",
       " 'weapons': 445,\n",
       " 'specialist': 446,\n",
       " 'seeks': 447,\n",
       " 'revenge': 448,\n",
       " 'destroyed': 449,\n",
       " 'home': 450,\n",
       " 'Riela': 451,\n",
       " 'seemingly': 452,\n",
       " 'young': 453,\n",
       " 'woman': 454,\n",
       " 'unknowingly': 455,\n",
       " 'descendant': 456,\n",
       " 'Together': 457,\n",
       " 'fellow': 458,\n",
       " 'these': 459,\n",
       " 'tasked': 460,\n",
       " 'fight': 461,\n",
       " 'mysterious': 462,\n",
       " 'Calamity': 463,\n",
       " 'consisting': 464,\n",
       " 'mostly': 465,\n",
       " 'soldiers': 466,\n",
       " 'exist': 467,\n",
       " 'upper': 468,\n",
       " 'echelons': 469,\n",
       " 'exploit': 470,\n",
       " 'concept': 471,\n",
       " 'plausible': 472,\n",
       " 'order': 473,\n",
       " 'send': 474,\n",
       " 'make': 475,\n",
       " 'lose': 476,\n",
       " 'face': 477,\n",
       " 'war': 478,\n",
       " 'times': 479,\n",
       " 'this': 480,\n",
       " 'works': 481,\n",
       " 'advantage': 482,\n",
       " 'successful': 483,\n",
       " 'incursion': 484,\n",
       " 'territory': 485,\n",
       " 'orders': 486,\n",
       " 'cause': 487,\n",
       " 'certain': 488,\n",
       " '422nd': 489,\n",
       " 'great': 490,\n",
       " 'distress': 491,\n",
       " 'One': 492,\n",
       " 'member': 493,\n",
       " 'becomes': 494,\n",
       " 'so': 495,\n",
       " 'enraged': 496,\n",
       " 'he': 497,\n",
       " 'abandons': 498,\n",
       " 'post': 499,\n",
       " 'defects': 500,\n",
       " 'ranks': 501,\n",
       " 'attached': 502,\n",
       " 'ideal': 503,\n",
       " 'independence': 504,\n",
       " 'proposed': 505,\n",
       " 'leader': 506,\n",
       " 'Dahau': 507,\n",
       " 'At': 508,\n",
       " 'within': 509,\n",
       " 'erase': 510,\n",
       " 'protect': 511,\n",
       " 'own': 512,\n",
       " 'interests': 513,\n",
       " 'allies': 514,\n",
       " 'enemies': 515,\n",
       " 'combined': 516,\n",
       " 'presence': 517,\n",
       " 'traitor': 518,\n",
       " 'desperately': 519,\n",
       " 'keep': 520,\n",
       " 'themselves': 521,\n",
       " 'alive': 522,\n",
       " 'effort': 523,\n",
       " 'This': 524,\n",
       " 'continues': 525,\n",
       " 'until': 526,\n",
       " 'commanding': 527,\n",
       " 'Ramsey': 528,\n",
       " 'Crowe': 529,\n",
       " 'had': 530,\n",
       " 'been': 531,\n",
       " 'kept': 532,\n",
       " 'under': 533,\n",
       " 'house': 534,\n",
       " 'arrest': 535,\n",
       " 'escorted': 536,\n",
       " 'capital': 537,\n",
       " 'city': 538,\n",
       " 'present': 539,\n",
       " 'evidence': 540,\n",
       " 'weary': 541,\n",
       " 'expose': 542,\n",
       " 'General': 543,\n",
       " 'Treason': 544,\n",
       " 'due': 545,\n",
       " 'events': 546,\n",
       " 'partly': 547,\n",
       " 'major': 548,\n",
       " 'losses': 549,\n",
       " 'manpower': 550,\n",
       " 'suffers': 551,\n",
       " 'towards': 552,\n",
       " 'end': 553,\n",
       " 'Empire': 554,\n",
       " 'offered': 555,\n",
       " 'formal': 556,\n",
       " 'position': 557,\n",
       " 'rather': 558,\n",
       " 'serve': 559,\n",
       " 'anonymous': 560,\n",
       " 'shadow': 561,\n",
       " 'force': 562,\n",
       " 'short': 563,\n",
       " 'lived': 564,\n",
       " 'however': 565,\n",
       " 'following': 566,\n",
       " 'Maximilian': 567,\n",
       " 'defeat': 568,\n",
       " 'ancient': 569,\n",
       " 'super': 570,\n",
       " 'benefactor': 571,\n",
       " 'Without': 572,\n",
       " 'support': 573,\n",
       " 'chance': 574,\n",
       " 'prove': 575,\n",
       " 'last': 576,\n",
       " 'card': 577,\n",
       " 'creating': 578,\n",
       " 'new': 579,\n",
       " 'armed': 580,\n",
       " 'invading': 581,\n",
       " 'just': 582,\n",
       " 'nations': 583,\n",
       " 'cease': 584,\n",
       " 'fire': 585,\n",
       " 'certainly': 586,\n",
       " 'wreck': 587,\n",
       " 'newfound': 588,\n",
       " 'peace': 589,\n",
       " 'decides': 590,\n",
       " 'again': 591,\n",
       " 'asking': 592,\n",
       " 'list': 593,\n",
       " 'all': 594,\n",
       " 'command': 595,\n",
       " 'killed': 596,\n",
       " 'action': 597,\n",
       " 'Now': 598,\n",
       " 'owing': 599,\n",
       " 'allegiance': 600,\n",
       " 'none': 601,\n",
       " 'confronts': 602,\n",
       " 'destroys': 603,\n",
       " 'then': 604,\n",
       " 'goes': 605,\n",
       " 'separate': 606,\n",
       " 'ways': 607,\n",
       " 'begin': 608,\n",
       " 'lives': 609,\n",
       " 'Development': 610,\n",
       " 'Concept': 611,\n",
       " 'after': 612,\n",
       " 'finished': 613,\n",
       " 'full': 614,\n",
       " 'beginning': 615,\n",
       " 'shortly': 616,\n",
       " 'took': 617,\n",
       " 'approximately': 618,\n",
       " 'staff': 619,\n",
       " 'look': 620,\n",
       " 'popular': 621,\n",
       " 'response': 622,\n",
       " 'what': 623,\n",
       " 'wanted': 624,\n",
       " 'next': 625,\n",
       " 'Like': 626,\n",
       " 'predecessor': 627,\n",
       " 'wanting': 628,\n",
       " 'refine': 629,\n",
       " 'mechanics': 630,\n",
       " 'created': 631,\n",
       " 'come': 632,\n",
       " 'revolutionary': 633,\n",
       " 'idea': 634,\n",
       " 'warrant': 635,\n",
       " 'entry': 636,\n",
       " 'Speaking': 637,\n",
       " 'interview': 638,\n",
       " 'stated': 639,\n",
       " 'considered': 640,\n",
       " 'true': 641,\n",
       " 'sequel': 642,\n",
       " 'required': 643,\n",
       " 'amount': 644,\n",
       " 'trial': 645,\n",
       " 'error': 646,\n",
       " 'platform': 647,\n",
       " 'gave': 648,\n",
       " 'improve': 649,\n",
       " 'upon': 650,\n",
       " 'best': 651,\n",
       " 'parts': 652,\n",
       " 'In': 653,\n",
       " 'addition': 654,\n",
       " 'scenario': 655,\n",
       " 'written': 656,\n",
       " 'Hiroyuki': 657,\n",
       " 'Its': 658,\n",
       " 'darker': 659,\n",
       " 'somber': 660,\n",
       " 'majority': 661,\n",
       " 'material': 662,\n",
       " 'design': 663,\n",
       " 'improvements': 664,\n",
       " 'were': 665,\n",
       " 'made': 666,\n",
       " 'graphics': 667,\n",
       " 'layouts': 668,\n",
       " 'structure': 669,\n",
       " 'number': 670,\n",
       " 'playable': 671,\n",
       " 'upgrade': 672,\n",
       " 'involved': 673,\n",
       " 'models': 674,\n",
       " 'body': 675,\n",
       " 'achieve': 676,\n",
       " 'cooperative': 677,\n",
       " 'incorporated': 678,\n",
       " 'second': 679,\n",
       " 'removed': 680,\n",
       " 'memory': 681,\n",
       " 'space': 682,\n",
       " 'needed': 683,\n",
       " 'adjusted': 684,\n",
       " 'settings': 685,\n",
       " 'ease': 686,\n",
       " 'play': 687,\n",
       " 'could': 688,\n",
       " 'appeal': 689,\n",
       " 'retaining': 690,\n",
       " 'essential': 691,\n",
       " 'components': 692,\n",
       " 'newer': 693,\n",
       " 'systems': 694,\n",
       " 'decided': 695,\n",
       " 'designs': 696,\n",
       " 'worked': 697,\n",
       " 'When': 698,\n",
       " 'faced': 699,\n",
       " 'problem': 700,\n",
       " 'uniforms': 701,\n",
       " 'essentially': 702,\n",
       " 'individuality': 703,\n",
       " 'despite': 704,\n",
       " 'him': 705,\n",
       " 'needing': 706,\n",
       " 'create': 707,\n",
       " 'identify': 708,\n",
       " 'maintaining': 709,\n",
       " 'sense': 710,\n",
       " 'reality': 711,\n",
       " 'world': 712,\n",
       " 'color': 713,\n",
       " 'engine': 714,\n",
       " 'anime': 715,\n",
       " 'produced': 716,\n",
       " 'Production': 717,\n",
       " 'I.G.': 718,\n",
       " 'Music': 719,\n",
       " 'music': 720,\n",
       " 'originally': 721,\n",
       " 'heard': 722,\n",
       " 'about': 723,\n",
       " 'project': 724,\n",
       " 'thought': 725,\n",
       " 'light': 726,\n",
       " 'tone': 727,\n",
       " 'similar': 728,\n",
       " 'themes': 729,\n",
       " 'much': 730,\n",
       " 'expected': 731,\n",
       " 'An': 732,\n",
       " 'designed': 733,\n",
       " 'vision': 734,\n",
       " 'rejected': 735,\n",
       " 'He': 736,\n",
       " 'seven': 737,\n",
       " 'production': 738,\n",
       " 'need': 739,\n",
       " 'initially': 740,\n",
       " 'recorded': 741,\n",
       " 'orchestra': 742,\n",
       " 'guitar': 743,\n",
       " 'bass': 744,\n",
       " 'synthesizer': 745,\n",
       " 'before': 746,\n",
       " 'segments': 747,\n",
       " 'piece': 748,\n",
       " 'incorporating': 749,\n",
       " 'hopeful': 750,\n",
       " 'tune': 751,\n",
       " 'played': 752,\n",
       " 'ending': 753,\n",
       " 'modern': 754,\n",
       " 'divorced': 755,\n",
       " 'fantasy': 756,\n",
       " 'musical': 757,\n",
       " 'instruments': 758,\n",
       " 'constructed': 759,\n",
       " 'working': 760,\n",
       " 'synthesized': 761,\n",
       " 'felt': 762,\n",
       " 'incorporate': 763,\n",
       " 'live': 764,\n",
       " 'arranged': 765,\n",
       " 'several': 766,\n",
       " 'later': 767,\n",
       " 'tracks': 768,\n",
       " 'song': 769,\n",
       " 'If': 770,\n",
       " 'You': 771,\n",
       " 'Wish': 772,\n",
       " '...': 773,\n",
       " 'Kimi': 774,\n",
       " 'singer': 775,\n",
       " 'reason': 776,\n",
       " 'fought': 777,\n",
       " 'particular': 778,\n",
       " 'wish': 779,\n",
       " 'precious': 780,\n",
       " 'responsibility': 781,\n",
       " 'duty': 782,\n",
       " 'lyrics': 783,\n",
       " 'singles': 784,\n",
       " 'Release': 785,\n",
       " 'September': 786,\n",
       " 'teaser': 787,\n",
       " 'website': 788,\n",
       " 'revealed': 789,\n",
       " 'hinting': 790,\n",
       " 'issue': 791,\n",
       " 'Famitsu': 792,\n",
       " 'listed': 793,\n",
       " 'arriving': 794,\n",
       " 'public': 795,\n",
       " 'appearance': 796,\n",
       " 'Tokyo': 797,\n",
       " 'Game': 798,\n",
       " 'Show': 799,\n",
       " 'TGS': 800,\n",
       " 'demo': 801,\n",
       " 'available': 802,\n",
       " 'journalists': 803,\n",
       " 'attendees': 804,\n",
       " 'publicity': 805,\n",
       " 'details': 806,\n",
       " 'too': 807,\n",
       " 'potential': 808,\n",
       " 'still': 809,\n",
       " 'flux': 810,\n",
       " 'reveal': 811,\n",
       " 'promote': 812,\n",
       " 'detail': 813,\n",
       " 'leading': 814,\n",
       " 'episodic': 815,\n",
       " 'Flash': 816,\n",
       " 'visual': 817,\n",
       " 'novel': 818,\n",
       " '27': 819,\n",
       " 'said': 820,\n",
       " 'capacity': 821,\n",
       " 'DLC': 822,\n",
       " 'plans': 823,\n",
       " 'finalized': 824,\n",
       " 'Multiple': 825,\n",
       " 'featuring': 826,\n",
       " 'between': 827,\n",
       " 'February': 828,\n",
       " 'April': 829,\n",
       " 'Extra': 830,\n",
       " 'Edition': 831,\n",
       " '23': 832,\n",
       " 'sold': 833,\n",
       " 'lower': 834,\n",
       " 'price': 835,\n",
       " 'chosen': 836,\n",
       " 'pre': 837,\n",
       " 'bonus': 838,\n",
       " 'People': 839,\n",
       " 'owned': 840,\n",
       " 'transfer': 841,\n",
       " 'save': 842,\n",
       " 'data': 843,\n",
       " 'versions': 844,\n",
       " 'Unlike': 845,\n",
       " 'west': 846,\n",
       " 'According': 847,\n",
       " 'poor': 848,\n",
       " 'general': 849,\n",
       " 'unpopularity': 850,\n",
       " 'PSP': 851,\n",
       " 'unofficial': 852,\n",
       " 'patch': 853,\n",
       " '2012': 854,\n",
       " 'copy': 855,\n",
       " 'download': 856,\n",
       " 'apply': 857,\n",
       " 'translated': 858,\n",
       " 'English': 859,\n",
       " 'Reception': 860,\n",
       " 'On': 861,\n",
       " 'day': 862,\n",
       " 'topped': 863,\n",
       " 'exclusive': 864,\n",
       " 'multi': 865,\n",
       " 'charts': 866,\n",
       " 'By': 867,\n",
       " '102': 868,\n",
       " '@,@': 869,\n",
       " 'coming': 870,\n",
       " 'overall': 871,\n",
       " 'Last': 872,\n",
       " 'Story': 873,\n",
       " 'Wii': 874,\n",
       " '152': 875,\n",
       " '500': 876,\n",
       " 'enjoyed': 877,\n",
       " 'particularly': 878,\n",
       " 'pleased': 879,\n",
       " 'gaming': 880,\n",
       " 'site': 881,\n",
       " 'Watch': 882,\n",
       " 'negatively': 883,\n",
       " 'noting': 884,\n",
       " 'pacing': 885,\n",
       " 'recycled': 886,\n",
       " 'generally': 887,\n",
       " 'entertaining': 888,\n",
       " 'putting': 889,\n",
       " 'spikes': 890,\n",
       " 'writer': 891,\n",
       " 'Play': 892,\n",
       " 'Test': 893,\n",
       " 'article': 894,\n",
       " 'provided': 895,\n",
       " 'profound': 896,\n",
       " 'feeling': 897,\n",
       " 'closure': 898,\n",
       " 'annoying': 899,\n",
       " 'limitations': 900,\n",
       " 'aspects': 901,\n",
       " 'special': 902,\n",
       " 'abilities': 903,\n",
       " 'positively': 904,\n",
       " 'noted': 905,\n",
       " 'Official': 906,\n",
       " 'Magazine': 907,\n",
       " '-': 908,\n",
       " 'UK': 909,\n",
       " 'moral': 910,\n",
       " 'standing': 911,\n",
       " 'art': 912,\n",
       " 'style': 913,\n",
       " 'latter': 914,\n",
       " 'continued': 915,\n",
       " 'quality': 916,\n",
       " 'tweaks': 917,\n",
       " 'balance': 918,\n",
       " 'criticism': 919,\n",
       " 'affected': 920,\n",
       " 'Heath': 921,\n",
       " 'Hindman': 922,\n",
       " 'non': 923,\n",
       " 'removal': 924,\n",
       " 'praising': 925,\n",
       " 'returning': 926,\n",
       " 'serious': 927,\n",
       " 'Points': 928,\n",
       " 'criticized': 929,\n",
       " 'review': 930,\n",
       " 'awkward': 931,\n",
       " 'cutscenes': 932,\n",
       " 'seemed': 933,\n",
       " 'include': 934,\n",
       " 'scene': 935,\n",
       " 'good': 936,\n",
       " 'issues': 937,\n",
       " 'occasional': 938,\n",
       " 'problems': 939,\n",
       " 'AI': 940,\n",
       " 'preview': 941,\n",
       " 'Ryan': 942,\n",
       " 'Geddes': 943,\n",
       " 'IGN': 944,\n",
       " 'left': 945,\n",
       " 'excited': 946,\n",
       " 'go': 947,\n",
       " 'completing': 948,\n",
       " 'enjoying': 949,\n",
       " 'improved': 950,\n",
       " 'visuals': 951,\n",
       " 'Kotaku': 952,\n",
       " 'Richard': 953,\n",
       " 'highly': 954,\n",
       " 'citing': 955,\n",
       " 'form': 956,\n",
       " 'His': 957,\n",
       " 'criticisms': 958,\n",
       " 'length': 959,\n",
       " 'repetition': 960,\n",
       " 'expressing': 961,\n",
       " 'regret': 962,\n",
       " 'Legacy': 963,\n",
       " 'featured': 964,\n",
       " 'Nintendo': 965,\n",
       " '3DS': 966,\n",
       " 'crossover': 967,\n",
       " 'Project': 968,\n",
       " 'X': 969,\n",
       " 'Zone': 970,\n",
       " 'representing': 971,\n",
       " 'develop': 972,\n",
       " 'forms': 973,\n",
       " 'Adaptations': 974,\n",
       " 'episode': 975,\n",
       " 'Taken': 976,\n",
       " 'Sake': 977,\n",
       " 'Network': 978,\n",
       " 'planned': 979,\n",
       " 'availability': 980,\n",
       " 'period': 981,\n",
       " 'extended': 982,\n",
       " 'stoppage': 983,\n",
       " 'summer': 984,\n",
       " 'DVD': 985,\n",
       " 'June': 986,\n",
       " '29': 987,\n",
       " 'August': 988,\n",
       " '31': 989,\n",
       " 'Black': 990,\n",
       " 'Blue': 991,\n",
       " 'editions': 992,\n",
       " 'purchase': 993,\n",
       " 'set': 994,\n",
       " 'half': 995,\n",
       " 'detailing': 996,\n",
       " 'rivals': 997,\n",
       " 'announced': 998,\n",
       " '1': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_corpus.dictionary.word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = original_corpus.dictionary.word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "995"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx[\"half\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'will god let her fall or give her a new path \\\\? <eos>'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0] + ' <eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns the index of the word or the index of \"UNK\" otherwise\n",
    "word2idx.get(\"will\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 301, 5011, 7292,  362, 6357,  311, 2194,  362,   28,  579, 9429,\n",
       "          0])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([word2idx.get(w, 0) for w in X_train[0].split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(s, word2idx=word2idx, N=35):\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([word2idx.get(w, 0) for w in s.split() + [\"<eos>\"]])\n",
    "    l = min(N, len(enc1))\n",
    "    enc[:l] = enc1[:l]\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 301, 5011, 7292,  362, 6357,  311, 2194,  362,   28,  579, 9429,\n",
       "          0,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0], dtype=int32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sentence(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 35)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.vstack([encode_sentence(x) for x in X_train])\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 35)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = np.vstack([encode_sentence(x) for x in X_test])\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning from language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "nemb = 300\n",
    "nhid = 300\n",
    "nlayers = 2\n",
    "ntokens = 33279\n",
    "\n",
    "model_path = \"/data/yinterian/wikitext-2/mode117.pth\"\n",
    "def load_model(m, p):\n",
    "    m.load_state_dict(torch.load(p))\n",
    "\n",
    "class NetLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetLM, self).__init__()\n",
    "        self.LM = RNNModel(ntokens, nemb, nhid, nlayers).cuda()\n",
    "        load_model(self.LM, model_path)\n",
    "        self.nhid = nhid\n",
    "        \n",
    "        for param in self.LM.parameters():\n",
    "            param.requires_grad = False\n",
    "        # new layers to get a classifier\n",
    "        self.linear1 = nn.Linear(nhid, 100) # binary classification\n",
    "        self.linear2 = nn.Linear(100, 1)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.bn = nn.BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True)\n",
    "   \n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.LM.drop(self.LM.encoder(input))\n",
    "        output, hidden = self.LM.rnn(emb, hidden)\n",
    "        out = self.drop(F.relu(self.linear1(hidden)))\n",
    "        out = self.bn(out)\n",
    "        return self.linear2(out)\n",
    "\n",
    "    def init_hidden(bash_size):\n",
    "        # variable of size [num_layers*num_directions, b_sz, hidden_sz]\n",
    "        return Variable(torch.zeros(self.num_directions, batch_size, self.hidden_size)).cuda() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.from_numpy(x_train)).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I am not bodering with mini-batches since our dataset is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NetLM().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = filter(lambda p: p.requires_grad, model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful to check the parameters\n",
    "#list(parameters) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this filters parameters with p.requires_grad=True\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, epochs=10, lr=0.01):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        x = Variable(torch.from_numpy(x_train)).cuda()\n",
    "        y = Variable(torch.from_numpy(y_train)).cuda().unsqueeze(1)\n",
    "        y_hat = model(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss.data[0])\n",
    "    test_metrics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_metrics(m):\n",
    "    model.eval()\n",
    "    x = Variable(torch.from_numpy(x_test)).cuda()\n",
    "    y = Variable(torch.from_numpy(y_test)).cuda().unsqueeze(1)\n",
    "    y_hat = m(x)\n",
    "    loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "    y_pred = y_hat > 0\n",
    "    correct = (y_pred.float() == y).float().sum()\n",
    "    accuracy = correct/pred.shape[0]\n",
    "    print(\"test loss %.3f and accuracy %.3f\" % (loss.data[0], accuracy.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'hidden'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-48a22289249d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_epocs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-104-3120740aac7f>\u001b[0m in \u001b[0;36mtrain_epocs\u001b[0;34m(model, epochs, lr)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'hidden'"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34370580315589905\n",
      "0.5294861793518066\n",
      "0.32328617572784424\n",
      "0.40581214427948\n",
      "0.2806266248226166\n",
      "0.2568565309047699\n",
      "0.30520763993263245\n",
      "0.29174017906188965\n",
      "0.24017520248889923\n",
      "0.22176489233970642\n",
      "test loss 0.285 and accuracy 0.892\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23691332340240479\n",
      "0.2138260155916214\n",
      "0.20660893619060516\n",
      "0.20420822501182556\n",
      "0.20270921289920807\n",
      "0.20497538149356842\n",
      "0.19893155992031097\n",
      "0.1915552318096161\n",
      "0.19009748101234436\n",
      "0.18447645008563995\n",
      "test loss 0.246 and accuracy 0.898\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([100, 300, 3]), torch.Size([100]), torch.Size([100, 300, 4]), torch.Size([100]), torch.Size([100, 300, 5]), torch.Size([100]), torch.Size([1, 300]), torch.Size([1])]\n"
     ]
    }
   ],
   "source": [
    "# how to figure out the parameters\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "print([p.size() for p in parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreezing the embeddings\n",
    "model.embedding.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([400007, 300]), torch.Size([100, 300, 3]), torch.Size([100]), torch.Size([100, 300, 4]), torch.Size([100]), torch.Size([100, 300, 5]), torch.Size([100]), torch.Size([1, 300]), torch.Size([1])]\n"
     ]
    }
   ],
   "source": [
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "print([p.size() for p in parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1830516755580902\n",
      "0.17343704402446747\n",
      "0.16393160820007324\n",
      "0.15581797063350677\n",
      "0.14433661103248596\n",
      "0.1368999481201172\n",
      "0.1289738416671753\n",
      "0.12384654581546783\n",
      "0.11491061747074127\n",
      "0.1075991541147232\n",
      "test loss 0.228 and accuracy 0.913\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10160469263792038\n",
      "0.10249238461256027\n",
      "0.10043996572494507\n",
      "0.10032773017883301\n",
      "0.09892096370458603\n",
      "0.09907413274049759\n",
      "0.09923809766769409\n",
      "0.09644830971956253\n",
      "0.09582842141389847\n",
      "0.0971798449754715\n",
      "test loss 0.227 and accuracy 0.912\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Show how to save model\n",
    "* Show how to predict on new data\n",
    "* Test a version with a smaller word embedding matrix\n",
    "* Try Another tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You may not need to keep all word embeddings.\n",
    "* Extend this code by finetunning the embedding layer.\n",
    "* Use fasttext instead of globe model. (https://fasttext.cc/docs/en/english-vectors.html)\n",
    "\n",
    "   `! pip install git+https://github.com/facebookresearch/fastText.git`\n",
    "* Extend this code to do cross-validation. Look at https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py for an example on how to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Language model from here https://github.com/pytorch/examples/blob/master/word_language_model/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
