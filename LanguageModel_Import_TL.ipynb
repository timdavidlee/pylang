{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Labeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import _pickle as pickle\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/paperspace/data/wikitext/wikitext-2/wiki.train.tokens'),\n",
       " PosixPath('/home/paperspace/data/wikitext/wikitext-2/mode117.pth'),\n",
       " PosixPath('/home/paperspace/data/wikitext/wikitext-2/wiki.valid.tokens'),\n",
       " PosixPath('/home/paperspace/data/wikitext/wikitext-2/sample.txt'),\n",
       " PosixPath('/home/paperspace/data/wikitext/wikitext-2/wiki.test.tokens')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_LM=Path(\"/home/paperspace/data/wikitext/wikitext-2\")\n",
    "list(PATH_LM.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained Word to ID mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('UNK', 0), ('<eos>', 1), ('=', 2), ('Valkyria', 3), ('Chronicles', 4), ('III', 5), ('Senjō', 6), ('no', 7), ('3', 8), (':', 9)]\n",
      "['UNK', '<eos>', '=', 'Valkyria', 'Chronicles', 'III', 'Senjō', 'no', '3', ':']\n"
     ]
    }
   ],
   "source": [
    "# load pretrained dictionary mapping\n",
    "with open('dict17.pkl', 'rb') as f:\n",
    "    pretrn_word2idx, pretrn_idx2word = pickle.load(f)\n",
    "print(list(pretrn_word2idx.items())[:10])\n",
    "print(list(pretrn_idx2word)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/paperspace/data/rotten_imdb/glove.6B.100d.txt'),\n",
       " PosixPath('/home/paperspace/data/rotten_imdb/quote.tok.gt9.5000'),\n",
       " PosixPath('/home/paperspace/data/rotten_imdb/glove.6B.50d.txt'),\n",
       " PosixPath('/home/paperspace/data/rotten_imdb/plot.tok.gt9.5000'),\n",
       " PosixPath('/home/paperspace/data/rotten_imdb/glove.6B.200d.txt'),\n",
       " PosixPath('/home/paperspace/data/rotten_imdb/rotten_imdb.tar.gz'),\n",
       " PosixPath('/home/paperspace/data/rotten_imdb/subjdata.README.1.0'),\n",
       " PosixPath('/home/paperspace/data/rotten_imdb/glove.6B.300d.txt'),\n",
       " PosixPath('/home/paperspace/data/rotten_imdb/rotten_imdb.tar')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "PATH = Path(\"/home/paperspace/data/rotten_imdb/\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Every dataset is lower cased except for TREC\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def read_file(path):\n",
    "    \"\"\" Read file returns a shuttled list.\n",
    "    \"\"\"\n",
    "    with open(path, encoding = \"ISO-8859-1\") as f:\n",
    "        content = np.array(f.readlines())\n",
    "    return content\n",
    "\n",
    "def get_vocab(list_of_content):\n",
    "    \"\"\"Computes Dict of counts of words.\n",
    "    \n",
    "    Computes the number of times a word is on a document.\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(float)\n",
    "    for content in list_of_content:\n",
    "        for line in content:\n",
    "            line = clean_str(line.strip())\n",
    "            words = set(line.split())\n",
    "            for word in words:\n",
    "                vocab[word] += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Data Prep XY\n",
    "# ======================================================\n",
    "\n",
    "def make_XY():\n",
    "    \"\"\"\n",
    "    Load the subjective / objective dataset\n",
    "    \"\"\"\n",
    "    sub_content = read_file(PATH/\"quote.tok.gt9.5000\")\n",
    "    obj_content = read_file(PATH/\"plot.tok.gt9.5000\")\n",
    "    sub_content = np.array([clean_str(line.strip()) for line in sub_content])\n",
    "    obj_content = np.array([clean_str(line.strip()) for line in obj_content])\n",
    "    sub_y = np.zeros(len(sub_content))\n",
    "    obj_y = np.ones(len(obj_content))\n",
    "    X = np.append(sub_content, obj_content)\n",
    "    y = np.append(sub_y, obj_y)\n",
    "    return X,y\n",
    "\n",
    "\n",
    "def make_train_val(X,y):\n",
    "    \"\"\"\n",
    "    Train and test split\n",
    "    \"\"\"\n",
    "    X_tr, X_vl, y_tr, y_vl = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X_tr, X_vl, y_tr, y_vl\n",
    "\n",
    "\n",
    "def encode_sentence(s, word2idx, N=35):\n",
    "    \"\"\"\n",
    "    Takes in a long text and encodes it with dictionary\n",
    "    then makes vectors of sized N.\n",
    "    \"\"\"\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([word2idx.get(w, 0) for w in s.split() + [\"<eos>\"]])\n",
    "    l = min(N, len(enc1))\n",
    "    enc[:l] = enc1[:l]\n",
    "    return enc\n",
    "\n",
    "def encode_sent_array(list_of_sentences, word2idx, N=35):\n",
    "    return np.vstack([encode_sentence(sent, word2idx, N) for sent in list_of_sentences])\n",
    "\n",
    "# ======================================================\n",
    "# LM Model for reference\n",
    "# ======================================================\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Container module with an encoder, a recurrent module, and a decoder.\n",
    "    \n",
    "    ntoken: number of tokens\n",
    "    ninp: number of inputs\n",
    "    nhid: number of hidden units\n",
    "    nlayers: number of layers\n",
    "    dropout: % dropout\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        self.init_weights()\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range = 0.1\n",
    "        self.encoder.weight.data.uniform_(-init_range, init_range)\n",
    "        self.decoder.bias.data.fill_(0.0)\n",
    "        self.decoder.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\"\n",
    "        input: current input\n",
    "        hidden: hidden state from the previous step\n",
    "        \"\"\"\n",
    "        # pulls the embeddings for the input submitted\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        \n",
    "        # then applies the RNN against the embedding layer\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        \"\"\"\n",
    "        Initialize the hidden weights\n",
    "        \"\"\"\n",
    "        weight = next(self.parameters()).data\n",
    "        return Variable(weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "\n",
    "# ======================================================\n",
    "# Our RNN Classifier that imports the RNN model\n",
    "# ======================================================\n",
    "\n",
    "def load_model(m, p):\n",
    "    m.load_state_dict(torch.load(p))\n",
    "\n",
    "class NetLM(nn.Module):\n",
    "    def __init__(self, model_path, ntokens, nemb, nhid, nlayers, bsz, bidir=False):\n",
    "        super(NetLM, self).__init__()\n",
    "        \n",
    "        # if bidirectional is applied, (forward and backward)\n",
    "        # otherwise its a single forward pass\n",
    "        self.ndir = 2 if bidir else 1\n",
    "        self.nlayers = nlayers  \n",
    "        self.nemb = nemb\n",
    "        self.bsz = bsz\n",
    "        self.LM = RNNModel(ntokens, nemb, nhid, nlayers).cuda()\n",
    "        load_model(self.LM, model_path)\n",
    "            \n",
    "        self.nhid = nhid\n",
    "        \n",
    "        # freeze the RNN        \n",
    "        for param in self.LM.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.linear1 = nn.Linear(nhid*3, 100) # binary classification\n",
    "        self.linear2 = nn.Linear(100, 1)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.bn = nn.BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True)\n",
    "   \n",
    "    def forward(self, input, hidden):\n",
    "        bz = input.shape[1]\n",
    "        emb = self.LM.drop(self.LM.encoder(input))\n",
    "        output, hidden = self.LM.rnn(emb, hidden)\n",
    "        # create concat pooling \n",
    "        out_avg = F.adaptive_avg_pool1d(output.permute(1,2,0), (1,)).view(bz,-1)\n",
    "        out_max = F.adaptive_max_pool1d(output.permute(1,2,0), (1,)).view(bz,-1)\n",
    "        out = torch.cat([output[-1], out_avg, out_max], dim=1)\n",
    "        \n",
    "        out = self.drop(F.relu(self.linear1(out)))\n",
    "        out = self.bn(out)\n",
    "        return self.linear2(out), hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # variable of size [num_layers*num_directions, b_sz, hidden_sz]\n",
    "        return Variable(torch.zeros(self.ndir * self.nlayers, batch_size, self.nhid)).cuda()\n",
    "    \n",
    "# ====================================================================\n",
    "# Training functions\n",
    "# ====================================================================\n",
    "\n",
    "def train_epocs(model, x_train, y_train, x_test, y_test, epochs=10, lr=0.01):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    \n",
    "    # assume the data input size is the following\n",
    "    # [bptt, batch_size, embedding size]\n",
    "    # will need to rearrange some of the dimensions\n",
    "\n",
    "    b_sz = x_train.shape[0]\n",
    "    print(b_sz)\n",
    "    model.train()\n",
    "    hidden = model.init_hidden(b_sz)\n",
    "    for i in range(epochs):\n",
    "        # wrap the data in variables\n",
    "        x = Variable(torch.from_numpy(x_train)).long().cuda()\n",
    "        x = x.permute(1,0)\n",
    "\n",
    "        y = Variable(torch.from_numpy(y_train)).float().cuda().unsqueeze(1)\n",
    "        \n",
    "        # wrap hidden states for the model\n",
    "        hidden = Variable(hidden.data)\n",
    "        \n",
    "        # pass our phrase through the model\n",
    "        # get the updated hidden state\n",
    "        y_hat, hidden = model(x, hidden)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        optimizer.step()\n",
    "        print(loss.data[0])\n",
    "    test_metrics(model, x_test, y_test, hidden)\n",
    "\n",
    "\n",
    "def test_metrics(m, x_test, y_test, hidden):\n",
    "    m.eval()\n",
    "    b_sz = x_test.shape[0]\n",
    "    hidden = model.init_hidden(b_sz)\n",
    "    x = Variable(torch.from_numpy(x_test)).long().cuda()\n",
    "    x = x.permute(1,0)\n",
    "    y = Variable(torch.from_numpy(y_test)).float().cuda().unsqueeze(1)\n",
    "    y_hat, hidden = m(x, hidden)\n",
    "    print(type(y_hat.data), type(y.data))\n",
    "    loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "    y_pred = y_hat > 0\n",
    "    correct = (y_pred.float() == y).float().sum()\n",
    "    accuracy = correct/y_pred.shape[0]\n",
    "    print(\"test loss %.3f and accuracy %.3f\" % (loss.data[0], accuracy.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your dataset\n",
    "X,y = make_XY()\n",
    "\n",
    "# train test split\n",
    "X_tr, X_vl, y_tr, y_vl = make_train_val(X,y)\n",
    "\n",
    "# encode into numeric\n",
    "X_tr_enc = encode_sent_array(X_tr, pretrn_word2idx)\n",
    "X_vl_enc = encode_sent_array(X_vl, pretrn_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will god let her fall or give her a new path \\?\n",
      "[ 301 5011 7292  362 6357  311 2194  362   28  579 9429    0    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0]\n",
      "will god let her fall or give her a new path UNK <eos> UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n"
     ]
    }
   ],
   "source": [
    "# sample encoding / decoding\n",
    "print(X_tr[0])\n",
    "print(X_tr_enc[0])\n",
    "print(' '.join([pretrn_idx2word[idx] for idx in X_tr_enc[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup RNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = dict(nemb = 300,\n",
    "                    nhid = 300,\n",
    "                    nlayers = 2,\n",
    "                    ntokens = 33279,\n",
    "                    bsz = X_tr_enc.shape[0]\n",
    "                   )\n",
    "\n",
    "model = NetLM(PATH_LM/'mode117.pth', **model_params).cuda()\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "0.7100933194160461\n",
      "0.7103177309036255\n",
      "0.7077656388282776\n",
      "0.7098127603530884\n",
      "0.7107676863670349\n",
      "0.7076292634010315\n",
      "0.7083832621574402\n",
      "0.7107159495353699\n",
      "0.7098456025123596\n",
      "0.7109596729278564\n",
      "<class 'torch.cuda.FloatTensor'> <class 'torch.cuda.FloatTensor'>\n",
      "test loss 0.695 and accuracy 0.500\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, X_tr_enc, y_tr, X_vl_enc, y_vl,  epochs=10, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " -5.3440e-02  2.0715e-02  9.1994e-02  ...  -6.1571e-02  5.3499e-02 -4.3861e-02\n",
       "  1.0783e-02  1.7796e-01  9.4854e-04  ...   1.5818e-01 -7.0780e-02  7.3512e-05\n",
       " -4.4077e-02  1.1453e-01  1.2156e-02  ...   1.2476e-01 -2.0462e-02 -1.5806e-01\n",
       "                 ...                   ⋱                   ...                \n",
       "  2.7101e-02  5.4215e-02 -1.7806e-03  ...   1.0721e-01 -2.6214e-04  6.4145e-02\n",
       " -3.3080e-02 -5.3587e-02 -6.5689e-02  ...   7.5485e-02  1.7420e-03  2.6622e-02\n",
       "  6.3480e-02  5.1375e-02 -1.6410e-02  ...   5.1402e-02 -2.6295e-03  1.9764e-04\n",
       " [torch.cuda.FloatTensor of size 33279x300 (GPU 0)], Parameter containing:\n",
       "  5.1592e-02 -6.2415e-02  2.1826e-02  ...   2.1090e-01  5.9799e-02  4.1257e-02\n",
       "  1.1713e-01 -2.8393e-02 -6.8886e-02  ...   9.5341e-02  4.1305e-02  6.0903e-02\n",
       "  2.2183e-01 -5.5019e-02 -2.8827e-02  ...   4.6756e-02 -4.2940e-02 -1.0566e-01\n",
       "                 ...                   ⋱                   ...                \n",
       " -1.5925e-02  2.0011e-01 -9.4440e-02  ...   5.6875e-02 -3.6375e-01  1.9999e-01\n",
       "  1.7932e-01 -1.8354e-01 -5.1942e-02  ...   1.4203e-01  1.3493e-01 -7.1987e-02\n",
       " -1.3520e-01  1.2347e-01 -3.0844e-01  ...   1.0174e-02  1.8009e-01 -1.3053e-01\n",
       " [torch.cuda.FloatTensor of size 900x300 (GPU 0)], Parameter containing:\n",
       "  5.7474e-02 -1.9012e-01  6.8524e-02  ...  -4.6800e-02  5.2235e-03  2.3627e-02\n",
       "  1.3195e-01  2.3329e-01 -4.3816e-02  ...  -4.9299e-02  1.2343e-02 -1.0945e-01\n",
       "  9.6117e-02  1.4828e-01 -2.5686e-01  ...   1.8095e-03  7.7510e-02 -6.2928e-02\n",
       "                 ...                   ⋱                   ...                \n",
       " -1.1006e-02 -4.8714e-02  2.5754e-02  ...  -2.4690e+00  2.1207e-02  1.0153e-01\n",
       " -2.5355e-03 -1.0219e-01 -1.4360e-02  ...  -6.1403e-02 -2.1017e+00  1.1367e-01\n",
       " -3.1723e-04 -7.7257e-02  1.3980e-03  ...  -7.8801e-02 -6.9952e-02 -2.2611e+00\n",
       " [torch.cuda.FloatTensor of size 900x300 (GPU 0)], Parameter containing:\n",
       " -0.2236\n",
       " -0.3316\n",
       " -0.3606\n",
       " -0.2075\n",
       " -0.0871\n",
       " -0.2772\n",
       " -0.4477\n",
       " -0.4305\n",
       " -0.3754\n",
       " -0.1581\n",
       " -0.3365\n",
       " -0.4432\n",
       " -0.4294\n",
       " -0.0868\n",
       " -0.2500\n",
       " -0.4418\n",
       " -0.1917\n",
       " -0.5080\n",
       " -0.4264\n",
       " -0.2189\n",
       " -0.3045\n",
       " -0.3956\n",
       " -0.4127\n",
       " -0.2409\n",
       " -0.3918\n",
       " -0.1009\n",
       " -0.0556\n",
       " -0.0698\n",
       " -0.3015\n",
       " -0.4212\n",
       " -0.2257\n",
       " -0.3047\n",
       " -0.3220\n",
       " -0.3318\n",
       " -0.3675\n",
       " -0.2751\n",
       " -0.2445\n",
       " -0.1306\n",
       " -0.2409\n",
       " -0.2739\n",
       " -0.3321\n",
       " -0.2440\n",
       " -0.1472\n",
       " -0.2705\n",
       " -0.1105\n",
       " -0.4290\n",
       " -0.1290\n",
       " -0.3469\n",
       " -0.4087\n",
       " -0.0675\n",
       " -0.2888\n",
       " -0.3631\n",
       " -0.4150\n",
       " -0.4649\n",
       " -0.3554\n",
       " -0.3297\n",
       " -0.3249\n",
       " -0.3205\n",
       " -0.2826\n",
       " -0.4026\n",
       " -0.2113\n",
       " -0.1256\n",
       " -0.4470\n",
       " -0.4117\n",
       " -0.5997\n",
       " -0.2590\n",
       " -0.3364\n",
       " -0.1270\n",
       " -0.4879\n",
       " -0.3225\n",
       " -0.1586\n",
       " -0.2568\n",
       " -0.3414\n",
       " -0.2180\n",
       " -0.1895\n",
       " -0.3239\n",
       " -0.1308\n",
       " -0.3043\n",
       " -0.4618\n",
       " -0.3998\n",
       " -0.3451\n",
       " -0.3504\n",
       " -0.1710\n",
       " -0.4495\n",
       " -0.1041\n",
       " -0.4061\n",
       " -0.2634\n",
       " -0.2929\n",
       " -0.4011\n",
       " -0.3304\n",
       " -0.2380\n",
       " -0.2285\n",
       " -0.2794\n",
       " -0.2079\n",
       " -0.4411\n",
       " -0.3796\n",
       " -0.3299\n",
       " -0.2330\n",
       " -0.4860\n",
       " -0.1878\n",
       " -0.3048\n",
       " -0.2664\n",
       " -0.2367\n",
       " -0.6674\n",
       " -0.3402\n",
       " -0.2865\n",
       " -0.4128\n",
       " -0.4746\n",
       " -0.3670\n",
       " -0.2539\n",
       " -0.4098\n",
       " -0.1441\n",
       " -0.5593\n",
       " -0.4192\n",
       " -0.2713\n",
       " -0.4765\n",
       " -0.5356\n",
       " -0.3728\n",
       " -0.1805\n",
       " -0.1930\n",
       " -0.2540\n",
       "  0.0291\n",
       " -0.2054\n",
       " -0.1660\n",
       " -0.2448\n",
       " -0.2090\n",
       " -0.1777\n",
       " -0.3797\n",
       " -0.2493\n",
       " -0.3715\n",
       " -0.3889\n",
       " -0.3684\n",
       " -0.2725\n",
       " -0.4075\n",
       " -0.3690\n",
       " -0.2218\n",
       " -0.4395\n",
       " -0.0428\n",
       " -0.2785\n",
       " -0.3307\n",
       " -0.1433\n",
       " -0.4081\n",
       " -0.2630\n",
       " -0.2694\n",
       " -0.2474\n",
       " -0.3276\n",
       " -0.2143\n",
       " -0.3613\n",
       " -0.4009\n",
       " -0.0900\n",
       " -0.4818\n",
       " -0.4412\n",
       " -0.1771\n",
       " -0.3420\n",
       " -0.4013\n",
       " -0.2810\n",
       " -0.2315\n",
       " -0.4832\n",
       " -0.1491\n",
       " -0.5180\n",
       " -0.0300\n",
       " -0.4481\n",
       " -0.4666\n",
       " -0.2794\n",
       " -0.1560\n",
       " -0.3300\n",
       " -0.2019\n",
       " -0.2299\n",
       " -0.2969\n",
       " -0.1793\n",
       " -0.2766\n",
       " -0.3426\n",
       " -0.2414\n",
       " -0.2737\n",
       " -0.2669\n",
       " -0.2278\n",
       " -0.1943\n",
       " -0.1959\n",
       " -0.2904\n",
       " -0.2199\n",
       " -0.4727\n",
       " -0.1818\n",
       " -0.2036\n",
       " -0.2154\n",
       " -0.1874\n",
       " -0.4977\n",
       " -0.2154\n",
       " -0.3134\n",
       " -0.3279\n",
       " -0.2568\n",
       " -0.1904\n",
       " -0.4136\n",
       " -0.3236\n",
       " -0.3548\n",
       " -0.2757\n",
       " -0.1736\n",
       " -0.1847\n",
       " -0.5532\n",
       " -0.1855\n",
       " -0.2454\n",
       " -0.2640\n",
       " -0.1783\n",
       " -0.0727\n",
       " -0.3241\n",
       " -0.0201\n",
       " -0.2724\n",
       " -0.0437\n",
       " -0.1210\n",
       " -0.2298\n",
       " -0.4473\n",
       " -0.3265\n",
       " -0.2053\n",
       " -0.3193\n",
       " -0.1414\n",
       " -0.2899\n",
       " -0.2996\n",
       " -0.1934\n",
       " -0.2435\n",
       " -0.2244\n",
       " -0.1506\n",
       " -0.1590\n",
       " -0.3096\n",
       " -0.2776\n",
       " -0.1843\n",
       " -0.3717\n",
       " -0.4045\n",
       " -0.2888\n",
       " -0.5049\n",
       " -0.2088\n",
       " -0.3558\n",
       " -0.8034\n",
       " -0.2735\n",
       " -0.3006\n",
       " -0.2095\n",
       " -0.7631\n",
       " -0.3151\n",
       " -0.4335\n",
       " -0.2128\n",
       " -0.2851\n",
       " -0.1224\n",
       " -0.3256\n",
       " -0.3745\n",
       " -0.0860\n",
       " -0.5704\n",
       " -0.3183\n",
       " -0.3918\n",
       " -0.5596\n",
       " -0.0474\n",
       " -0.4820\n",
       "  0.0855\n",
       " -0.3327\n",
       " -0.1776\n",
       " -0.1135\n",
       " -0.3416\n",
       " -0.4257\n",
       " -0.2096\n",
       " -0.2321\n",
       " -0.2699\n",
       " -0.2345\n",
       " -0.6001\n",
       " -0.1638\n",
       " -0.1759\n",
       " -0.2643\n",
       " -0.4129\n",
       " -0.4528\n",
       " -0.0385\n",
       " -0.3923\n",
       " -0.1389\n",
       " -0.3167\n",
       " -0.3116\n",
       " -0.2565\n",
       " -0.7136\n",
       " -0.2583\n",
       " -0.3731\n",
       " -0.4768\n",
       " -0.2674\n",
       " -0.2663\n",
       " -0.2527\n",
       " -0.2938\n",
       " -0.4254\n",
       " -0.3986\n",
       " -0.2681\n",
       " -0.4783\n",
       " -0.2642\n",
       " -0.1968\n",
       " -0.2918\n",
       " -0.3414\n",
       " -0.2453\n",
       " -0.3339\n",
       " -0.1412\n",
       " -0.3203\n",
       " -0.3129\n",
       " -0.4588\n",
       " -0.2234\n",
       " -0.2358\n",
       " -0.2194\n",
       " -0.2875\n",
       " -0.1163\n",
       " -0.3087\n",
       " -0.4928\n",
       "  0.2669\n",
       "  0.1043\n",
       "  0.1771\n",
       "  0.3033\n",
       "  0.4453\n",
       "  0.2625\n",
       "  0.3581\n",
       "  0.1245\n",
       "  0.5208\n",
       "  0.3785\n",
       "  0.2052\n",
       "  0.3003\n",
       "  0.2221\n",
       "  0.4373\n",
       "  0.2081\n",
       "  0.1029\n",
       "  0.3824\n",
       "  0.2529\n",
       "  0.2843\n",
       "  0.1959\n",
       "  0.2457\n",
       "  0.3389\n",
       "  0.2744\n",
       "  0.3475\n",
       "  0.2121\n",
       "  0.2882\n",
       "  0.3962\n",
       "  0.3191\n",
       "  0.1690\n",
       "  0.1881\n",
       "  0.2888\n",
       "  0.1926\n",
       "  0.3362\n",
       "  0.2215\n",
       "  0.3243\n",
       "  0.2009\n",
       "  0.3376\n",
       "  0.4300\n",
       "  0.3817\n",
       "  0.2351\n",
       "  0.1521\n",
       "  0.3086\n",
       "  0.3333\n",
       "  0.1681\n",
       "  0.4134\n",
       "  0.1783\n",
       "  0.3318\n",
       "  0.4012\n",
       "  0.4004\n",
       "  0.5111\n",
       "  0.2273\n",
       "  0.4006\n",
       "  0.2981\n",
       "  0.1830\n",
       "  0.1018\n",
       "  0.1730\n",
       "  0.2886\n",
       "  0.1731\n",
       "  0.3376\n",
       "  0.2765\n",
       "  0.4530\n",
       "  0.4318\n",
       "  0.2133\n",
       "  0.1587\n",
       "  0.2084\n",
       "  0.2921\n",
       "  0.1157\n",
       "  0.3209\n",
       "  0.2163\n",
       "  0.3454\n",
       "  0.3404\n",
       "  0.2799\n",
       "  0.1920\n",
       "  0.2708\n",
       "  0.3833\n",
       "  0.3710\n",
       "  0.5407\n",
       "  0.1022\n",
       "  0.1717\n",
       "  0.2016\n",
       "  0.2691\n",
       "  0.3633\n",
       "  0.2783\n",
       "  0.2951\n",
       "  0.3464\n",
       "  0.3072\n",
       "  0.2775\n",
       "  0.3349\n",
       "  0.1841\n",
       "  0.2302\n",
       "  0.1947\n",
       "  0.5417\n",
       "  0.2970\n",
       "  0.3790\n",
       "  0.3587\n",
       "  0.2024\n",
       "  0.2704\n",
       "  0.2983\n",
       "  0.3488\n",
       "  0.3550\n",
       "  0.3239\n",
       "  0.3254\n",
       "  0.4680\n",
       "  0.1450\n",
       "  0.2850\n",
       "  0.3970\n",
       "  0.2564\n",
       "  0.1227\n",
       "  0.1521\n",
       "  0.2243\n",
       "  0.3445\n",
       "  0.4853\n",
       "  0.0554\n",
       "  0.3443\n",
       "  0.2083\n",
       "  0.1351\n",
       "  0.2260\n",
       "  0.3215\n",
       "  0.2695\n",
       "  0.3942\n",
       "  0.2612\n",
       "  0.4561\n",
       "  0.2208\n",
       "  0.3963\n",
       "  0.2171\n",
       "  0.2075\n",
       "  0.2451\n",
       "  0.2404\n",
       "  0.3340\n",
       "  0.3079\n",
       "  0.3869\n",
       "  0.3766\n",
       "  0.3509\n",
       "  0.3249\n",
       "  0.1740\n",
       "  0.3265\n",
       "  0.1874\n",
       "  0.3914\n",
       "  0.2861\n",
       "  0.4852\n",
       "  0.3190\n",
       "  0.2989\n",
       "  0.3351\n",
       "  0.1890\n",
       "  0.3214\n",
       "  0.3490\n",
       "  0.3234\n",
       "  0.2046\n",
       "  0.2081\n",
       "  0.3667\n",
       "  0.2195\n",
       "  0.2988\n",
       "  0.4404\n",
       "  0.3444\n",
       "  0.1698\n",
       "  0.2584\n",
       "  0.2380\n",
       "  0.1353\n",
       "  0.4598\n",
       "  0.0631\n",
       "  0.4528\n",
       "  0.2298\n",
       "  0.2336\n",
       "  0.2766\n",
       "  0.2159\n",
       "  0.2967\n",
       "  0.3042\n",
       "  0.3077\n",
       "  0.1989\n",
       "  0.4784\n",
       "  0.2589\n",
       "  0.2189\n",
       "  0.3537\n",
       "  0.3849\n",
       "  0.3149\n",
       "  0.4172\n",
       "  0.3396\n",
       "  0.2123\n",
       "  0.3626\n",
       "  0.2327\n",
       "  0.0704\n",
       "  0.1864\n",
       "  0.1445\n",
       "  0.2881\n",
       "  0.3893\n",
       "  0.1569\n",
       "  0.3510\n",
       "  0.2469\n",
       "  0.2653\n",
       "  0.3176\n",
       "  0.2978\n",
       "  0.2293\n",
       "  0.1957\n",
       "  0.2954\n",
       "  0.2878\n",
       "  0.2581\n",
       "  0.5130\n",
       "  0.1944\n",
       "  0.2687\n",
       "  0.3650\n",
       "  0.2015\n",
       "  0.2203\n",
       "  0.4086\n",
       "  0.1812\n",
       "  0.5810\n",
       "  0.2783\n",
       "  0.3852\n",
       "  0.3476\n",
       "  0.3143\n",
       "  0.4067\n",
       "  0.2422\n",
       "  0.3730\n",
       "  0.1624\n",
       "  0.2958\n",
       "  0.3967\n",
       "  0.2898\n",
       "  0.2511\n",
       "  0.4350\n",
       "  0.3072\n",
       "  0.3941\n",
       "  0.2057\n",
       "  0.2403\n",
       "  0.1318\n",
       "  0.3532\n",
       "  0.2870\n",
       "  0.2532\n",
       "  0.2332\n",
       "  0.2075\n",
       "  0.4485\n",
       "  0.2528\n",
       "  0.2071\n",
       "  0.2274\n",
       "  0.2705\n",
       "  0.3673\n",
       "  0.1149\n",
       "  0.3164\n",
       "  0.1852\n",
       "  0.3321\n",
       "  0.3607\n",
       "  0.2783\n",
       "  0.3910\n",
       "  0.2746\n",
       "  0.2712\n",
       "  0.3522\n",
       "  0.3015\n",
       "  0.2188\n",
       "  0.1438\n",
       "  0.3861\n",
       "  0.4326\n",
       "  0.5949\n",
       "  0.1555\n",
       "  0.4673\n",
       "  0.4153\n",
       "  0.3021\n",
       "  0.2766\n",
       "  0.4214\n",
       "  0.1985\n",
       "  0.4139\n",
       "  0.4364\n",
       "  0.3574\n",
       "  0.2570\n",
       "  0.2071\n",
       "  0.3106\n",
       "  0.2341\n",
       "  0.3016\n",
       "  0.2943\n",
       "  0.4026\n",
       "  0.3497\n",
       "  0.2731\n",
       "  0.2449\n",
       "  0.3252\n",
       "  0.1965\n",
       "  0.3265\n",
       "  0.3520\n",
       "  0.2307\n",
       "  0.2975\n",
       "  0.3035\n",
       "  0.5103\n",
       "  0.3123\n",
       "  0.2438\n",
       "  0.3430\n",
       "  0.3620\n",
       "  0.1459\n",
       "  0.5166\n",
       "  0.2534\n",
       "  0.3121\n",
       "  0.3198\n",
       "  0.1657\n",
       "  0.2741\n",
       "  0.1790\n",
       "  0.2576\n",
       "  0.4077\n",
       "  0.2576\n",
       "  0.5696\n",
       "  0.3691\n",
       "  0.2957\n",
       "  0.4012\n",
       "  0.4211\n",
       "  0.1970\n",
       "  0.1394\n",
       " -0.0361\n",
       " -0.0369\n",
       "  0.0621\n",
       "  0.0911\n",
       " -0.1146\n",
       " -0.0898\n",
       " -0.0168\n",
       "  0.0804\n",
       " -0.3468\n",
       " -0.0622\n",
       " -0.0173\n",
       "  0.0810\n",
       "  0.0754\n",
       " -0.0092\n",
       "  0.1363\n",
       " -0.0109\n",
       " -0.1361\n",
       "  0.0263\n",
       " -0.0933\n",
       " -0.0734\n",
       " -0.0395\n",
       " -0.1115\n",
       "  0.1118\n",
       "  0.0039\n",
       "  0.2684\n",
       " -0.0139\n",
       " -0.1975\n",
       " -0.2135\n",
       " -0.0295\n",
       "  0.1194\n",
       " -0.1107\n",
       " -0.1681\n",
       "  0.0566\n",
       " -0.0178\n",
       "  0.1225\n",
       " -0.3467\n",
       " -0.0954\n",
       "  0.2658\n",
       "  0.3249\n",
       " -0.1583\n",
       " -0.1452\n",
       "  0.1495\n",
       " -0.0122\n",
       " -0.1402\n",
       " -0.0889\n",
       "  0.0953\n",
       " -0.1063\n",
       "  0.2923\n",
       " -0.0642\n",
       "  0.1817\n",
       "  0.0263\n",
       "  0.2220\n",
       "  0.0160\n",
       " -0.1865\n",
       " -0.1590\n",
       " -0.0960\n",
       "  0.0414\n",
       "  0.1740\n",
       "  0.0867\n",
       " -0.0974\n",
       " -0.0421\n",
       "  0.0218\n",
       "  0.1634\n",
       "  0.0905\n",
       "  0.2416\n",
       " -0.1259\n",
       "  0.3652\n",
       " -0.0265\n",
       " -0.1577\n",
       " -0.0624\n",
       " -0.1538\n",
       "  0.0112\n",
       " -0.0809\n",
       " -0.0564\n",
       " -0.0578\n",
       "  0.1111\n",
       " -0.2176\n",
       "  0.0855\n",
       "  0.1129\n",
       "  0.1862\n",
       "  0.0509\n",
       "  0.1310\n",
       "  0.1385\n",
       " -0.1828\n",
       " -0.0716\n",
       " -0.0900\n",
       "  0.2394\n",
       "  0.1352\n",
       " -0.0116\n",
       " -0.0654\n",
       " -0.0104\n",
       " -0.0262\n",
       " -0.0520\n",
       " -0.0976\n",
       " -0.0659\n",
       " -0.0154\n",
       "  0.1252\n",
       "  0.0627\n",
       " -0.2289\n",
       "  0.0291\n",
       "  0.0568\n",
       "  0.0010\n",
       " -0.0946\n",
       " -0.0519\n",
       "  0.1193\n",
       " -0.1631\n",
       "  0.0878\n",
       "  0.0761\n",
       " -0.0819\n",
       "  0.0892\n",
       "  0.0038\n",
       "  0.1340\n",
       " -0.1999\n",
       "  0.0143\n",
       "  0.1422\n",
       " -0.0437\n",
       "  0.0529\n",
       " -0.0189\n",
       "  0.0569\n",
       "  0.0507\n",
       " -0.0652\n",
       "  0.0615\n",
       " -0.2935\n",
       "  0.1595\n",
       " -0.0654\n",
       "  0.1406\n",
       "  0.0425\n",
       "  0.0423\n",
       " -0.1325\n",
       " -0.0471\n",
       " -0.2007\n",
       "  0.0982\n",
       " -0.1588\n",
       " -0.1111\n",
       " -0.0966\n",
       " -0.0739\n",
       "  0.2365\n",
       "  0.1725\n",
       "  0.0142\n",
       " -0.1059\n",
       "  0.1017\n",
       " -0.0024\n",
       " -0.0897\n",
       " -0.0495\n",
       " -0.0910\n",
       "  0.0921\n",
       " -0.0592\n",
       " -0.0994\n",
       "  0.0355\n",
       "  0.3403\n",
       "  0.0813\n",
       " -0.0605\n",
       " -0.0853\n",
       " -0.0301\n",
       "  0.1143\n",
       " -0.0353\n",
       "  0.0907\n",
       "  0.0816\n",
       "  0.0748\n",
       "  0.2042\n",
       " -0.0330\n",
       "  0.0183\n",
       "  0.0126\n",
       " -0.0966\n",
       "  0.0872\n",
       " -0.0021\n",
       "  0.0933\n",
       "  0.1064\n",
       "  0.0383\n",
       " -0.1907\n",
       "  0.1926\n",
       " -0.0775\n",
       " -0.1309\n",
       "  0.1568\n",
       " -0.0970\n",
       " -0.0325\n",
       "  0.1803\n",
       " -0.1520\n",
       " -0.0945\n",
       "  0.1060\n",
       "  0.1779\n",
       "  0.0836\n",
       "  0.2026\n",
       "  0.0589\n",
       "  0.0061\n",
       "  0.1307\n",
       " -0.2695\n",
       " -0.1093\n",
       " -0.0355\n",
       "  0.0335\n",
       "  0.0161\n",
       " -0.0972\n",
       "  0.2877\n",
       "  0.0152\n",
       " -0.0726\n",
       "  0.0961\n",
       " -0.2073\n",
       " -0.1222\n",
       " -0.0958\n",
       " -0.1048\n",
       "  0.0515\n",
       "  0.2588\n",
       "  0.2899\n",
       "  0.2162\n",
       "  0.0676\n",
       "  0.1831\n",
       "  0.0243\n",
       " -0.0749\n",
       " -0.1665\n",
       " -0.0600\n",
       " -0.1061\n",
       "  0.1126\n",
       " -0.1794\n",
       "  0.0576\n",
       " -0.1799\n",
       " -0.1365\n",
       " -0.0075\n",
       "  0.2856\n",
       " -0.1779\n",
       " -0.1406\n",
       " -0.0966\n",
       " -0.0379\n",
       " -0.1542\n",
       "  0.1580\n",
       " -0.1504\n",
       "  0.0945\n",
       " -0.1757\n",
       " -0.1573\n",
       " -0.2506\n",
       "  0.1107\n",
       " -0.0335\n",
       " -0.1867\n",
       " -0.1533\n",
       "  0.0675\n",
       "  0.1936\n",
       " -0.0415\n",
       "  0.0029\n",
       " -0.1205\n",
       " -0.1824\n",
       " -0.0788\n",
       " -0.0674\n",
       "  0.0193\n",
       "  0.0276\n",
       "  0.0474\n",
       " -0.0814\n",
       "  0.0270\n",
       " -0.0323\n",
       "  0.2363\n",
       "  0.1418\n",
       " -0.1426\n",
       " -0.0458\n",
       "  0.2280\n",
       "  0.2719\n",
       "  0.0790\n",
       "  0.0171\n",
       " -0.3442\n",
       "  0.0341\n",
       " -0.1387\n",
       "  0.1391\n",
       "  0.1576\n",
       " -0.0227\n",
       "  0.0486\n",
       "  0.0804\n",
       "  0.0424\n",
       " -0.1053\n",
       " -0.0424\n",
       " -0.0072\n",
       " -0.1130\n",
       " -0.1781\n",
       " -0.0929\n",
       " -0.0705\n",
       " -0.2245\n",
       " -0.1273\n",
       " -0.1270\n",
       "  0.0110\n",
       "  0.0384\n",
       "  0.0839\n",
       "  0.2302\n",
       " -0.0148\n",
       "  0.1113\n",
       "  0.1052\n",
       " -0.2451\n",
       "  0.1296\n",
       " -0.0438\n",
       "  0.0221\n",
       " -0.0435\n",
       " -0.0565\n",
       "  0.1593\n",
       " -0.0682\n",
       "  0.1150\n",
       " -0.1079\n",
       " -0.0075\n",
       "  0.0867\n",
       " -0.0731\n",
       " -0.0134\n",
       " -0.0548\n",
       " -0.0096\n",
       " -0.1856\n",
       " -0.0562\n",
       "  0.0058\n",
       " [torch.cuda.FloatTensor of size 900 (GPU 0)], Parameter containing:\n",
       " -0.3038\n",
       " -0.2914\n",
       " -0.3938\n",
       " -0.2576\n",
       " -0.0843\n",
       " -0.2243\n",
       " -0.4302\n",
       " -0.3423\n",
       " -0.3687\n",
       " -0.1931\n",
       " -0.2331\n",
       " -0.4276\n",
       " -0.4015\n",
       " -0.0726\n",
       " -0.2590\n",
       " -0.4492\n",
       " -0.2123\n",
       " -0.5172\n",
       " -0.4299\n",
       " -0.2010\n",
       " -0.3733\n",
       " -0.3805\n",
       " -0.4233\n",
       " -0.2217\n",
       " -0.4193\n",
       " -0.1223\n",
       " -0.0918\n",
       " -0.1044\n",
       " -0.3732\n",
       " -0.4112\n",
       " -0.1787\n",
       " -0.3984\n",
       " -0.3562\n",
       " -0.3297\n",
       " -0.3023\n",
       " -0.1778\n",
       " -0.1748\n",
       " -0.0808\n",
       " -0.1903\n",
       " -0.3337\n",
       " -0.2689\n",
       " -0.2229\n",
       " -0.1154\n",
       " -0.3315\n",
       " -0.1090\n",
       " -0.4014\n",
       " -0.1074\n",
       " -0.3606\n",
       " -0.3749\n",
       " -0.1185\n",
       " -0.2805\n",
       " -0.3987\n",
       " -0.4635\n",
       " -0.4638\n",
       " -0.3701\n",
       " -0.3681\n",
       " -0.3201\n",
       " -0.3943\n",
       " -0.2713\n",
       " -0.3791\n",
       " -0.1393\n",
       " -0.1027\n",
       " -0.4054\n",
       " -0.3168\n",
       " -0.5866\n",
       " -0.1711\n",
       " -0.2788\n",
       " -0.1210\n",
       " -0.4760\n",
       " -0.2412\n",
       " -0.1526\n",
       " -0.2804\n",
       " -0.4019\n",
       " -0.2600\n",
       " -0.2066\n",
       " -0.3080\n",
       " -0.1880\n",
       " -0.2457\n",
       " -0.3643\n",
       " -0.3419\n",
       " -0.2567\n",
       " -0.3083\n",
       " -0.1786\n",
       " -0.4083\n",
       " -0.0999\n",
       " -0.4211\n",
       " -0.2147\n",
       " -0.2704\n",
       " -0.3780\n",
       " -0.3088\n",
       " -0.2861\n",
       " -0.2226\n",
       " -0.3276\n",
       " -0.2339\n",
       " -0.3811\n",
       " -0.3524\n",
       " -0.3094\n",
       " -0.2190\n",
       " -0.4495\n",
       " -0.2020\n",
       " -0.3382\n",
       " -0.2001\n",
       " -0.2823\n",
       " -0.6020\n",
       " -0.3354\n",
       " -0.3100\n",
       " -0.4978\n",
       " -0.3745\n",
       " -0.4350\n",
       " -0.2868\n",
       " -0.4211\n",
       " -0.1055\n",
       " -0.5178\n",
       " -0.4018\n",
       " -0.3645\n",
       " -0.4375\n",
       " -0.5952\n",
       " -0.4430\n",
       " -0.1660\n",
       " -0.2088\n",
       " -0.2756\n",
       "  0.0311\n",
       " -0.1819\n",
       " -0.1609\n",
       " -0.2414\n",
       " -0.1108\n",
       " -0.2546\n",
       " -0.4578\n",
       " -0.3193\n",
       " -0.3646\n",
       " -0.4145\n",
       " -0.4360\n",
       " -0.2378\n",
       " -0.4193\n",
       " -0.3344\n",
       " -0.3168\n",
       " -0.4368\n",
       " -0.1076\n",
       " -0.2602\n",
       " -0.3697\n",
       " -0.1738\n",
       " -0.4841\n",
       " -0.2500\n",
       " -0.2595\n",
       " -0.3137\n",
       " -0.3297\n",
       " -0.2537\n",
       " -0.3809\n",
       " -0.4030\n",
       " -0.0402\n",
       " -0.5081\n",
       " -0.4181\n",
       " -0.2264\n",
       " -0.2903\n",
       " -0.3701\n",
       " -0.3087\n",
       " -0.2671\n",
       " -0.4412\n",
       " -0.1972\n",
       " -0.5047\n",
       " -0.0595\n",
       " -0.3735\n",
       " -0.5782\n",
       " -0.2913\n",
       " -0.2314\n",
       " -0.3288\n",
       " -0.2174\n",
       " -0.2900\n",
       " -0.3840\n",
       " -0.1938\n",
       " -0.3247\n",
       " -0.3101\n",
       " -0.2602\n",
       " -0.2920\n",
       " -0.1950\n",
       " -0.2372\n",
       " -0.1980\n",
       " -0.2254\n",
       " -0.3642\n",
       " -0.2060\n",
       " -0.4802\n",
       " -0.1943\n",
       " -0.2768\n",
       " -0.2092\n",
       " -0.2404\n",
       " -0.4909\n",
       " -0.2106\n",
       " -0.3567\n",
       " -0.2741\n",
       " -0.2401\n",
       " -0.2719\n",
       " -0.3669\n",
       " -0.3699\n",
       " -0.3431\n",
       " -0.2773\n",
       " -0.1978\n",
       " -0.1845\n",
       " -0.5410\n",
       " -0.2495\n",
       " -0.2279\n",
       " -0.3075\n",
       " -0.2575\n",
       " -0.0648\n",
       " -0.3131\n",
       " -0.0003\n",
       " -0.2708\n",
       " -0.0138\n",
       " -0.1056\n",
       " -0.3278\n",
       " -0.4714\n",
       " -0.3295\n",
       " -0.2560\n",
       " -0.3780\n",
       " -0.2338\n",
       " -0.3193\n",
       " -0.3220\n",
       " -0.2406\n",
       " -0.2799\n",
       " -0.2839\n",
       " -0.1729\n",
       " -0.2444\n",
       " -0.3551\n",
       " -0.2548\n",
       " -0.1782\n",
       " -0.3888\n",
       " -0.3318\n",
       " -0.2803\n",
       " -0.5042\n",
       " -0.2673\n",
       " -0.4109\n",
       " -0.7661\n",
       " -0.2639\n",
       " -0.2417\n",
       " -0.2924\n",
       " -0.6962\n",
       " -0.2843\n",
       " -0.4609\n",
       " -0.2064\n",
       " -0.3123\n",
       " -0.1420\n",
       " -0.3192\n",
       " -0.3055\n",
       " -0.1382\n",
       " -0.5417\n",
       " -0.2776\n",
       " -0.3108\n",
       " -0.5224\n",
       " -0.0222\n",
       " -0.4891\n",
       "  0.0011\n",
       " -0.4019\n",
       " -0.0984\n",
       " -0.1705\n",
       " -0.3209\n",
       " -0.3258\n",
       " -0.2013\n",
       " -0.1824\n",
       " -0.2430\n",
       " -0.1765\n",
       " -0.5883\n",
       " -0.1677\n",
       " -0.1631\n",
       " -0.2504\n",
       " -0.4454\n",
       " -0.5013\n",
       " -0.0889\n",
       " -0.3966\n",
       " -0.1490\n",
       " -0.3241\n",
       " -0.3531\n",
       " -0.1926\n",
       " -0.6909\n",
       " -0.2568\n",
       " -0.4013\n",
       " -0.4328\n",
       " -0.2901\n",
       " -0.3375\n",
       " -0.2064\n",
       " -0.2983\n",
       " -0.3647\n",
       " -0.3515\n",
       " -0.2720\n",
       " -0.4760\n",
       " -0.2829\n",
       " -0.1823\n",
       " -0.3174\n",
       " -0.3494\n",
       " -0.2059\n",
       " -0.3932\n",
       " -0.1045\n",
       " -0.3217\n",
       " -0.3657\n",
       " -0.4193\n",
       " -0.2551\n",
       " -0.2185\n",
       " -0.3113\n",
       " -0.2950\n",
       " -0.0297\n",
       " -0.2920\n",
       " -0.5095\n",
       "  0.2795\n",
       "  0.0865\n",
       "  0.1594\n",
       "  0.3370\n",
       "  0.4061\n",
       "  0.2719\n",
       "  0.3255\n",
       "  0.1593\n",
       "  0.5374\n",
       "  0.3344\n",
       "  0.2742\n",
       "  0.3451\n",
       "  0.2787\n",
       "  0.4954\n",
       "  0.2873\n",
       "  0.1640\n",
       "  0.4269\n",
       "  0.2827\n",
       "  0.2441\n",
       "  0.2347\n",
       "  0.2031\n",
       "  0.3880\n",
       "  0.2706\n",
       "  0.2984\n",
       "  0.2927\n",
       "  0.2812\n",
       "  0.3577\n",
       "  0.3056\n",
       "  0.0891\n",
       "  0.1711\n",
       "  0.3023\n",
       "  0.1576\n",
       "  0.3523\n",
       "  0.1995\n",
       "  0.4220\n",
       "  0.2503\n",
       "  0.2997\n",
       "  0.4371\n",
       "  0.4098\n",
       "  0.1525\n",
       "  0.1915\n",
       "  0.3092\n",
       "  0.3187\n",
       "  0.1683\n",
       "  0.4032\n",
       "  0.1634\n",
       "  0.3919\n",
       "  0.3814\n",
       "  0.3434\n",
       "  0.5193\n",
       "  0.2748\n",
       "  0.4075\n",
       "  0.3269\n",
       "  0.2897\n",
       "  0.1112\n",
       "  0.1479\n",
       "  0.3016\n",
       "  0.2807\n",
       "  0.3128\n",
       "  0.2352\n",
       "  0.4284\n",
       "  0.3671\n",
       "  0.1933\n",
       "  0.2337\n",
       "  0.2212\n",
       "  0.2617\n",
       "  0.1341\n",
       "  0.3037\n",
       "  0.2033\n",
       "  0.2629\n",
       "  0.3880\n",
       "  0.2240\n",
       "  0.1830\n",
       "  0.2655\n",
       "  0.3982\n",
       "  0.4447\n",
       "  0.4984\n",
       "  0.0279\n",
       "  0.1610\n",
       "  0.2831\n",
       "  0.2097\n",
       "  0.3893\n",
       "  0.3441\n",
       "  0.2560\n",
       "  0.3286\n",
       "  0.3405\n",
       "  0.2594\n",
       "  0.3325\n",
       "  0.1906\n",
       "  0.1855\n",
       "  0.1875\n",
       "  0.5207\n",
       "  0.2608\n",
       "  0.3252\n",
       "  0.3301\n",
       "  0.1648\n",
       "  0.2700\n",
       "  0.2609\n",
       "  0.3232\n",
       "  0.3763\n",
       "  0.3521\n",
       "  0.3392\n",
       "  0.3772\n",
       "  0.1028\n",
       "  0.2965\n",
       "  0.4112\n",
       "  0.1907\n",
       "  0.1453\n",
       "  0.2104\n",
       "  0.2144\n",
       "  0.2811\n",
       "  0.3866\n",
       "  0.1504\n",
       "  0.3379\n",
       "  0.2565\n",
       "  0.1885\n",
       "  0.1560\n",
       "  0.2841\n",
       "  0.3180\n",
       "  0.3780\n",
       "  0.2333\n",
       "  0.4167\n",
       "  0.2507\n",
       "  0.3437\n",
       "  0.2679\n",
       "  0.1619\n",
       "  0.2596\n",
       "  0.3204\n",
       "  0.2972\n",
       "  0.2350\n",
       "  0.3399\n",
       "  0.4126\n",
       "  0.3940\n",
       "  0.2540\n",
       "  0.2126\n",
       "  0.3920\n",
       "  0.2281\n",
       "  0.4063\n",
       "  0.3156\n",
       "  0.4331\n",
       "  0.3560\n",
       "  0.2651\n",
       "  0.3445\n",
       "  0.2726\n",
       "  0.3042\n",
       "  0.2770\n",
       "  0.2815\n",
       "  0.2084\n",
       "  0.2136\n",
       "  0.4030\n",
       "  0.2023\n",
       "  0.3235\n",
       "  0.3942\n",
       "  0.3155\n",
       "  0.2035\n",
       "  0.3056\n",
       "  0.1866\n",
       "  0.0928\n",
       "  0.4117\n",
       "  0.1183\n",
       "  0.4229\n",
       "  0.2305\n",
       "  0.2287\n",
       "  0.3651\n",
       "  0.2372\n",
       "  0.2540\n",
       "  0.3066\n",
       "  0.2249\n",
       "  0.2498\n",
       "  0.4887\n",
       "  0.3252\n",
       "  0.2294\n",
       "  0.3588\n",
       "  0.3655\n",
       "  0.3441\n",
       "  0.3368\n",
       "  0.3392\n",
       "  0.2007\n",
       "  0.3300\n",
       "  0.2833\n",
       "  0.1409\n",
       "  0.1469\n",
       "  0.1190\n",
       "  0.2963\n",
       "  0.4743\n",
       "  0.1922\n",
       "  0.3482\n",
       "  0.1779\n",
       "  0.2592\n",
       "  0.2885\n",
       "  0.1973\n",
       "  0.2202\n",
       "  0.2158\n",
       "  0.2783\n",
       "  0.3629\n",
       "  0.3239\n",
       "  0.4914\n",
       "  0.1370\n",
       "  0.3254\n",
       "  0.2742\n",
       "  0.1752\n",
       "  0.3081\n",
       "  0.4149\n",
       "  0.2155\n",
       "  0.6466\n",
       "  0.2872\n",
       "  0.3341\n",
       "  0.3540\n",
       "  0.2503\n",
       "  0.4522\n",
       "  0.2953\n",
       "  0.4478\n",
       "  0.2523\n",
       "  0.3013\n",
       "  0.3590\n",
       "  0.2722\n",
       "  0.2737\n",
       "  0.4794\n",
       "  0.3330\n",
       "  0.3466\n",
       "  0.2737\n",
       "  0.1931\n",
       "  0.1407\n",
       "  0.4102\n",
       "  0.2143\n",
       "  0.1660\n",
       "  0.2561\n",
       "  0.2150\n",
       "  0.4386\n",
       "  0.2509\n",
       "  0.2279\n",
       "  0.1925\n",
       "  0.2484\n",
       "  0.3609\n",
       "  0.0872\n",
       "  0.2793\n",
       "  0.1549\n",
       "  0.3300\n",
       "  0.3794\n",
       "  0.3118\n",
       "  0.3623\n",
       "  0.2735\n",
       "  0.2979\n",
       "  0.3429\n",
       "  0.2780\n",
       "  0.2055\n",
       "  0.1343\n",
       "  0.4130\n",
       "  0.3621\n",
       "  0.5156\n",
       "  0.2109\n",
       "  0.5459\n",
       "  0.3294\n",
       "  0.3083\n",
       "  0.2774\n",
       "  0.4662\n",
       "  0.2683\n",
       "  0.3806\n",
       "  0.4313\n",
       "  0.3318\n",
       "  0.2680\n",
       "  0.2517\n",
       "  0.3825\n",
       "  0.2780\n",
       "  0.3648\n",
       "  0.2751\n",
       "  0.3645\n",
       "  0.3163\n",
       "  0.2377\n",
       "  0.2587\n",
       "  0.3893\n",
       "  0.1787\n",
       "  0.3169\n",
       "  0.3002\n",
       "  0.2893\n",
       "  0.2301\n",
       "  0.3354\n",
       "  0.4779\n",
       "  0.2034\n",
       "  0.2417\n",
       "  0.3625\n",
       "  0.3088\n",
       "  0.2371\n",
       "  0.5629\n",
       "  0.3356\n",
       "  0.4000\n",
       "  0.2577\n",
       "  0.2401\n",
       "  0.3111\n",
       "  0.1878\n",
       "  0.2950\n",
       "  0.4084\n",
       "  0.2761\n",
       "  0.5194\n",
       "  0.3601\n",
       "  0.3024\n",
       "  0.3941\n",
       "  0.4202\n",
       "  0.2894\n",
       "  0.1831\n",
       "  0.0242\n",
       " -0.1457\n",
       "  0.1161\n",
       " -0.2072\n",
       "  0.0576\n",
       "  0.0640\n",
       "  0.0471\n",
       " -0.1458\n",
       " -0.0120\n",
       " -0.0720\n",
       " -0.0367\n",
       "  0.2555\n",
       " -0.0922\n",
       " -0.2116\n",
       " -0.0508\n",
       " -0.0092\n",
       "  0.0309\n",
       "  0.2130\n",
       "  0.0920\n",
       "  0.0434\n",
       "  0.1457\n",
       " -0.0153\n",
       "  0.1136\n",
       " -0.0891\n",
       " -0.2142\n",
       " -0.0096\n",
       "  0.0350\n",
       "  0.0919\n",
       "  0.0027\n",
       " -0.1388\n",
       "  0.4372\n",
       "  0.1248\n",
       " -0.0331\n",
       " -0.1432\n",
       " -0.2268\n",
       "  0.0268\n",
       "  0.1547\n",
       " -0.1472\n",
       "  0.0688\n",
       "  0.1492\n",
       "  0.0042\n",
       " -0.1984\n",
       " -0.0604\n",
       "  0.0466\n",
       "  0.2662\n",
       " -0.1612\n",
       " -0.0399\n",
       "  0.0368\n",
       " -0.0707\n",
       " -0.0171\n",
       " -0.0238\n",
       "  0.0351\n",
       " -0.2610\n",
       "  0.1314\n",
       " -0.0345\n",
       "  0.0997\n",
       " -0.0345\n",
       " -0.0459\n",
       "  0.1060\n",
       "  0.2549\n",
       "  0.2629\n",
       " -0.0434\n",
       " -0.0540\n",
       "  0.0276\n",
       " -0.1750\n",
       "  0.1219\n",
       " -0.2516\n",
       "  0.1479\n",
       "  0.0184\n",
       "  0.0595\n",
       " -0.0085\n",
       "  0.0612\n",
       "  0.0808\n",
       "  0.0082\n",
       "  0.0215\n",
       "  0.2005\n",
       "  0.0930\n",
       "  0.1126\n",
       " -0.0451\n",
       " -0.1363\n",
       " -0.1031\n",
       "  0.1137\n",
       " -0.0346\n",
       "  0.2602\n",
       "  0.1882\n",
       " -0.0299\n",
       " -0.2461\n",
       " -0.1252\n",
       "  0.0563\n",
       "  0.0454\n",
       "  0.0163\n",
       " -0.1092\n",
       "  0.0062\n",
       "  0.0021\n",
       "  0.3104\n",
       " -0.1518\n",
       " -0.1621\n",
       "  0.1886\n",
       "  0.0546\n",
       " -0.0339\n",
       " -0.0128\n",
       " -0.0522\n",
       "  0.0230\n",
       "  0.2400\n",
       " -0.0725\n",
       " -0.0894\n",
       " -0.1042\n",
       " -0.1997\n",
       "  0.1241\n",
       " -0.0930\n",
       " -0.0277\n",
       " -0.0001\n",
       "  0.1790\n",
       " -0.0243\n",
       " -0.1186\n",
       "  0.0372\n",
       "  0.1086\n",
       "  0.0313\n",
       " -0.0992\n",
       " -0.1682\n",
       "  0.0932\n",
       "  0.0687\n",
       "  0.1070\n",
       " -0.0224\n",
       "  0.0985\n",
       " -0.1355\n",
       "  0.0124\n",
       " -0.0872\n",
       "  0.1106\n",
       " -0.0408\n",
       "  0.1005\n",
       "  0.0897\n",
       " -0.1715\n",
       " -0.1004\n",
       "  0.2517\n",
       "  0.0835\n",
       " -0.0312\n",
       " -0.0620\n",
       " -0.1033\n",
       "  0.3000\n",
       " -0.2568\n",
       " -0.1092\n",
       "  0.1783\n",
       "  0.0912\n",
       "  0.0613\n",
       "  0.0187\n",
       " -0.0222\n",
       "  0.1290\n",
       "  0.1336\n",
       "  0.2747\n",
       " -0.0962\n",
       "  0.0622\n",
       " -0.1603\n",
       "  0.0686\n",
       "  0.0473\n",
       "  0.0722\n",
       " -0.0742\n",
       " -0.0108\n",
       " -0.0285\n",
       " -0.1382\n",
       "  0.0105\n",
       "  0.0887\n",
       "  0.0649\n",
       "  0.1080\n",
       "  0.0569\n",
       "  0.0346\n",
       " -0.0275\n",
       " -0.1149\n",
       " -0.1582\n",
       "  0.1683\n",
       " -0.2716\n",
       "  0.0948\n",
       "  0.0722\n",
       " -0.2754\n",
       "  0.0965\n",
       "  0.1775\n",
       " -0.1381\n",
       "  0.1319\n",
       "  0.0654\n",
       "  0.0714\n",
       " -0.2415\n",
       " -0.0829\n",
       "  0.0333\n",
       "  0.0726\n",
       "  0.0052\n",
       " -0.3742\n",
       "  0.1384\n",
       "  0.0627\n",
       " -0.0818\n",
       "  0.0014\n",
       " -0.0194\n",
       " -0.0286\n",
       " -0.3226\n",
       " -0.1697\n",
       " -0.0276\n",
       " -0.1121\n",
       "  0.0246\n",
       " -0.1365\n",
       "  0.0074\n",
       " -0.1186\n",
       " -0.1081\n",
       " -0.1675\n",
       "  0.0919\n",
       " -0.1223\n",
       "  0.1056\n",
       " -0.1272\n",
       "  0.3584\n",
       "  0.1166\n",
       "  0.1290\n",
       " -0.0630\n",
       "  0.0935\n",
       " -0.0288\n",
       "  0.2774\n",
       " -0.0034\n",
       "  0.1116\n",
       "  0.0643\n",
       " -0.2530\n",
       "  0.0129\n",
       "  0.2007\n",
       "  0.0608\n",
       "  0.0029\n",
       " -0.1033\n",
       "  0.0833\n",
       " -0.0164\n",
       "  0.0815\n",
       " -0.2782\n",
       "  0.1215\n",
       "  0.2777\n",
       "  0.2207\n",
       " -0.1676\n",
       "  0.0728\n",
       "  0.1134\n",
       "  0.1217\n",
       " -0.2560\n",
       " -0.4810\n",
       "  0.0355\n",
       "  0.0787\n",
       " -0.0937\n",
       "  0.0661\n",
       " -0.2209\n",
       "  0.0132\n",
       "  0.0573\n",
       "  0.0121\n",
       " -0.0461\n",
       "  0.1445\n",
       " -0.0951\n",
       "  0.1489\n",
       "  0.0095\n",
       " -0.1271\n",
       "  0.0620\n",
       "  0.0979\n",
       " -0.2066\n",
       "  0.0840\n",
       "  0.1035\n",
       "  0.1265\n",
       "  0.2049\n",
       " -0.0990\n",
       " -0.2317\n",
       " -0.0191\n",
       "  0.0614\n",
       "  0.0474\n",
       " -0.1515\n",
       "  0.0230\n",
       " -0.2047\n",
       " -0.0486\n",
       " -0.0033\n",
       "  0.1977\n",
       "  0.0363\n",
       "  0.1932\n",
       "  0.1995\n",
       "  0.0631\n",
       "  0.4480\n",
       "  0.1987\n",
       "  0.0015\n",
       " -0.1491\n",
       " -0.0186\n",
       " -0.1509\n",
       " -0.1832\n",
       "  0.0624\n",
       " -0.0466\n",
       " -0.2047\n",
       "  0.1342\n",
       "  0.0125\n",
       " -0.1821\n",
       " -0.0621\n",
       "  0.0200\n",
       " -0.1248\n",
       " -0.0003\n",
       "  0.0486\n",
       " -0.1718\n",
       "  0.2157\n",
       "  0.2732\n",
       " -0.1427\n",
       "  0.0107\n",
       " -0.0573\n",
       " -0.0098\n",
       " -0.1305\n",
       " -0.1740\n",
       " -0.1328\n",
       "  0.2260\n",
       " [torch.cuda.FloatTensor of size 900 (GPU 0)], Parameter containing:\n",
       " -2.0636e-01 -2.8268e-01 -5.0431e-01  ...  -5.5790e-01  4.4074e-02  4.4667e-02\n",
       "  2.4711e-01  1.2097e-01 -9.0616e-02  ...   2.0826e-02  1.5289e-01  9.4035e-02\n",
       "  6.3937e-02  1.3111e-01 -1.7903e-01  ...  -1.7708e-01  8.6915e-02  5.7008e-02\n",
       "                 ...                   ⋱                   ...                \n",
       " -2.0600e-01 -7.2727e-02 -1.1816e-01  ...   2.3295e-03 -1.7558e-01  2.5165e-02\n",
       "  6.1358e-02 -2.0322e-02  1.8970e-02  ...   4.0258e-02  6.1518e-02 -8.5700e-02\n",
       " -5.8906e-02 -1.4584e-02  1.5835e-01  ...   5.3593e-02  3.6702e-02  1.9673e-02\n",
       " [torch.cuda.FloatTensor of size 900x300 (GPU 0)], Parameter containing:\n",
       " -8.8024e-01  1.6138e-01 -3.4951e-02  ...   5.3747e-02 -2.1747e-02 -3.2984e-02\n",
       "  1.1969e-01  3.2894e-01  6.7750e-02  ...   1.2919e-01  6.4343e-02  2.5979e-02\n",
       "  2.5492e-01 -1.4492e-01 -1.1974e-01  ...  -2.9131e-01 -4.1821e-02  4.4165e-02\n",
       "                 ...                   ⋱                   ...                \n",
       " -2.1027e-01 -9.7850e-03  2.3847e-01  ...  -8.1938e-01 -1.3964e-02  9.8571e-02\n",
       "  3.9783e-01 -3.4175e-01 -3.3248e-01  ...   1.3581e-01 -9.2332e-01  8.7012e-02\n",
       "  3.5759e-01 -9.3813e-02  3.7759e-01  ...   8.7382e-02 -2.3583e-01 -1.4541e+00\n",
       " [torch.cuda.FloatTensor of size 900x300 (GPU 0)], Parameter containing:\n",
       " -5.3386e-01\n",
       " -1.3094e-01\n",
       " -6.9063e-01\n",
       " -2.3315e-01\n",
       " -9.3477e-01\n",
       " -6.2978e-01\n",
       " -9.5106e-02\n",
       " -5.2995e-01\n",
       " -9.2700e-01\n",
       " -4.1409e-01\n",
       " -5.8712e-01\n",
       " -7.3094e-02\n",
       " -4.3030e-01\n",
       " -5.6552e-01\n",
       " -4.6219e-01\n",
       " -4.5265e-01\n",
       " -1.0405e+00\n",
       "  3.4841e-01\n",
       " -3.4591e-01\n",
       " -5.3825e-01\n",
       "  1.5564e-01\n",
       " -4.0701e-01\n",
       " -7.1589e-02\n",
       "  5.5311e-01\n",
       "  1.1755e-01\n",
       " -5.4010e-02\n",
       " -7.4783e-01\n",
       " -6.9933e-01\n",
       " -1.0372e-01\n",
       " -3.0628e-01\n",
       " -5.0591e-01\n",
       "  1.5111e-02\n",
       " -6.1930e-01\n",
       "  6.0283e-02\n",
       " -2.5720e-01\n",
       "  2.3867e-01\n",
       " -4.2017e-02\n",
       " -4.7404e-01\n",
       " -1.0831e-01\n",
       " -5.4212e-01\n",
       " -3.4317e-01\n",
       " -2.1994e-01\n",
       " -5.2596e-01\n",
       " -2.2296e-01\n",
       " -6.0520e-01\n",
       " -2.9964e-01\n",
       " -5.4646e-01\n",
       "  3.3190e-01\n",
       "  1.3458e-01\n",
       " -6.8041e-01\n",
       " -5.7312e-02\n",
       " -4.5277e-01\n",
       " -7.5240e-01\n",
       " -3.4328e-01\n",
       " -3.6144e-01\n",
       " -5.2748e-01\n",
       " -1.8949e-01\n",
       " -3.2896e-01\n",
       " -5.2455e-01\n",
       " -2.3691e-01\n",
       " -2.2425e-01\n",
       " -2.4028e-01\n",
       " -5.9773e-02\n",
       " -1.0980e-01\n",
       "  1.9090e-01\n",
       " -5.1516e-01\n",
       "  7.8662e-02\n",
       "  4.2800e-02\n",
       " -2.5420e-01\n",
       " -8.6050e-01\n",
       " -7.3662e-02\n",
       " -4.7756e-01\n",
       " -3.5380e-01\n",
       " -3.1041e-01\n",
       " -5.8096e-01\n",
       " -5.6339e-01\n",
       "  4.8805e-02\n",
       " -5.4579e-02\n",
       "  2.5669e-02\n",
       " -1.4897e-01\n",
       " -4.2695e-01\n",
       " -4.6885e-01\n",
       " -9.5084e-02\n",
       " -2.9486e-01\n",
       " -4.4474e-01\n",
       " -3.6632e-01\n",
       " -3.7775e-01\n",
       " -2.1439e-01\n",
       " -1.6978e-01\n",
       " -4.0807e-01\n",
       " -1.7200e-01\n",
       " -6.1492e-01\n",
       " -5.1150e-01\n",
       " -5.3770e-02\n",
       " -4.0850e-01\n",
       "  4.1189e-02\n",
       " -6.7821e-01\n",
       " -3.4105e-01\n",
       "  2.6105e-01\n",
       " -3.9593e-01\n",
       " -4.5259e-01\n",
       " -3.5585e-01\n",
       " -7.8873e-01\n",
       " -6.1329e-02\n",
       "  3.2671e-01\n",
       " -1.8718e-01\n",
       " -1.8570e-01\n",
       " -1.1322e-01\n",
       "  1.0072e-02\n",
       " -3.2440e-01\n",
       " -2.6725e-01\n",
       " -2.9723e-01\n",
       " -3.0718e-01\n",
       " -5.5755e-01\n",
       " -2.6236e-01\n",
       " -5.8392e-01\n",
       " -7.4377e-02\n",
       " -2.9521e-01\n",
       " -3.8303e-01\n",
       " -2.9023e-01\n",
       " -4.3116e-01\n",
       " -4.2202e-01\n",
       "  3.8348e-01\n",
       "  1.3937e-01\n",
       " -1.3263e-01\n",
       "  1.6948e-02\n",
       " -3.8564e-01\n",
       " -2.7633e-01\n",
       " -4.5249e-01\n",
       " -8.7433e-01\n",
       " -5.1663e-01\n",
       " -5.5998e-01\n",
       " -4.5384e-01\n",
       " -6.6823e-01\n",
       " -8.1082e-03\n",
       " -2.6462e-01\n",
       " -4.5576e-01\n",
       " -3.8862e-01\n",
       " -2.1968e-01\n",
       "  2.7497e-01\n",
       " -4.3721e-01\n",
       " -4.5470e-02\n",
       "  7.7087e-01\n",
       " -3.0525e-01\n",
       "  1.2372e-01\n",
       " -2.9173e-01\n",
       " -3.6778e-01\n",
       " -3.1167e-01\n",
       " -3.0437e-01\n",
       " -2.4882e-01\n",
       " -1.3151e-01\n",
       " -1.6941e-01\n",
       " -2.8035e-01\n",
       " -5.1946e-01\n",
       " -3.5762e-01\n",
       " -4.4816e-01\n",
       " -5.8545e-02\n",
       " -2.6048e-01\n",
       " -3.3369e-01\n",
       " -7.7500e-02\n",
       " -3.9845e-01\n",
       " -7.6665e-01\n",
       " -6.6842e-01\n",
       " -2.8129e-01\n",
       "  8.4699e-02\n",
       " -2.7826e-01\n",
       " -6.8518e-01\n",
       " -3.8933e-01\n",
       " -4.4213e-01\n",
       " -3.0429e-01\n",
       " -2.2906e-01\n",
       " -2.3970e-01\n",
       " -3.9561e-01\n",
       " -2.8203e-01\n",
       " -1.1465e-01\n",
       " -2.6113e-01\n",
       " -3.3538e-01\n",
       " -2.6334e-01\n",
       "  1.3824e-02\n",
       "  9.9980e-02\n",
       " -5.7045e-01\n",
       "  3.5149e-01\n",
       " -2.7213e-01\n",
       " -2.5289e-01\n",
       " -3.2905e-01\n",
       " -4.1900e-01\n",
       " -3.8678e-01\n",
       " -1.6051e-01\n",
       " -3.2513e-01\n",
       " -6.2567e-02\n",
       " -1.8023e-01\n",
       " -8.9943e-02\n",
       " -1.4799e-02\n",
       " -4.4740e-01\n",
       " -6.5122e-01\n",
       " -2.1302e-01\n",
       " -6.6717e-01\n",
       " -6.6739e-01\n",
       "  2.9679e-01\n",
       " -4.0502e-01\n",
       "  7.2863e-02\n",
       "  1.5911e-01\n",
       "  1.8903e-01\n",
       " -2.8350e-01\n",
       " -2.6290e-02\n",
       " -5.2028e-01\n",
       "  1.0536e-01\n",
       " -7.9712e-01\n",
       "  4.9956e-01\n",
       " -1.7388e-01\n",
       " -2.6961e-01\n",
       "  9.9599e-02\n",
       " -1.3556e-01\n",
       " -1.8773e-02\n",
       " -1.0183e+00\n",
       " -4.3645e-01\n",
       " -1.6136e-01\n",
       " -1.0234e-01\n",
       " -5.3011e-01\n",
       " -4.4109e-01\n",
       " -2.9224e-02\n",
       "  8.8478e-02\n",
       " -7.0901e-01\n",
       " -5.7314e-01\n",
       " -1.8398e-01\n",
       "  1.9551e-01\n",
       "  7.2422e-02\n",
       " -4.7940e-01\n",
       " -5.3099e-01\n",
       " -1.9224e-01\n",
       " -5.3534e-01\n",
       " -2.8716e-01\n",
       " -4.7369e-01\n",
       " -4.2366e-01\n",
       " -2.9730e-01\n",
       " -4.2238e-01\n",
       " -2.7799e-01\n",
       "  4.3723e-02\n",
       "  2.8236e-01\n",
       " -1.0479e+00\n",
       " -6.9754e-01\n",
       " -6.0160e-02\n",
       " -4.8032e-01\n",
       " -8.3853e-01\n",
       "  3.3311e-01\n",
       " -7.3367e-01\n",
       " -5.8273e-01\n",
       "  7.1731e-02\n",
       " -5.6940e-01\n",
       " -3.7287e-01\n",
       " -5.0295e-01\n",
       " -2.2401e-01\n",
       " -3.9358e-01\n",
       " -6.3107e-01\n",
       " -5.3255e-02\n",
       " -4.7622e-02\n",
       " -5.1339e-01\n",
       "  2.2673e-01\n",
       " -1.0646e-01\n",
       " -5.3072e-01\n",
       " -3.4699e-01\n",
       " -2.8105e-01\n",
       " -3.2921e-01\n",
       " -2.8394e-01\n",
       " -2.9298e-01\n",
       " -5.3561e-01\n",
       " -1.5735e-01\n",
       " -5.4507e-01\n",
       " -3.3668e-01\n",
       " -5.5431e-02\n",
       " -2.6188e-01\n",
       " -2.4056e-01\n",
       " -5.8106e-01\n",
       " -1.5079e-01\n",
       " -6.5320e-01\n",
       " -3.4984e-01\n",
       " -4.5676e-01\n",
       "  2.6150e-01\n",
       " -9.1022e-01\n",
       "  9.3877e-02\n",
       " -3.5288e-01\n",
       " -6.4592e-01\n",
       " -7.2887e-01\n",
       " -9.3817e-02\n",
       "  1.3232e-01\n",
       "  7.0698e-02\n",
       " -5.4970e-01\n",
       " -1.5707e-01\n",
       " -2.9478e-01\n",
       "  7.4507e-02\n",
       " -7.0828e-01\n",
       " -2.7323e-01\n",
       " -3.5335e-01\n",
       " -4.1560e-01\n",
       " -8.4053e-02\n",
       " -6.7204e-01\n",
       " -1.2461e-01\n",
       " -7.4496e-02\n",
       " -1.2503e+00\n",
       " -1.2273e-01\n",
       " -1.1078e-01\n",
       " -4.0434e-02\n",
       " -4.4125e-01\n",
       " -3.8671e-01\n",
       " -6.0287e-01\n",
       " -5.1304e-01\n",
       "  1.8877e-01\n",
       "  3.8410e-02\n",
       " -7.2895e-01\n",
       "  5.1699e-01\n",
       "  9.9177e-03\n",
       "  1.3475e+00\n",
       " -4.4479e-01\n",
       " -3.9559e-01\n",
       " -3.9987e-01\n",
       " -3.7449e-01\n",
       " -1.4491e-01\n",
       "  2.6436e-01\n",
       " -3.5921e-01\n",
       " -1.7764e-01\n",
       "  1.3459e+00\n",
       " -2.0827e-01\n",
       " -8.8954e-02\n",
       "  8.6647e-01\n",
       "  1.1131e+00\n",
       "  4.6115e-01\n",
       "  6.7743e-01\n",
       " -2.0219e-01\n",
       "  2.4692e-01\n",
       " -4.2279e-01\n",
       "  8.0563e-01\n",
       "  1.5110e-02\n",
       " -3.0107e-01\n",
       "  1.3704e+00\n",
       "  7.1195e-01\n",
       "  4.7705e-01\n",
       " -7.3137e-02\n",
       " -3.6393e-01\n",
       "  7.5360e-01\n",
       " -2.2245e-01\n",
       "  1.0121e-01\n",
       " -4.6998e-01\n",
       " -3.1842e-01\n",
       "  1.4049e+00\n",
       " -4.5970e-01\n",
       "  3.5978e-01\n",
       " -3.5255e-01\n",
       "  1.2527e+00\n",
       "  1.5266e+00\n",
       " -2.6338e-02\n",
       "  1.2379e+00\n",
       " -1.4813e-01\n",
       " -6.4088e-01\n",
       " -9.4621e-02\n",
       "  1.7294e-01\n",
       " -5.6848e-01\n",
       " -1.3262e-01\n",
       " -1.6224e-01\n",
       "  6.7194e-02\n",
       " -2.1516e-01\n",
       "  1.6968e-03\n",
       "  9.4457e-02\n",
       "  1.0817e+00\n",
       "  1.2127e+00\n",
       "  9.5853e-01\n",
       " -5.4515e-01\n",
       "  5.4347e-01\n",
       "  4.2482e-01\n",
       "  1.9228e-01\n",
       " -3.5486e-02\n",
       " -1.1150e-01\n",
       " -4.1513e-02\n",
       "  2.3898e-01\n",
       " -1.5091e-01\n",
       " -4.0681e-01\n",
       " -1.8196e-01\n",
       "  6.2639e-01\n",
       "  5.0594e-01\n",
       "  1.0265e+00\n",
       " -3.1194e-01\n",
       " -3.7581e-01\n",
       " -5.6650e-01\n",
       "  9.6833e-01\n",
       "  7.6611e-02\n",
       " -7.0754e-01\n",
       " -6.9178e-02\n",
       " -2.4631e-01\n",
       " -1.6377e-01\n",
       "  3.2978e-01\n",
       " -2.3484e-01\n",
       "  4.3923e-01\n",
       " -6.0961e-01\n",
       "  2.2847e-01\n",
       "  8.8029e-01\n",
       " -3.1860e-01\n",
       " -1.3685e-01\n",
       " -2.0592e-01\n",
       "  6.5522e-01\n",
       "  1.1816e+00\n",
       " -3.0501e-01\n",
       " -1.7836e-01\n",
       "  1.6314e-01\n",
       " -4.3505e-01\n",
       "  1.6407e-01\n",
       "  1.0188e+00\n",
       " -2.8155e-01\n",
       "  9.0529e-02\n",
       "  1.5198e+00\n",
       " -1.4722e-01\n",
       "  7.1490e-02\n",
       "  3.3268e-02\n",
       "  4.3418e-02\n",
       "  1.0883e-01\n",
       " -3.0908e-01\n",
       " -9.5671e-02\n",
       " -4.1967e-01\n",
       " -7.9900e-02\n",
       "  4.1497e-02\n",
       " -2.3229e-01\n",
       "  2.1202e-02\n",
       "  8.1657e-02\n",
       "  3.6003e-02\n",
       "  7.3990e-01\n",
       "  6.8159e-01\n",
       "  2.9953e-01\n",
       "  6.4849e-01\n",
       " -1.4051e-01\n",
       "  5.5526e-02\n",
       " -3.6405e-01\n",
       " -2.7267e-01\n",
       " -1.1910e-01\n",
       " -4.9518e-01\n",
       " -3.5735e-01\n",
       " -2.5947e-01\n",
       "  4.8437e-01\n",
       " -4.4578e-02\n",
       " -1.8513e-01\n",
       " -6.2712e-02\n",
       " -1.6956e-01\n",
       "  8.3299e-01\n",
       " -2.9159e-01\n",
       "  1.1971e-01\n",
       "  1.2343e+00\n",
       "  1.5158e-01\n",
       "  1.0297e+00\n",
       "  2.0813e-01\n",
       " -6.6881e-02\n",
       " -5.7868e-01\n",
       "  2.0950e-01\n",
       " -2.3308e-01\n",
       "  2.1926e-01\n",
       "  2.7768e-01\n",
       " -3.2721e-01\n",
       "  3.1385e-01\n",
       " -2.9831e-01\n",
       "  1.8507e-02\n",
       "  6.3963e-01\n",
       " -4.6916e-01\n",
       "  3.6580e-01\n",
       "  2.1546e-01\n",
       " -7.9339e-02\n",
       " -4.2028e-01\n",
       " -5.6864e-01\n",
       " -4.5943e-01\n",
       "  7.4870e-01\n",
       " -4.7490e-01\n",
       "  2.6552e-01\n",
       " -2.0088e-01\n",
       " -1.3247e-01\n",
       " -5.2301e-01\n",
       " -2.3246e-02\n",
       "  4.6405e-04\n",
       "  3.6412e-01\n",
       " -3.9997e-01\n",
       "  1.5976e-01\n",
       " -3.3774e-01\n",
       "  6.4929e-01\n",
       " -4.4161e-02\n",
       "  7.7159e-02\n",
       "  1.3343e-01\n",
       " -1.3561e-01\n",
       "  5.5697e-01\n",
       " -1.7501e-01\n",
       "  1.2545e+00\n",
       " -3.4811e-01\n",
       "  3.5161e-02\n",
       "  4.5202e-02\n",
       " -3.5256e-02\n",
       "  3.2021e-01\n",
       "  4.7913e-01\n",
       "  1.1024e+00\n",
       "  5.0747e-01\n",
       "  1.3849e-01\n",
       " -2.3648e-01\n",
       " -3.7761e-01\n",
       " -1.9657e-01\n",
       " -4.2989e-01\n",
       " -6.0596e-02\n",
       "  1.0904e+00\n",
       "  2.9214e-01\n",
       "  6.5416e-01\n",
       "  8.7964e-01\n",
       "  4.1424e-01\n",
       " -3.8217e-03\n",
       "  7.0614e-01\n",
       "  1.3835e+00\n",
       "  1.3197e+00\n",
       " -4.0820e-01\n",
       "  9.8613e-01\n",
       "  2.5197e-02\n",
       "  2.0679e-01\n",
       "  1.5729e+00\n",
       " -1.3498e-01\n",
       "  1.4321e-01\n",
       " -3.8200e-01\n",
       "  7.4569e-01\n",
       "  3.5658e-01\n",
       "  1.1285e+00\n",
       " -1.4003e-01\n",
       "  3.9116e-02\n",
       "  4.1494e-02\n",
       "  9.7914e-03\n",
       "  1.3952e-02\n",
       " -3.3944e-01\n",
       "  1.5530e+00\n",
       "  2.6761e-01\n",
       "  3.6312e-02\n",
       " -1.6561e-01\n",
       "  2.2181e-02\n",
       "  1.4287e-01\n",
       "  1.7058e-02\n",
       "  2.6994e-01\n",
       " -4.3627e-01\n",
       " -2.5887e-02\n",
       "  2.5358e-01\n",
       "  7.5408e-02\n",
       "  1.1251e+00\n",
       "  6.9386e-01\n",
       "  4.0170e-01\n",
       " -6.7522e-01\n",
       " -3.3661e-01\n",
       "  1.1892e+00\n",
       "  1.0648e-01\n",
       " -2.8453e-01\n",
       "  4.8365e-01\n",
       " -2.1077e-01\n",
       " -1.5398e-01\n",
       "  1.3791e+00\n",
       " -5.1731e-01\n",
       "  4.4556e-02\n",
       " -1.2652e-01\n",
       "  1.3108e+00\n",
       " -3.8803e-02\n",
       " -4.1997e-01\n",
       " -8.2521e-02\n",
       "  1.7260e-01\n",
       "  4.8840e-02\n",
       "  8.3623e-01\n",
       "  1.3946e+00\n",
       " -4.8310e-01\n",
       " -6.3385e-02\n",
       " -2.4673e-01\n",
       "  2.2377e-01\n",
       " -2.0080e-01\n",
       " -1.6857e-01\n",
       "  4.1097e-01\n",
       "  7.4565e-02\n",
       " -3.0627e-01\n",
       " -3.9027e-01\n",
       " -1.9392e-02\n",
       "  2.9932e-01\n",
       " -2.1543e-01\n",
       "  4.5227e-02\n",
       " -3.5352e-01\n",
       " -1.4465e-01\n",
       "  4.4777e-01\n",
       " -1.2775e-01\n",
       "  6.1624e-01\n",
       " -5.3598e-01\n",
       "  1.8404e+00\n",
       " -2.1582e-01\n",
       "  1.1051e-01\n",
       "  1.5686e-01\n",
       " -3.8413e-01\n",
       "  5.5207e-01\n",
       " -2.2543e-02\n",
       " -3.0404e-01\n",
       "  1.1173e+00\n",
       "  1.0871e+00\n",
       "  1.1743e+00\n",
       " -3.2084e-01\n",
       " -3.6741e-01\n",
       " -9.9966e-02\n",
       " -3.2805e-01\n",
       "  8.9161e-05\n",
       " -2.6345e-01\n",
       "  1.4264e+00\n",
       " -6.9598e-03\n",
       " -3.1205e-01\n",
       " -3.5876e-01\n",
       " -3.7268e-01\n",
       "  6.4381e-01\n",
       "  2.3004e-01\n",
       " -8.4973e-02\n",
       "  9.2242e-02\n",
       "  8.4501e-03\n",
       " -2.9488e-01\n",
       "  4.5668e-01\n",
       " -4.7577e-02\n",
       "  2.9101e-01\n",
       " -2.8135e-01\n",
       "  2.2487e-01\n",
       " -1.2295e-01\n",
       "  1.1978e-01\n",
       " -2.0771e-01\n",
       " -3.0964e-01\n",
       " -2.7019e-01\n",
       "  2.9529e-01\n",
       " -2.5010e-01\n",
       "  1.6127e-01\n",
       "  3.7874e-01\n",
       "  9.7009e-02\n",
       " -3.6196e-01\n",
       "  6.9873e-01\n",
       "  1.2133e-01\n",
       "  1.0386e-01\n",
       " -2.6668e-03\n",
       " -1.9169e-01\n",
       " -9.7723e-02\n",
       "  2.7173e-01\n",
       " -3.9179e-01\n",
       " -5.9053e-02\n",
       "  2.4030e-01\n",
       " -3.5901e-01\n",
       "  1.5855e-02\n",
       " -7.0475e-01\n",
       "  1.3565e-01\n",
       "  7.6338e-02\n",
       " -2.1571e-01\n",
       " -2.5746e-02\n",
       " -6.7640e-01\n",
       " -1.0123e+00\n",
       " -1.5501e-01\n",
       " -3.7287e-01\n",
       "  1.3129e-01\n",
       " -1.6621e-01\n",
       " -2.0288e-01\n",
       " -4.0400e-01\n",
       "  8.3071e-01\n",
       "  1.6324e-01\n",
       "  8.9773e-01\n",
       "  8.1010e-03\n",
       " -2.0665e-01\n",
       " -2.8726e-01\n",
       "  1.2481e-01\n",
       " -8.7772e-02\n",
       " -3.1011e-02\n",
       "  1.5163e-01\n",
       " -5.4004e-01\n",
       " -3.6042e-01\n",
       " -4.4853e-01\n",
       "  3.2613e-01\n",
       "  5.5911e-01\n",
       "  1.5538e-02\n",
       "  5.6007e-01\n",
       " -1.7832e-01\n",
       " -1.0980e+00\n",
       "  6.4353e-01\n",
       "  2.2954e-01\n",
       " -3.4130e-01\n",
       "  7.7851e-02\n",
       " -6.7490e-02\n",
       " -9.8279e-02\n",
       " -3.5983e-03\n",
       " -3.5335e-02\n",
       " -3.0134e-01\n",
       " -1.2709e-01\n",
       "  7.7071e-02\n",
       "  1.2643e-01\n",
       " -1.4155e-01\n",
       "  2.9914e-01\n",
       "  1.1698e-01\n",
       "  7.3726e-02\n",
       " -5.4266e-01\n",
       "  2.3775e-01\n",
       " -9.4500e-02\n",
       "  2.7391e-01\n",
       " -3.0226e-01\n",
       " -7.7927e-01\n",
       "  1.6135e-01\n",
       "  2.9188e-01\n",
       "  2.7461e-01\n",
       " -3.2232e-02\n",
       "  1.5935e-01\n",
       " -7.1352e-02\n",
       "  6.7978e-02\n",
       "  3.7743e-01\n",
       "  1.3310e-01\n",
       " -5.8771e-01\n",
       " -1.8760e-02\n",
       "  1.9000e-01\n",
       "  7.4905e-01\n",
       "  2.6979e-01\n",
       "  1.2245e-01\n",
       " -7.8204e-02\n",
       "  3.8595e-01\n",
       " -1.3263e-01\n",
       "  2.9272e-01\n",
       "  2.9695e-02\n",
       "  4.5957e-01\n",
       "  2.4036e-01\n",
       "  2.3348e-02\n",
       " -3.1570e-01\n",
       "  1.1693e-01\n",
       "  2.1985e-01\n",
       " -1.5744e-01\n",
       "  3.2851e-02\n",
       "  2.5692e-01\n",
       "  1.2478e-01\n",
       " -4.9019e-01\n",
       " -1.6427e-02\n",
       " -4.1925e-01\n",
       " -1.5517e-01\n",
       "  1.9258e-01\n",
       " -2.5517e-01\n",
       " -3.5898e-02\n",
       " -4.8108e-01\n",
       " -6.2937e-01\n",
       "  3.6909e-01\n",
       "  9.1555e-02\n",
       " -2.3309e-01\n",
       " -3.3457e-02\n",
       "  3.8639e-02\n",
       "  4.6518e-01\n",
       "  5.3091e-01\n",
       "  6.9974e-01\n",
       " -2.5292e-02\n",
       "  4.4531e-01\n",
       " -3.9288e-01\n",
       " -4.2723e-01\n",
       "  3.0924e-01\n",
       " -4.6047e-01\n",
       " -9.4796e-01\n",
       "  8.2957e-01\n",
       "  2.6628e-01\n",
       " -5.2758e-01\n",
       " -1.8931e-01\n",
       " -3.9810e-02\n",
       " -4.7842e-01\n",
       "  4.0155e-02\n",
       "  3.2406e-01\n",
       "  1.3661e-01\n",
       " -1.3367e-01\n",
       "  2.9408e-01\n",
       "  1.7579e-01\n",
       " -3.7100e-01\n",
       "  4.6782e-01\n",
       "  1.6732e-01\n",
       " -6.5586e-01\n",
       "  9.5087e-02\n",
       " -7.2586e-01\n",
       " -3.0435e-01\n",
       "  1.4469e-01\n",
       "  1.2757e-01\n",
       "  7.2509e-02\n",
       "  1.5864e-01\n",
       "  7.5271e-01\n",
       " -7.8796e-02\n",
       "  3.4153e-01\n",
       "  1.9559e-01\n",
       " -2.4638e-01\n",
       "  1.6757e-01\n",
       "  4.6884e-01\n",
       " -3.4115e-01\n",
       "  2.4583e-01\n",
       " -9.2005e-02\n",
       "  2.9435e-01\n",
       " -2.3791e-01\n",
       "  2.2859e-01\n",
       " -9.2047e-02\n",
       " -2.5075e-01\n",
       " -1.8562e-02\n",
       " -7.6261e-02\n",
       "  1.0682e+00\n",
       "  1.6673e-02\n",
       "  2.9304e-01\n",
       " -5.6972e-02\n",
       "  2.1881e-01\n",
       " -2.1151e-01\n",
       "  6.3684e-01\n",
       "  2.4271e-02\n",
       "  8.3797e-01\n",
       " -3.1982e-01\n",
       " -5.5454e-01\n",
       " -5.0155e-01\n",
       " -2.4444e-01\n",
       "  2.3591e-01\n",
       " -5.2032e-03\n",
       " -1.4703e+00\n",
       "  5.8345e-01\n",
       "  2.8932e-01\n",
       " -5.8273e-01\n",
       "  6.8686e-02\n",
       " -3.9959e-01\n",
       " -6.3795e-01\n",
       "  6.4701e-02\n",
       "  3.1051e-01\n",
       "  3.7659e-01\n",
       " -1.0438e+00\n",
       " -8.6922e-03\n",
       "  6.7834e-01\n",
       " -2.4773e-01\n",
       "  3.1174e-03\n",
       " -2.1651e-01\n",
       " -2.6349e-03\n",
       "  2.4831e-01\n",
       "  5.9971e-01\n",
       " -6.3519e-01\n",
       " -1.6008e-01\n",
       "  1.4271e-01\n",
       " -1.0287e-01\n",
       " -2.4029e-01\n",
       " -3.8985e-01\n",
       " -1.6989e-01\n",
       " -7.2255e-02\n",
       "  5.0224e-01\n",
       "  5.2461e-01\n",
       "  2.8682e-01\n",
       " -7.1316e-01\n",
       "  6.0094e-01\n",
       " -6.2887e-01\n",
       " -6.1275e-01\n",
       "  2.7298e-01\n",
       " -1.2739e-01\n",
       " -7.6184e-01\n",
       " -3.9714e-02\n",
       " -1.8860e-01\n",
       " -6.2715e-02\n",
       "  1.1963e-01\n",
       "  6.9408e-02\n",
       "  2.9103e-01\n",
       " -3.8211e-01\n",
       "  3.0652e-01\n",
       " -1.7009e-01\n",
       "  3.4198e-01\n",
       "  1.5548e-01\n",
       "  9.8497e-01\n",
       " -6.1496e-01\n",
       " -5.5063e-02\n",
       " -2.9312e-01\n",
       "  5.4500e-02\n",
       "  5.7763e-03\n",
       " -5.1015e-01\n",
       " -2.3345e-01\n",
       "  9.4573e-02\n",
       "  1.5253e-01\n",
       " -4.3562e-01\n",
       " -9.3328e-01\n",
       " -1.3777e-01\n",
       "  2.0952e-01\n",
       " -1.4143e-01\n",
       " -2.9656e-01\n",
       "  5.8561e-01\n",
       " -7.4079e-01\n",
       " -5.1877e-01\n",
       "  4.1516e-01\n",
       "  4.3095e-01\n",
       " -2.4523e-01\n",
       "  2.2116e-01\n",
       "  7.4124e-01\n",
       "  1.0934e+00\n",
       "  2.3700e-02\n",
       " -7.1893e-01\n",
       " -2.9468e-02\n",
       " -4.8500e-01\n",
       " -2.2688e-02\n",
       "  4.8516e-02\n",
       " -4.7994e-01\n",
       " -3.9784e-02\n",
       "  1.3347e+00\n",
       " -2.2188e-02\n",
       "  3.6315e-01\n",
       " -5.1915e-01\n",
       "  3.0592e-01\n",
       "  1.0406e-01\n",
       "  1.1153e-01\n",
       " -3.0867e-01\n",
       " -2.3466e-01\n",
       "  7.1281e-01\n",
       " -8.5471e-01\n",
       "  2.3615e-01\n",
       " -2.9527e-02\n",
       "  2.4000e-01\n",
       "  4.1183e-02\n",
       "  6.6957e-02\n",
       "  7.4997e-01\n",
       "  4.0257e-01\n",
       "  1.9966e-01\n",
       "  2.3599e-02\n",
       "  2.3495e-01\n",
       " [torch.cuda.FloatTensor of size 900 (GPU 0)], Parameter containing:\n",
       " -0.5639\n",
       " -0.0873\n",
       " -0.6802\n",
       " -0.2378\n",
       " -0.8453\n",
       " -0.7224\n",
       " -0.0349\n",
       " -0.5612\n",
       " -0.8875\n",
       " -0.4094\n",
       " -0.6117\n",
       " -0.0302\n",
       " -0.3822\n",
       " -0.5024\n",
       " -0.5195\n",
       " -0.4690\n",
       " -1.1040\n",
       "  0.3645\n",
       " -0.4505\n",
       " -0.4601\n",
       "  0.1348\n",
       " -0.3652\n",
       " -0.1382\n",
       "  0.5323\n",
       "  0.1384\n",
       "  0.0292\n",
       " -0.6752\n",
       " -0.6363\n",
       " -0.1178\n",
       " -0.3157\n",
       " -0.5380\n",
       " -0.0040\n",
       " -0.7039\n",
       "  0.0912\n",
       " -0.2765\n",
       "  0.2303\n",
       " -0.0588\n",
       " -0.5423\n",
       " -0.0705\n",
       " -0.6034\n",
       " -0.3562\n",
       " -0.2245\n",
       " -0.5725\n",
       " -0.2193\n",
       " -0.6263\n",
       " -0.1947\n",
       " -0.5645\n",
       "  0.3979\n",
       "  0.2045\n",
       " -0.5972\n",
       " -0.0626\n",
       " -0.5095\n",
       " -0.7434\n",
       " -0.3047\n",
       " -0.3402\n",
       " -0.4615\n",
       " -0.1871\n",
       " -0.3198\n",
       " -0.4731\n",
       " -0.2914\n",
       " -0.1872\n",
       " -0.2687\n",
       "  0.0280\n",
       " -0.0933\n",
       "  0.1922\n",
       " -0.5029\n",
       "  0.0506\n",
       "  0.0806\n",
       " -0.1621\n",
       " -0.9094\n",
       " -0.1269\n",
       " -0.4651\n",
       " -0.3005\n",
       " -0.3085\n",
       " -0.5523\n",
       " -0.4836\n",
       "  0.0141\n",
       "  0.0351\n",
       "  0.0109\n",
       " -0.1757\n",
       " -0.4152\n",
       " -0.4524\n",
       " -0.1269\n",
       " -0.2064\n",
       " -0.3771\n",
       " -0.3687\n",
       " -0.3902\n",
       " -0.2683\n",
       " -0.1593\n",
       " -0.3991\n",
       " -0.2627\n",
       " -0.6265\n",
       " -0.4808\n",
       " -0.1527\n",
       " -0.4573\n",
       " -0.0054\n",
       " -0.7083\n",
       " -0.3402\n",
       "  0.2645\n",
       " -0.4552\n",
       " -0.4078\n",
       " -0.3640\n",
       " -0.6910\n",
       " -0.1013\n",
       "  0.2893\n",
       " -0.0762\n",
       " -0.2443\n",
       " -0.1082\n",
       "  0.0344\n",
       " -0.3163\n",
       " -0.3493\n",
       " -0.3448\n",
       " -0.2913\n",
       " -0.5241\n",
       " -0.3015\n",
       " -0.5618\n",
       " -0.1141\n",
       " -0.3153\n",
       " -0.3457\n",
       " -0.2837\n",
       " -0.4758\n",
       " -0.3729\n",
       "  0.3592\n",
       "  0.1930\n",
       " -0.1986\n",
       " -0.0080\n",
       " -0.4386\n",
       " -0.2103\n",
       " -0.3755\n",
       " -0.8175\n",
       " -0.4404\n",
       " -0.5152\n",
       " -0.4239\n",
       " -0.6764\n",
       "  0.0414\n",
       " -0.2256\n",
       " -0.4434\n",
       " -0.3745\n",
       " -0.2681\n",
       "  0.2198\n",
       " -0.4578\n",
       " -0.0778\n",
       "  0.7604\n",
       " -0.2480\n",
       "  0.1474\n",
       " -0.2725\n",
       " -0.3821\n",
       " -0.3074\n",
       " -0.2760\n",
       " -0.2341\n",
       " -0.0772\n",
       " -0.0721\n",
       " -0.2455\n",
       " -0.4422\n",
       " -0.3846\n",
       " -0.4254\n",
       " -0.0484\n",
       " -0.2299\n",
       " -0.2634\n",
       "  0.0075\n",
       " -0.3525\n",
       " -0.7096\n",
       " -0.7757\n",
       " -0.2565\n",
       "  0.1377\n",
       " -0.2445\n",
       " -0.7467\n",
       " -0.4165\n",
       " -0.3638\n",
       " -0.3568\n",
       " -0.2776\n",
       " -0.2369\n",
       " -0.3127\n",
       " -0.2864\n",
       " -0.0668\n",
       " -0.3137\n",
       " -0.2457\n",
       " -0.2152\n",
       "  0.0255\n",
       "  0.0736\n",
       " -0.6018\n",
       "  0.3369\n",
       " -0.2712\n",
       " -0.2639\n",
       " -0.3070\n",
       " -0.3843\n",
       " -0.3954\n",
       " -0.2254\n",
       " -0.3867\n",
       " -0.0882\n",
       " -0.2365\n",
       " -0.1000\n",
       " -0.0082\n",
       " -0.4775\n",
       " -0.6720\n",
       " -0.1675\n",
       " -0.6711\n",
       " -0.6331\n",
       "  0.2929\n",
       " -0.4374\n",
       "  0.1254\n",
       "  0.2153\n",
       "  0.2429\n",
       " -0.3060\n",
       " -0.0537\n",
       " -0.5040\n",
       "  0.1228\n",
       " -0.7835\n",
       "  0.5004\n",
       " -0.1598\n",
       " -0.2307\n",
       "  0.1258\n",
       " -0.1204\n",
       " -0.0191\n",
       " -1.0797\n",
       " -0.4384\n",
       " -0.0963\n",
       " -0.0413\n",
       " -0.5306\n",
       " -0.4767\n",
       " -0.0999\n",
       "  0.0394\n",
       " -0.7140\n",
       " -0.6613\n",
       " -0.1900\n",
       "  0.1230\n",
       "  0.0748\n",
       " -0.4312\n",
       " -0.5100\n",
       " -0.1346\n",
       " -0.5487\n",
       " -0.3147\n",
       " -0.3971\n",
       " -0.4432\n",
       " -0.2254\n",
       " -0.5003\n",
       " -0.2718\n",
       "  0.1398\n",
       "  0.3291\n",
       " -1.0550\n",
       " -0.7759\n",
       " -0.0698\n",
       " -0.4342\n",
       " -0.8252\n",
       "  0.2865\n",
       " -0.6342\n",
       " -0.6546\n",
       "  0.0359\n",
       " -0.6160\n",
       " -0.3870\n",
       " -0.5932\n",
       " -0.1714\n",
       " -0.3146\n",
       " -0.6404\n",
       " -0.1131\n",
       " -0.0633\n",
       " -0.4949\n",
       "  0.1878\n",
       " -0.1268\n",
       " -0.5429\n",
       " -0.4054\n",
       " -0.2930\n",
       " -0.2636\n",
       " -0.2200\n",
       " -0.2270\n",
       " -0.5600\n",
       " -0.2192\n",
       " -0.5669\n",
       " -0.3283\n",
       " -0.0873\n",
       " -0.3003\n",
       " -0.3044\n",
       " -0.6776\n",
       " -0.1108\n",
       " -0.6094\n",
       " -0.4236\n",
       " -0.5574\n",
       "  0.2758\n",
       " -0.9457\n",
       "  0.1426\n",
       " -0.3375\n",
       " -0.5486\n",
       " -0.6673\n",
       " -0.1547\n",
       "  0.1282\n",
       "  0.0188\n",
       " -0.4651\n",
       " -0.1688\n",
       " -0.2422\n",
       "  0.0132\n",
       " -0.6901\n",
       " -0.3167\n",
       " -0.3756\n",
       " -0.4747\n",
       " -0.1052\n",
       " -0.6932\n",
       " -0.1274\n",
       " -0.0768\n",
       " -1.1832\n",
       " -0.1287\n",
       " -0.0985\n",
       "  0.0256\n",
       " -0.4080\n",
       " -0.3789\n",
       " -0.5559\n",
       " -0.5209\n",
       "  0.1430\n",
       " -0.0022\n",
       " -0.6341\n",
       "  0.5470\n",
       "  0.0012\n",
       "  1.3624\n",
       " -0.4027\n",
       " -0.4246\n",
       " -0.4285\n",
       " -0.4571\n",
       " -0.1915\n",
       "  0.2612\n",
       " -0.3994\n",
       " -0.1927\n",
       "  1.2858\n",
       " -0.2447\n",
       " -0.1137\n",
       "  0.8264\n",
       "  1.0397\n",
       "  0.3863\n",
       "  0.7077\n",
       " -0.2454\n",
       "  0.2857\n",
       " -0.4268\n",
       "  0.8060\n",
       "  0.0115\n",
       " -0.3083\n",
       "  1.3198\n",
       "  0.6389\n",
       "  0.4808\n",
       " -0.0656\n",
       " -0.4115\n",
       "  0.7285\n",
       " -0.2657\n",
       "  0.1030\n",
       " -0.4798\n",
       " -0.2947\n",
       "  1.4750\n",
       " -0.4438\n",
       "  0.3914\n",
       " -0.3266\n",
       "  1.2278\n",
       "  1.5486\n",
       " -0.0024\n",
       "  1.2363\n",
       " -0.2086\n",
       " -0.5688\n",
       " -0.0845\n",
       "  0.1733\n",
       " -0.5358\n",
       " -0.1496\n",
       " -0.1633\n",
       "  0.0644\n",
       " -0.1964\n",
       " -0.0138\n",
       "  0.0304\n",
       "  1.0673\n",
       "  1.1298\n",
       "  0.9576\n",
       " -0.4749\n",
       "  0.5165\n",
       "  0.4338\n",
       "  0.2285\n",
       " -0.0345\n",
       " -0.1663\n",
       " -0.0443\n",
       "  0.2789\n",
       " -0.1600\n",
       " -0.4014\n",
       " -0.1949\n",
       "  0.5873\n",
       "  0.4022\n",
       "  1.0986\n",
       " -0.3672\n",
       " -0.3452\n",
       " -0.5538\n",
       "  0.9706\n",
       " -0.0087\n",
       " -0.6831\n",
       " -0.0775\n",
       " -0.3533\n",
       " -0.1660\n",
       "  0.3359\n",
       " -0.2973\n",
       "  0.3391\n",
       " -0.5477\n",
       "  0.2114\n",
       "  0.8947\n",
       " -0.2725\n",
       " -0.0685\n",
       " -0.1920\n",
       "  0.6663\n",
       "  1.1804\n",
       " -0.3575\n",
       " -0.1904\n",
       "  0.1831\n",
       " -0.4522\n",
       "  0.1695\n",
       "  1.0889\n",
       " -0.3218\n",
       "  0.0036\n",
       "  1.5140\n",
       " -0.1481\n",
       "  0.0125\n",
       "  0.0225\n",
       "  0.0691\n",
       "  0.0785\n",
       " -0.2177\n",
       " -0.0534\n",
       " -0.4676\n",
       " -0.0900\n",
       "  0.0124\n",
       " -0.2137\n",
       "  0.0392\n",
       "  0.0640\n",
       "  0.0523\n",
       "  0.7020\n",
       "  0.6321\n",
       "  0.3019\n",
       "  0.7366\n",
       " -0.2023\n",
       "  0.0785\n",
       " -0.3243\n",
       " -0.2776\n",
       " -0.1730\n",
       " -0.3961\n",
       " -0.3958\n",
       " -0.2299\n",
       "  0.4365\n",
       "  0.0363\n",
       " -0.2132\n",
       " -0.0881\n",
       " -0.1870\n",
       "  0.8020\n",
       " -0.3541\n",
       "  0.0722\n",
       "  1.1497\n",
       "  0.1797\n",
       "  1.0946\n",
       "  0.2062\n",
       " -0.0176\n",
       " -0.5775\n",
       "  0.1652\n",
       " -0.2069\n",
       "  0.1988\n",
       "  0.2647\n",
       " -0.4076\n",
       "  0.2751\n",
       " -0.3937\n",
       " -0.0436\n",
       "  0.6992\n",
       " -0.4631\n",
       "  0.3236\n",
       "  0.1905\n",
       " -0.0787\n",
       " -0.5272\n",
       " -0.6609\n",
       " -0.4401\n",
       "  0.6733\n",
       " -0.4937\n",
       "  0.2451\n",
       " -0.1787\n",
       " -0.2073\n",
       " -0.5052\n",
       "  0.0049\n",
       "  0.0742\n",
       "  0.3881\n",
       " -0.2939\n",
       "  0.1774\n",
       " -0.2554\n",
       "  0.6700\n",
       " -0.0840\n",
       "  0.1229\n",
       "  0.1163\n",
       " -0.0681\n",
       "  0.5315\n",
       " -0.1541\n",
       "  1.2848\n",
       " -0.3512\n",
       "  0.0464\n",
       " -0.0311\n",
       " -0.0551\n",
       "  0.2709\n",
       "  0.5559\n",
       "  1.1184\n",
       "  0.4719\n",
       "  0.2033\n",
       " -0.2222\n",
       " -0.3626\n",
       " -0.2704\n",
       " -0.4570\n",
       " -0.0656\n",
       "  1.0982\n",
       "  0.2883\n",
       "  0.6221\n",
       "  0.8985\n",
       "  0.4271\n",
       " -0.0342\n",
       "  0.7339\n",
       "  1.3445\n",
       "  1.3540\n",
       " -0.4495\n",
       "  1.0864\n",
       " -0.0539\n",
       "  0.2317\n",
       "  1.5573\n",
       " -0.1443\n",
       "  0.1161\n",
       " -0.4113\n",
       "  0.7339\n",
       "  0.3092\n",
       "  1.1625\n",
       " -0.1830\n",
       "  0.0392\n",
       " -0.0317\n",
       "  0.0310\n",
       "  0.0352\n",
       " -0.2677\n",
       "  1.5718\n",
       "  0.2753\n",
       "  0.0649\n",
       " -0.1282\n",
       " -0.0301\n",
       "  0.1088\n",
       "  0.0423\n",
       "  0.2671\n",
       " -0.4591\n",
       " -0.0146\n",
       "  0.2745\n",
       "  0.0612\n",
       "  1.1353\n",
       "  0.6287\n",
       "  0.3524\n",
       " -0.6710\n",
       " -0.3379\n",
       "  1.2378\n",
       "  0.0885\n",
       " -0.2699\n",
       "  0.3929\n",
       " -0.2839\n",
       " -0.2098\n",
       "  1.2905\n",
       " -0.5408\n",
       "  0.0273\n",
       " -0.1482\n",
       "  1.4013\n",
       " -0.0456\n",
       " -0.4420\n",
       " -0.1055\n",
       "  0.1442\n",
       " -0.0070\n",
       "  0.7986\n",
       "  1.4406\n",
       " -0.4442\n",
       " -0.0622\n",
       " -0.2240\n",
       "  0.2492\n",
       " -0.2199\n",
       " -0.1571\n",
       "  0.3669\n",
       "  0.0735\n",
       " -0.3798\n",
       " -0.4160\n",
       "  0.0299\n",
       "  0.2794\n",
       " -0.1798\n",
       "  0.1302\n",
       " -0.3566\n",
       " -0.0784\n",
       "  0.4828\n",
       " -0.1215\n",
       "  0.6411\n",
       " -0.5173\n",
       "  1.8434\n",
       " -0.1769\n",
       "  0.0694\n",
       "  0.1012\n",
       " -0.3582\n",
       "  0.4972\n",
       "  0.0648\n",
       " -0.3375\n",
       "  1.0457\n",
       "  1.0558\n",
       "  1.2502\n",
       " -0.2274\n",
       " -0.4418\n",
       " -0.1564\n",
       " -0.2619\n",
       "  0.0094\n",
       " -0.2820\n",
       "  1.4124\n",
       "  0.0138\n",
       " -0.3220\n",
       " -0.3677\n",
       " -0.4575\n",
       " -0.4618\n",
       "  0.9081\n",
       " -0.6219\n",
       " -0.4314\n",
       "  0.6012\n",
       " -0.3252\n",
       " -0.3311\n",
       "  0.0227\n",
       " -0.6959\n",
       " -0.1208\n",
       " -0.5795\n",
       " -0.1375\n",
       " -0.4229\n",
       " -0.0869\n",
       " -0.2369\n",
       " -0.3317\n",
       " -1.4619\n",
       "  0.8999\n",
       "  0.6600\n",
       "  0.0432\n",
       " -0.9644\n",
       " -0.3615\n",
       " -1.5657\n",
       "  0.4562\n",
       " -1.0023\n",
       " -0.2482\n",
       " -0.3610\n",
       " -0.6606\n",
       " -0.3899\n",
       "  0.0531\n",
       " -0.7614\n",
       " -0.1160\n",
       " -0.1188\n",
       " -0.6375\n",
       " -0.0217\n",
       "  0.1782\n",
       "  0.0811\n",
       "  0.5356\n",
       "  0.6110\n",
       " -0.1505\n",
       "  0.0075\n",
       " -0.0321\n",
       " -0.1090\n",
       " -0.1985\n",
       " -0.5156\n",
       "  0.2150\n",
       " -0.4056\n",
       " -0.1047\n",
       " -1.1161\n",
       " -0.6798\n",
       "  0.2686\n",
       "  0.4527\n",
       "  0.3476\n",
       " -0.0466\n",
       "  0.0848\n",
       " -1.0806\n",
       "  0.8390\n",
       "  0.3477\n",
       "  0.0003\n",
       " -0.4156\n",
       " -0.7362\n",
       "  0.1953\n",
       " -0.8120\n",
       " -1.7612\n",
       " -0.0373\n",
       "  0.5600\n",
       "  0.0759\n",
       "  0.6420\n",
       " -0.8816\n",
       " -0.4396\n",
       " -0.1114\n",
       " -0.5861\n",
       " -0.0091\n",
       "  0.8451\n",
       " -0.0680\n",
       " -0.7096\n",
       "  0.5309\n",
       " -0.5073\n",
       " -1.4499\n",
       "  0.2611\n",
       " -0.1444\n",
       " -0.3383\n",
       " -0.2999\n",
       " -0.2614\n",
       "  0.0688\n",
       " -0.1812\n",
       " -0.7933\n",
       " -0.2926\n",
       " -0.6195\n",
       "  0.5677\n",
       " -0.3768\n",
       "  0.4152\n",
       " -0.5305\n",
       "  0.4954\n",
       " -0.9047\n",
       " -0.3188\n",
       "  0.1060\n",
       "  1.1679\n",
       " -0.4101\n",
       "  0.6354\n",
       " -0.0903\n",
       " -0.1345\n",
       " -1.2072\n",
       "  1.2386\n",
       " -1.1860\n",
       " -0.3281\n",
       "  0.2484\n",
       "  0.9674\n",
       "  1.1640\n",
       "  0.7332\n",
       "  0.9656\n",
       " -0.5343\n",
       "  0.7939\n",
       "  0.0589\n",
       "  0.5312\n",
       " -0.9635\n",
       "  0.3965\n",
       "  0.5959\n",
       " -0.1291\n",
       " -0.6043\n",
       " -0.3766\n",
       "  1.2030\n",
       " -0.6619\n",
       "  0.1735\n",
       " -0.7789\n",
       "  0.3446\n",
       " -0.1717\n",
       "  0.2749\n",
       "  0.4590\n",
       " -1.0243\n",
       " -0.0143\n",
       "  0.7117\n",
       "  0.8011\n",
       " -1.7983\n",
       "  0.3650\n",
       " -0.2508\n",
       " -0.2111\n",
       "  0.0410\n",
       "  1.6025\n",
       "  0.3421\n",
       " -0.6185\n",
       "  1.2697\n",
       "  0.1822\n",
       " -0.7236\n",
       " -0.4590\n",
       "  1.4659\n",
       " -0.0136\n",
       " -0.3749\n",
       " -0.3691\n",
       "  0.2052\n",
       " -0.0840\n",
       "  0.5220\n",
       "  0.8451\n",
       " -0.4496\n",
       " -0.1494\n",
       "  0.2251\n",
       "  0.1385\n",
       " -0.5008\n",
       " -1.3327\n",
       "  0.3264\n",
       " -0.5516\n",
       " -1.5945\n",
       "  0.2654\n",
       "  1.0450\n",
       "  0.6389\n",
       "  0.5389\n",
       " -0.6003\n",
       "  0.1045\n",
       " -0.6644\n",
       "  1.2244\n",
       "  0.1046\n",
       " -0.2265\n",
       " -0.1751\n",
       "  0.7366\n",
       "  0.3499\n",
       "  0.1520\n",
       " -0.0712\n",
       "  1.2239\n",
       "  0.8406\n",
       " -0.0458\n",
       " -1.0272\n",
       "  0.3629\n",
       "  0.2386\n",
       " -0.0539\n",
       "  0.0197\n",
       " -0.9011\n",
       " -1.2903\n",
       " -0.4326\n",
       "  0.5724\n",
       " -0.6214\n",
       " -0.1604\n",
       "  2.2649\n",
       "  0.1922\n",
       "  0.1918\n",
       "  1.6024\n",
       "  0.8123\n",
       "  0.9181\n",
       "  0.7564\n",
       " -0.1872\n",
       "  0.7413\n",
       "  1.4930\n",
       " -1.1528\n",
       " -0.4543\n",
       " -0.7886\n",
       " -0.1039\n",
       " -0.1406\n",
       "  0.0962\n",
       "  1.5874\n",
       " -0.8716\n",
       " -0.1582\n",
       " -0.2352\n",
       "  0.9947\n",
       " -0.3934\n",
       "  0.8464\n",
       "  0.1409\n",
       "  0.5892\n",
       "  0.1216\n",
       " -0.5847\n",
       " -1.2586\n",
       " -0.5289\n",
       " -0.9560\n",
       " -0.0339\n",
       "  0.2776\n",
       " -0.1492\n",
       " -0.4079\n",
       " -0.7615\n",
       "  0.7659\n",
       "  0.1143\n",
       "  0.4513\n",
       "  0.2565\n",
       "  0.3449\n",
       "  0.4951\n",
       " -0.5499\n",
       " -0.5427\n",
       " -0.2547\n",
       "  0.1127\n",
       "  0.9720\n",
       " -1.0361\n",
       "  0.0674\n",
       "  0.1205\n",
       " -0.0035\n",
       "  0.3547\n",
       " -0.0517\n",
       " -1.7758\n",
       "  0.5823\n",
       " -0.2413\n",
       "  0.0280\n",
       "  0.2905\n",
       "  0.1249\n",
       " -0.1627\n",
       " -0.5117\n",
       " -0.5458\n",
       " -0.0095\n",
       "  0.3094\n",
       " -1.4676\n",
       " -0.0883\n",
       "  1.6908\n",
       "  0.5983\n",
       " -0.5606\n",
       " -0.6713\n",
       " -0.0252\n",
       "  0.3809\n",
       " -0.3001\n",
       "  0.4058\n",
       " -0.0170\n",
       " -1.0800\n",
       "  0.1323\n",
       " -0.2429\n",
       "  0.4150\n",
       "  0.0791\n",
       "  0.0107\n",
       " -0.4521\n",
       " -0.7157\n",
       "  0.1790\n",
       "  0.0015\n",
       " -0.3545\n",
       "  1.1200\n",
       " -0.3222\n",
       " -0.0012\n",
       "  0.6114\n",
       "  0.4344\n",
       " -0.4247\n",
       " -0.7908\n",
       " -0.8997\n",
       "  0.9946\n",
       " -0.5816\n",
       "  0.4420\n",
       " -0.1239\n",
       " -0.0519\n",
       "  0.0374\n",
       "  1.0376\n",
       " -0.7505\n",
       "  1.0232\n",
       " -0.4230\n",
       " -0.5821\n",
       " -0.1613\n",
       "  0.2945\n",
       " -0.0290\n",
       "  0.4420\n",
       " [torch.cuda.FloatTensor of size 900 (GPU 0)], Parameter containing:\n",
       "  1.1229e-02 -1.9338e-03 -8.3379e-02  ...   4.3728e-02 -4.0707e-02 -6.0252e-02\n",
       "  1.5296e-01 -1.0090e-01 -8.5323e-02  ...   9.4002e-02  9.1933e-02 -1.0586e-01\n",
       "  1.4872e-01 -1.0906e-01  6.5542e-02  ...   1.3270e-01 -1.3095e-01 -7.3148e-03\n",
       "                 ...                   ⋱                   ...                \n",
       "  1.6543e-01 -5.3161e-02 -8.0187e-02  ...   5.3745e-02  1.1915e-01  2.8544e-03\n",
       "  6.6034e-02  3.2015e-02 -3.1161e-02  ...   4.2728e-02 -5.8281e-02  1.9435e-02\n",
       "  4.0187e-02 -4.8259e-02 -1.5437e-01  ...  -1.6883e-01 -1.2237e-01  5.8362e-02\n",
       " [torch.cuda.FloatTensor of size 33279x300 (GPU 0)], Parameter containing:\n",
       " -2.1122e-01\n",
       "  5.8319e+00\n",
       "  3.8565e+00\n",
       "      ⋮     \n",
       " -1.3951e-01\n",
       " -1.0397e-01\n",
       " -7.4228e-02\n",
       " [torch.cuda.FloatTensor of size 33279 (GPU 0)], Parameter containing:\n",
       " -2.4456e-03 -1.7589e-02  2.3906e-02  ...  -1.7290e-02  9.2460e-03  2.7114e-02\n",
       " -2.3636e-02 -2.3302e-02  2.3475e-02  ...   3.1777e-02 -7.8609e-03  1.9710e-02\n",
       " -2.5185e-02 -2.1775e-02  1.6495e-02  ...   3.1401e-02  2.6350e-02  1.8037e-02\n",
       "                 ...                   ⋱                   ...                \n",
       " -9.4843e-03 -7.8527e-03  2.2789e-02  ...   6.6206e-03 -2.7285e-03  2.9313e-02\n",
       " -1.4682e-02 -5.1419e-03 -2.8329e-02  ...  -9.8871e-03 -2.8774e-02  1.6750e-02\n",
       "  2.2808e-02 -1.1687e-02  3.0018e-02  ...  -7.6081e-03 -2.1443e-02 -2.5280e-02\n",
       " [torch.cuda.FloatTensor of size 100x900 (GPU 0)], Parameter containing:\n",
       " 1.00000e-02 *\n",
       "  -0.7008\n",
       "  -1.2307\n",
       "   0.2602\n",
       "  -2.2237\n",
       "  -1.5779\n",
       "   1.4321\n",
       "  -2.5499\n",
       "   2.1625\n",
       "  -3.0062\n",
       "   3.2224\n",
       "   0.0008\n",
       "   1.8546\n",
       "   2.8761\n",
       "   0.4799\n",
       "  -0.1440\n",
       "  -1.6853\n",
       "  -2.5882\n",
       "  -2.8784\n",
       "  -0.2019\n",
       "   3.2769\n",
       "   0.8464\n",
       "   3.2967\n",
       "  -1.0505\n",
       "   1.1038\n",
       "  -0.6104\n",
       "   2.9904\n",
       "   2.8288\n",
       "  -0.8160\n",
       "  -1.4553\n",
       "   2.3028\n",
       "  -1.5657\n",
       "  -0.2371\n",
       "  -3.2560\n",
       "   1.6623\n",
       "   2.3444\n",
       "   2.6147\n",
       "   1.8316\n",
       "   2.7317\n",
       "  -1.7003\n",
       "  -3.2019\n",
       "  -0.1060\n",
       "  -0.0555\n",
       "  -1.3670\n",
       "   2.7610\n",
       "  -1.3671\n",
       "   3.2551\n",
       "  -1.4136\n",
       "   2.8556\n",
       "  -0.6171\n",
       "  -0.6336\n",
       "   0.1974\n",
       "   2.5246\n",
       "   1.0960\n",
       "   1.6631\n",
       "  -1.7279\n",
       "  -2.7151\n",
       "   2.3663\n",
       "  -2.4538\n",
       "   2.9157\n",
       "  -2.6022\n",
       "  -2.2674\n",
       "  -0.3288\n",
       "   2.6427\n",
       "   0.0645\n",
       "  -0.9200\n",
       "   0.2743\n",
       "   0.6676\n",
       "   2.9915\n",
       "  -3.2562\n",
       "  -2.9470\n",
       "   2.8249\n",
       "   2.8423\n",
       "  -1.4177\n",
       "  -1.3079\n",
       "  -0.1352\n",
       "   0.5530\n",
       "   1.6790\n",
       "  -0.8818\n",
       "   2.6876\n",
       "   1.4648\n",
       "  -0.0086\n",
       "   2.1913\n",
       "  -2.5072\n",
       "  -1.1894\n",
       "  -2.0049\n",
       "   2.3733\n",
       "   0.0502\n",
       "  -0.8901\n",
       "  -0.8286\n",
       "   0.4202\n",
       "   0.6230\n",
       "   1.4839\n",
       "  -1.1044\n",
       "  -0.3624\n",
       "  -3.1098\n",
       "   2.0843\n",
       "  -1.1007\n",
       "   1.4320\n",
       "  -0.2339\n",
       "   1.8538\n",
       " [torch.cuda.FloatTensor of size 100 (GPU 0)], Parameter containing:\n",
       " \n",
       " Columns 0 to 9 \n",
       " 1.00000e-02 *\n",
       "   8.6383  2.1404  7.1247 -4.6811 -1.8583  5.9881 -7.1478  0.9792  8.9364 -8.4412\n",
       " \n",
       " Columns 10 to 19 \n",
       " 1.00000e-02 *\n",
       "  -1.7221 -3.9976  3.1841  8.3385  3.3672 -3.2861  9.8621 -2.3312 -6.2160 -1.2265\n",
       " \n",
       " Columns 20 to 29 \n",
       " 1.00000e-02 *\n",
       "  -7.9955 -2.8087 -1.6778  4.2410 -6.9620  9.0351 -7.9283 -7.1001 -2.8628 -1.2376\n",
       " \n",
       " Columns 30 to 39 \n",
       " 1.00000e-02 *\n",
       "   3.0503  8.1928 -8.4817 -2.2764 -0.9779  7.8555 -4.6069  4.4160 -5.0586  0.2668\n",
       " \n",
       " Columns 40 to 49 \n",
       " 1.00000e-02 *\n",
       "  -4.2824  5.3694 -6.6230  7.4193  2.9856 -1.9213 -7.3690 -0.8844 -9.5763  2.1636\n",
       " \n",
       " Columns 50 to 59 \n",
       " 1.00000e-02 *\n",
       "  -5.4247 -6.1302 -6.2465 -3.6162 -8.2178 -7.6166 -2.3739  4.0527 -4.2390  0.2555\n",
       " \n",
       " Columns 60 to 69 \n",
       " 1.00000e-02 *\n",
       "  -6.3470 -2.2228  9.6868 -2.5908  3.9367 -6.1939 -5.1938  1.8980 -3.7569 -1.2480\n",
       " \n",
       " Columns 70 to 79 \n",
       " 1.00000e-02 *\n",
       "  -1.2293 -1.5214 -6.1721 -2.9628 -5.7801  1.7450 -5.3341 -3.9431 -4.5468 -5.7749\n",
       " \n",
       " Columns 80 to 89 \n",
       " 1.00000e-02 *\n",
       "  -0.1745  8.3248 -3.0244 -5.0048 -1.9826  8.1297 -4.7480 -2.6863  1.4975  6.6211\n",
       " \n",
       " Columns 90 to 99 \n",
       " 1.00000e-02 *\n",
       "   3.6086  4.5134  7.8025 -9.7248 -3.4325 -9.3759  3.9739 -3.0759  7.7271  9.2248\n",
       " [torch.cuda.FloatTensor of size 1x100 (GPU 0)], Parameter containing:\n",
       " 1.00000e-02 *\n",
       "  -2.6361\n",
       " [torch.cuda.FloatTensor of size 1 (GPU 0)], Parameter containing:\n",
       "  0.4282\n",
       "  0.9652\n",
       "  0.9909\n",
       "  0.6025\n",
       "  0.7687\n",
       "  0.7328\n",
       "  0.9921\n",
       "  0.2409\n",
       "  0.0123\n",
       "  0.3520\n",
       "  0.9604\n",
       "  0.0678\n",
       "  0.5153\n",
       "  0.4859\n",
       "  0.9943\n",
       "  0.3693\n",
       "  0.3671\n",
       "  0.2868\n",
       "  0.7988\n",
       "  0.5340\n",
       "  0.7651\n",
       "  0.5198\n",
       "  0.0908\n",
       "  0.9161\n",
       "  0.3661\n",
       "  0.4596\n",
       "  0.6799\n",
       "  0.4532\n",
       "  0.9049\n",
       "  0.5802\n",
       "  0.3100\n",
       "  0.8268\n",
       "  0.2438\n",
       "  0.4262\n",
       "  0.9575\n",
       "  0.8738\n",
       "  0.9033\n",
       "  0.7605\n",
       "  0.2211\n",
       "  0.1892\n",
       "  0.3622\n",
       "  0.3433\n",
       "  0.6490\n",
       "  0.1124\n",
       "  0.2079\n",
       "  0.4041\n",
       "  0.2168\n",
       "  0.9733\n",
       "  0.5059\n",
       "  0.8045\n",
       "  0.0445\n",
       "  0.0075\n",
       "  0.1038\n",
       "  0.0069\n",
       "  0.1698\n",
       "  0.3047\n",
       "  0.6513\n",
       "  0.4109\n",
       "  0.7386\n",
       "  0.8812\n",
       "  0.0049\n",
       "  0.4906\n",
       "  0.9340\n",
       "  0.3898\n",
       "  0.0400\n",
       "  0.6824\n",
       "  0.7436\n",
       "  0.8126\n",
       "  0.2508\n",
       "  0.0003\n",
       "  0.3825\n",
       "  0.4320\n",
       "  0.5881\n",
       "  0.5037\n",
       "  0.9989\n",
       "  0.4479\n",
       "  0.7924\n",
       "  0.2347\n",
       "  0.6947\n",
       "  0.6940\n",
       "  0.9637\n",
       "  0.0988\n",
       "  0.7863\n",
       "  0.4300\n",
       "  0.6812\n",
       "  0.5541\n",
       "  0.2694\n",
       "  0.6059\n",
       "  0.6005\n",
       "  0.7454\n",
       "  0.9837\n",
       "  0.6399\n",
       "  0.5286\n",
       "  0.3952\n",
       "  0.0283\n",
       "  0.7401\n",
       "  0.4613\n",
       "  0.8679\n",
       "  0.7757\n",
       "  0.7936\n",
       " [torch.cuda.FloatTensor of size 100 (GPU 0)], Parameter containing:\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       " [torch.cuda.FloatTensor of size 100 (GPU 0)]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "0.7100884914398193\n",
      "0.7096240520477295\n",
      "0.7077504992485046\n",
      "0.7099533081054688\n",
      "0.7088508009910583\n",
      "0.7088909149169922\n",
      "0.7100736498832703\n",
      "0.7124468088150024\n",
      "0.7097237706184387\n",
      "0.7134989500045776\n",
      "0.7091678977012634\n",
      "0.7117875814437866\n",
      "0.7105998396873474\n",
      "0.7118993997573853\n",
      "0.7146552801132202\n",
      "0.7125687599182129\n",
      "0.7135084867477417\n",
      "0.7131167650222778\n",
      "0.7122045755386353\n",
      "0.7137117385864258\n",
      "0.7115121483802795\n",
      "0.711830198764801\n",
      "0.7123868465423584\n",
      "0.7136136293411255\n",
      "0.7129429578781128\n",
      "0.7120830416679382\n",
      "0.7111865282058716\n",
      "0.7111737132072449\n",
      "0.7108631730079651\n",
      "0.713833212852478\n",
      "0.7126174569129944\n",
      "0.7150212526321411\n",
      "0.7128347754478455\n",
      "0.7110909223556519\n",
      "0.7111213803291321\n",
      "0.7133054733276367\n",
      "0.7120632529258728\n",
      "0.7109995484352112\n",
      "0.7102541923522949\n",
      "0.7116317749023438\n",
      "0.7145435810089111\n",
      "0.7135326862335205\n",
      "0.7127918601036072\n",
      "0.7121589183807373\n",
      "0.7136944532394409\n",
      "0.7132519483566284\n",
      "0.7124362587928772\n",
      "0.7131631970405579\n",
      "0.7118646502494812\n",
      "0.7117373943328857\n",
      "0.7124390602111816\n",
      "0.7120519876480103\n",
      "0.7131655216217041\n",
      "0.7130022644996643\n",
      "0.7107998728752136\n",
      "0.7117942571640015\n",
      "0.712675929069519\n",
      "0.7130609154701233\n",
      "0.7120732665061951\n",
      "0.7101408243179321\n",
      "0.7096853852272034\n",
      "0.7130017876625061\n",
      "0.7128223776817322\n",
      "0.7108746767044067\n",
      "0.7150073051452637\n",
      "0.7127543687820435\n",
      "0.7116696834564209\n",
      "0.7123852968215942\n",
      "0.7127137184143066\n",
      "0.7120638489723206\n",
      "0.7128506898880005\n",
      "0.7123546600341797\n",
      "0.7093554735183716\n",
      "0.711156964302063\n",
      "0.7119637727737427\n",
      "0.7118883728981018\n",
      "0.7147783637046814\n",
      "0.7126570343971252\n",
      "0.7132420539855957\n",
      "0.712495744228363\n",
      "0.7113708853721619\n",
      "0.7110323905944824\n",
      "0.7116738557815552\n",
      "0.7117156982421875\n",
      "0.7126127481460571\n",
      "0.7109944820404053\n",
      "0.7098689675331116\n",
      "0.7104895114898682\n",
      "0.7146821022033691\n",
      "0.7111690640449524\n",
      "0.7127829790115356\n",
      "0.7108805775642395\n",
      "0.7122328877449036\n",
      "0.7127888202667236\n",
      "0.7109512090682983\n",
      "0.7131872773170471\n",
      "0.7129932045936584\n",
      "0.7127020955085754\n",
      "0.7118756175041199\n",
      "0.7128916382789612\n",
      "<class 'torch.cuda.FloatTensor'> <class 'torch.cuda.FloatTensor'>\n",
      "test loss 0.707 and accuracy 0.437\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, X_tr_enc, y_tr, X_vl_enc, y_vl,  epochs=100, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast AI notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class LanguageModelData\n",
    "    def get_model(self, opt_fn, emb_sz, n_hid, n_layers, **kwargs):\n",
    "            \"\"\" Method returns a RNN_Learner object, that wraps an instance of the RNN_Encoder module.\n",
    "\n",
    "            Args:\n",
    "                opt_fn (Optimizer): the torch optimizer function to use\n",
    "                emb_sz (int): embedding size\n",
    "                n_hid (int): number of hidden inputs\n",
    "                n_layers (int): number of hidden layers\n",
    "                kwargs: other arguments\n",
    "\n",
    "            Returns:\n",
    "                An instance of the RNN_Learner class.\n",
    "\n",
    "            \"\"\"\n",
    "            m = get_language_model(self.nt, emb_sz, n_hid, n_layers, self.pad_idx, **kwargs)\n",
    "            model = SingleModel(to_gpu(m))\n",
    "            return RNN_Learner(self, model, opt_fn=opt_fn)\n",
    "    \n",
    "class RNN_Learner(Learner):\n",
    "    def __init__(self, data, models, **kwargs):\n",
    "        super().__init__(data, models, **kwargs)\n",
    "\n",
    "    def _get_crit(self, data): return F.cross_entropy\n",
    "\n",
    "    def save_encoder(self, name): save_model(self.model[0], self.get_model_path(name))\n",
    "\n",
    "    def load_encoder(self, name): load_model(self.model[0], self.get_model_path(name))\n",
    "    \n",
    "\n",
    "def get_language_model(n_tok, emb_sz, nhid, nlayers, pad_token,\n",
    "                 dropout=0.4, dropouth=0.3, dropouti=0.5, dropoute=0.1, wdrop=0.5, tie_weights=True):\n",
    "    \"\"\"Returns a SequentialRNN model.\n",
    "\n",
    "    A RNN_Encoder layer is instantiated using the parameters provided.\n",
    "\n",
    "    This is followed by the creation of a LinearDecoder layer.\n",
    "\n",
    "    Also by default (i.e. tie_weights = True), the embedding matrix used in the RNN_Encoder\n",
    "    is used to  instantiate the weights for the LinearDecoder layer.\n",
    "\n",
    "    The SequentialRNN layer is the native torch's Sequential wrapper that puts the RNN_Encoder and\n",
    "    LinearDecoder layers sequentially in the model.\n",
    "\n",
    "    Args:\n",
    "        n_tok (int): number of unique vocabulary words (or tokens) in the source dataset\n",
    "        emb_sz (int): the embedding size to use to encode each token\n",
    "        nhid (int): number of hidden activation per LSTM layer\n",
    "        nlayers (int): number of LSTM layers to use in the architecture\n",
    "        pad_token (int): the int value used for padding text.\n",
    "        dropouth (float): dropout to apply to the activations going from one LSTM layer to another\n",
    "        dropouti (float): dropout to apply to the input layer.\n",
    "        dropoute (float): dropout to apply to the embedding layer.\n",
    "        wdrop (float): dropout used for a LSTM's internal (or hidden) recurrent weights.\n",
    "        tie_weights (bool): decide if the weights of the embedding matrix in the RNN encoder should be tied to the\n",
    "            weights of the LinearDecoder layer.\n",
    "    Returns:\n",
    "        A SequentialRNN model\n",
    "    \"\"\"\n",
    "\n",
    "    rnn_enc = RNN_Encoder(n_tok, emb_sz, nhid=nhid, nlayers=nlayers, pad_token=pad_token,\n",
    "                 dropouth=dropouth, dropouti=dropouti, dropoute=dropoute, wdrop=wdrop)\n",
    "    enc = rnn_enc.encoder if tie_weights else None\n",
    "    return SequentialRNN(rnn_enc, LinearDecoder(n_tok, emb_sz, dropout, tie_encoder=enc))\n",
    "\n",
    "\n",
    "class RNN_Encoder(nn.Module):\n",
    "\n",
    "    \"\"\"A custom RNN encoder network that uses\n",
    "        - an embedding matrix to encode input,\n",
    "        - a stack of LSTM layers to drive the network, and\n",
    "        - variational dropouts in the embedding and LSTM layers\n",
    "\n",
    "        The architecture for this network was inspired by the work done in\n",
    "        \"Regularizing and Optimizing LSTM Language Models\".\n",
    "        (https://arxiv.org/pdf/1708.02182.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, ntoken, emb_sz, nhid, nlayers, pad_token, bidir=False,\n",
    "                 dropouth=0.3, dropouti=0.65, dropoute=0.1, wdrop=0.5):\n",
    "        \"\"\" Default constructor for the RNN_Encoder class\n",
    "\n",
    "            Args:\n",
    "                bs (int): batch size of input data\n",
    "                ntoken (int): number of vocabulary (or tokens) in the source dataset\n",
    "                emb_sz (int): the embedding size to use to encode each token\n",
    "                nhid (int): number of hidden activation per LSTM layer\n",
    "                nlayers (int): number of LSTM layers to use in the architecture\n",
    "                pad_token (int): the int value used for padding text.\n",
    "                dropouth (float): dropout to apply to the activations going from one LSTM layer to another\n",
    "                dropouti (float): dropout to apply to the input layer.\n",
    "                dropoute (float): dropout to apply to the embedding layer.\n",
    "                wdrop (float): dropout used for a LSTM's internal (or hidden) recurrent weights.\n",
    "\n",
    "            Returns:\n",
    "                None\n",
    "          \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.ndir = 2 if bidir else 1\n",
    "        self.bs = 1\n",
    "        self.encoder = nn.Embedding(ntoken, emb_sz, padding_idx=pad_token)\n",
    "        self.encoder_with_dropout = EmbeddingDropout(self.encoder)\n",
    "        self.rnns = [nn.LSTM(emb_sz if l == 0 else nhid, (nhid if l != nlayers - 1 else emb_sz)//self.ndir,\n",
    "             1, bidirectional=bidir) for l in range(nlayers)]\n",
    "        if wdrop: self.rnns = [WeightDrop(rnn, wdrop) for rnn in self.rnns]\n",
    "        self.rnns = torch.nn.ModuleList(self.rnns)\n",
    "        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "\n",
    "        self.emb_sz,self.nhid,self.nlayers,self.dropoute = emb_sz,nhid,nlayers,dropoute\n",
    "        self.dropouti = LockedDropout(dropouti)\n",
    "        self.dropouths = nn.ModuleList([LockedDropout(dropouth) for l in range(nlayers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\" Invoked during the forward propagation of the RNN_Encoder module.\n",
    "        Args:\n",
    "            input (Tensor): input of shape (sentence length x batch_size)\n",
    "\n",
    "        Returns:\n",
    "            raw_outputs (tuple(list (Tensor), list(Tensor)): list of tensors evaluated from each RNN layer without using\n",
    "            dropouth, list of tensors evaluated from each RNN layer using dropouth,\n",
    "        \"\"\"\n",
    "        sl,bs = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        with set_grad_enabled(self.training):\n",
    "            emb = self.encoder_with_dropout(input, dropout=self.dropoute if self.training else 0)\n",
    "            emb = self.dropouti(emb)\n",
    "            raw_output = emb\n",
    "            new_hidden,raw_outputs,outputs = [],[],[]\n",
    "            for l, (rnn,drop) in enumerate(zip(self.rnns, self.dropouths)):\n",
    "                current_input = raw_output\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "                new_hidden.append(new_h)\n",
    "                raw_outputs.append(raw_output)\n",
    "                if l != self.nlayers - 1: raw_output = drop(raw_output)\n",
    "                outputs.append(raw_output)\n",
    "\n",
    "            self.hidden = repackage_var(new_hidden)\n",
    "        return raw_outputs, outputs\n",
    "\n",
    "    def one_hidden(self, l):\n",
    "        nh = (self.nhid if l != self.nlayers - 1 else self.emb_sz)//self.ndir\n",
    "        if IS_TORCH_04: return Variable(self.weights.new(self.ndir, self.bs, nh).zero_())\n",
    "        else: return Variable(self.weights.new(self.ndir, self.bs, nh).zero_(), volatile=not self.training)\n",
    "\n",
    "    def reset(self):\n",
    "        self.weights = next(self.parameters()).data\n",
    "        self.hidden = [(self.one_hidden(l), self.one_hidden(l)) for l in range(self.nlayers)]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "def get_rnn_classifer(bptt, max_seq, n_class, n_tok, emb_sz, n_hid, n_layers, pad_token, layers, drops, bidir=False,\n",
    "                      dropouth=0.3, dropouti=0.5, dropoute=0.1, wdrop=0.5):\n",
    "    rnn_enc = MultiBatchRNN(bptt, max_seq, n_tok, emb_sz, n_hid, n_layers, pad_token=pad_token, bidir=bidir,\n",
    "                      dropouth=dropouth, dropouti=dropouti, dropoute=dropoute, wdrop=wdrop)\n",
    "    return SequentialRNN(rnn_enc, PoolingLinearClassifier(layers, drops))\n",
    "\n",
    "\n",
    "class LinearBlock(nn.Module):\n",
    "    def __init__(self, ni, nf, drop):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(ni, nf)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.bn = nn.BatchNorm1d(ni)\n",
    "\n",
    "    def forward(self, x): return self.lin(self.drop(self.bn(x)))\n",
    "\n",
    "    \n",
    "class PoolingLinearClassifier(nn.Module):\n",
    "    def __init__(self, layers, drops):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            LinearBlock(layers[i], layers[i + 1], drops[i]) for i in range(len(layers) - 1)])\n",
    "\n",
    "    def pool(self, x, bs, is_max):\n",
    "        f = F.adaptive_max_pool1d if is_max else F.adaptive_avg_pool1d\n",
    "        return f(x.permute(1,2,0), (1,)).view(bs,-1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = outputs[-1]\n",
    "        sl,bs,_ = output.size()\n",
    "        avgpool = self.pool(output, bs, False)\n",
    "        mxpool = self.pool(output, bs, True)\n",
    "        x = torch.cat([output[-1], mxpool, avgpool], 1)\n",
    "        for l in self.layers:\n",
    "            l_x = l(x)\n",
    "            x = F.relu(l_x)\n",
    "        return l_x, raw_outputs, outputs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
