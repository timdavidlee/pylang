{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subjectivity classification with CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import _pickle as pickle \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten_imdb.tar.gz -P ~/data/rotten_imdb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ~/data/wikitext/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gunzip /home/paperspace/data/rotten_imdb/rotten_imdb.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar -xf /home/paperspace/data/rotten_imdb/rotten_imdb.tar -C /home/paperspace/data/rotten_imdb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip -P ~/data/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip ~/data/glove/glove.6B.zip -d ~/data/rotten_imdb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/paperspace/data/rotten_imdb/glove.6B.100d.txt'),\n",
       " PosixPath('/home/paperspace/data/rotten_imdb/quote.tok.gt9.5000'),\n",
       " PosixPath('/home/paperspace/data/rotten_imdb/glove.6B.50d.txt'),\n",
       " PosixPath('/home/paperspace/data/rotten_imdb/plot.tok.gt9.5000'),\n",
       " PosixPath('/home/paperspace/data/rotten_imdb/glove.6B.200d.txt'),\n",
       " PosixPath('/home/paperspace/data/rotten_imdb/rotten_imdb.tar.gz'),\n",
       " PosixPath('/home/paperspace/data/rotten_imdb/subjdata.README.1.0'),\n",
       " PosixPath('/home/paperspace/data/rotten_imdb/glove.6B.300d.txt'),\n",
       " PosixPath('/home/paperspace/data/rotten_imdb/rotten_imdb.tar')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "PATH = Path(\"/home/paperspace/data/rotten_imdb/\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample Subjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smart and alert , thirteen conversations about one thing is a small gem . \r\n",
      "color , musical bounce and warm seas lapping on island shores . and just enough science to send you home thinking . \r\n",
      "it is not a mass-market entertainment but an uncompromising attempt by one artist to think about another . \r\n",
      "a light-hearted french film about the spiritual quest of a fashion model seeking peace of mind while in a love affair with a veterinarian who is a non-practicing jew . \r\n",
      "my wife is an actress has its moments in looking at the comic effects of jealousy . in the end , though , it is only mildly amusing when it could have been so much more . \r\n"
     ]
    }
   ],
   "source": [
    "!head -5 /home/paperspace/data/rotten_imdb/quote.tok.gt9.5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the movie begins in the past where a young boy named sam attempts to save celebi from a hunter . \r\n",
      "emerging from the human psyche and showing characteristics of abstract expressionism , minimalism and russian constructivism , graffiti removal has secured its place in the history of modern art while being created by artists who are unconscious of their artistic achievements . \r\n",
      "spurning her mother's insistence that she get on with her life , mary is thrown out of the house , rejected by joe , and expelled from school as she grows larger with child . \r\n",
      "amitabh can't believe the board of directors and his mind is filled with revenge and what better revenge than robbing the bank himself , ironic as it may sound . \r\n",
      "she , among others excentricities , talks to a small rock , gertrude , like if she was alive . \r\n"
     ]
    }
   ],
   "source": [
    "!head -5 /home/paperspace/data/rotten_imdb/plot.tok.gt9.5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# Data cleaning\n",
    "# ======================================================\n",
    " \n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Every dataset is lower cased except for TREC\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def read_file(path):\n",
    "    \"\"\" Read file returns a shuttled list.\n",
    "    \"\"\"\n",
    "    with open(path, encoding = \"ISO-8859-1\") as f:\n",
    "        content = np.array(f.readlines())\n",
    "    return content\n",
    "\n",
    "\n",
    "def get_vocab(list_of_content):\n",
    "    \"\"\"Computes Dict of counts of words.\n",
    "    \n",
    "    Computes the number of times a word is on a document.\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(float)\n",
    "    for content in list_of_content:\n",
    "        for line in content:\n",
    "            line = clean_str(line.strip())\n",
    "            words = set(line.split())\n",
    "            for word in words:\n",
    "                vocab[word] += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def add_unknown_words(word_vecs, vocab, min_df=1, D=300):\n",
    "    \"\"\"\n",
    "    For words that occur in at least min_df documents, create a separate word vector.    \n",
    "    0.25 is chosen so the unknown vectors have (approximately) same variance \n",
    "    as pre-trained ones\n",
    "    \"\"\"\n",
    "    for word in vocab:\n",
    "        if word not in word_vecs and vocab[word] >= min_df:\n",
    "            word_vecs[word] = np.random.uniform(-0.25,0.25,D)\n",
    "    # here for rare words we will use UNK\n",
    "    word_vecs[\"UNK\"] = np.random.uniform(-0.25,0.25,D)\n",
    "\n",
    "    \n",
    "def create_embedding_matrix(word_vecs, D=300):\n",
    "    \"\"\"\n",
    "    Creates embedding matrix from word vectors. \n",
    "    Embedding Matrix - word vectors in numpy form stacked\n",
    "    \"\"\"\n",
    "    V = len(word_vecs.keys())\n",
    "    vocab2index = {}\n",
    "    vocab = []\n",
    "    W = np.zeros((V+1, D), dtype=\"float32\")\n",
    "    W[0] = np.zeros(D, dtype='float32')\n",
    "    i = 1\n",
    "    for word in word_vecs:\n",
    "        W[i] = word_vecs[word]\n",
    "        vocab2index[word] = i\n",
    "        vocab.append(word)\n",
    "        i += 1\n",
    "    return W, np.array(vocab), vocab2index\n",
    "\n",
    "\n",
    "def encode_sentence(s, vocab2index, N=40):\n",
    "    \"\"\"\n",
    "    takes in a sentence and replaces words with indices otherwise UNK\n",
    "    \n",
    "    encode_sentence(X_tr[0])\n",
    "    array([    44,   1534,    887,     72,    808,     47,    456,     72,\n",
    "            8,     51,   2819, 400001,      0,      0,      0,      0,\n",
    "            0,      0,      0,      0,      0,      0,      0,      0,\n",
    "            0,      0,      0,      0,      0,      0,      0,      0,\n",
    "            0,      0,      0,      0,      0,      0,      0,      0],\n",
    "      dtype=int32)\n",
    "    \n",
    "    \"\"\"\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in s.split()])\n",
    "    l = min(N, len(enc1))\n",
    "    enc[:l] = enc1[:l]\n",
    "    return enc\n",
    "\n",
    "# ======================================================\n",
    "# Data Prep XY\n",
    "# ======================================================\n",
    "\n",
    "def make_XY():\n",
    "    sub_content = read_file(PATH/\"quote.tok.gt9.5000\")\n",
    "    obj_content = read_file(PATH/\"plot.tok.gt9.5000\")\n",
    "    sub_content = np.array([clean_str(line.strip()) for line in sub_content])\n",
    "    obj_content = np.array([clean_str(line.strip()) for line in obj_content])\n",
    "    sub_y = np.zeros(len(sub_content))\n",
    "    obj_y = np.ones(len(obj_content))\n",
    "    X = np.append(sub_content, obj_content)\n",
    "    y = np.append(sub_y, obj_y)\n",
    "    return X,y\n",
    "\n",
    "\n",
    "def make_train_val(X,y):\n",
    "    X_tr, X_vl, y_tr, y_vl = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X_tr, X_vl, y_tr, y_vl\n",
    "\n",
    "# ======================================================\n",
    "# Load pretrained wordvecs\n",
    "# ======================================================\n",
    "\n",
    "def loadGloveModel(gloveFile=\"/home/paperspace/data/rotten_imdb/glove.6B.300d.txt\"):\n",
    "    \"\"\" Loads word vectors into a dictionary.\"\"\"\n",
    "    f = open(gloveFile,'r')\n",
    "    word_vecs = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        word_vecs[word] = np.array([float(val) for val in splitLine[1:]])\n",
    "    return word_vecs\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Sentence Model\n",
    "# ======================================================\n",
    "\n",
    "class SentenceCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, V, D, glove_weights):\n",
    "        \"\"\"\n",
    "        This model is based on the glove weights (naive)\n",
    "        \"\"\"\n",
    "        super(SentenceCNN, self).__init__()\n",
    "        \n",
    "        # \n",
    "        self.glove_weights = glove_weights\n",
    "        self.embedding = nn.Embedding(V, D, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(self.glove_weights))\n",
    "        self.embedding.weight.requires_grad = False ## freeze embeddings\n",
    "\n",
    "        self.conv_3 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=3)\n",
    "        self.conv_4 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=4)\n",
    "        self.conv_5 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=5)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc = nn.Linear(300, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(1,2)\n",
    "        x3 = F.relu(self.conv_3(x))\n",
    "        x4 = F.relu(self.conv_4(x))\n",
    "        x5 = F.relu(self.conv_5(x))\n",
    "        x3 = nn.MaxPool1d(kernel_size = 38)(x3)\n",
    "        x4 = nn.MaxPool1d(kernel_size = 37)(x4)\n",
    "        x5 = nn.MaxPool1d(kernel_size = 36)(x5)\n",
    "        out = torch.cat([x3, x4, x5], 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dropout(out)\n",
    "        return self.fc(out)\n",
    "    \n",
    "     \n",
    "# ======================================================\n",
    "# Training functions\n",
    "# ======================================================\n",
    "\n",
    "def train_epocs(model, x_train, y_train, x_test, y_test, epochs=10, lr=0.01):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        x = Variable(torch.LongTensor(x_train)).cuda()\n",
    "        y = Variable(torch.Tensor(y_train)).cuda().unsqueeze(1)\n",
    "        y_hat = model(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss.data[0])\n",
    "    test_metrics(model, x_test, y_test)\n",
    "\n",
    "\n",
    "def test_metrics(m, x_test, y_test):\n",
    "    model.eval()\n",
    "    x = Variable(torch.LongTensor(x_test)).cuda()\n",
    "    y = Variable(torch.Tensor(y_test)).cuda().unsqueeze(1)\n",
    "    y_hat = m(x)\n",
    "    loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "    y_pred = y_hat > 0\n",
    "    correct = (y_pred.float() == y).float().sum()\n",
    "    accuracy = correct/y_pred.shape[0]\n",
    "    print(\"test loss %.3f and accuracy %.3f\" % (loss.data[0], accuracy.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_XY()\n",
    "X_tr, X_vl, y_tr, y_vl = make_train_val(X,y)\n",
    "vocab = get_vocab([X_tr])\n",
    "\n",
    "word_vecs = loadGloveModel()\n",
    "add_unknown_words(word_vecs, vocab, min_df=10, D=300)\n",
    "pretrained_weight, vocab, vocab2index = create_embedding_matrix(word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 40)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing the embedding matrix with the word vectors\n",
    "D = 300\n",
    "V = len(pretrained_weight)\n",
    "emb = nn.Embedding(V, D)\n",
    "emb.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
    "\n",
    "# finding the longest sentence\n",
    "x_len = np.array([len(x.split()) for x in X_tr])\n",
    "print(np.percentile(x_len, 95)) # let set the max sequence len to N=40\n",
    "\n",
    "# encode all the training and validation with the correct tokens Ids\n",
    "x_tr_enc = np.vstack([encode_sentence(x, vocab2index) for x in X_tr])\n",
    "x_tr_enc.shape\n",
    "x_vl = np.vstack([encode_sentence(x, vocab2index) for x in X_vl])\n",
    "x_vl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of word vec terms\n",
    "V = len(pretrained_weight)\n",
    "\n",
    "# dimension of the embedding vector\n",
    "D = 300\n",
    "\n",
    "# \n",
    "N = 40\n",
    "model = SentenceCNN(V, D, glove_weights=pretrained_weight).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7096273899078369\n",
      "2.2384562492370605\n",
      "0.3440512418746948\n",
      "0.870844841003418\n",
      "0.6296365857124329\n",
      "0.4508425295352936\n",
      "0.42016124725341797\n",
      "0.4354369342327118\n",
      "0.4553316533565521\n",
      "0.4700770974159241\n",
      "test loss 0.473 and accuracy 0.795\n",
      "0.4713665246963501\n",
      "0.4935859441757202\n",
      "0.3727800250053406\n",
      "0.2967967092990875\n",
      "0.3181613087654114\n",
      "0.2531818151473999\n",
      "0.2877346873283386\n",
      "0.23348352313041687\n",
      "0.2424101084470749\n",
      "0.21849432587623596\n",
      "test loss 0.237 and accuracy 0.908\n",
      "0.18760180473327637\n",
      "0.19064520299434662\n",
      "0.1845017820596695\n",
      "0.17814302444458008\n",
      "0.1783904731273651\n",
      "0.17854417860507965\n",
      "0.16784361004829407\n",
      "0.16526371240615845\n",
      "0.16534225642681122\n",
      "0.162178173661232\n",
      "test loss 0.230 and accuracy 0.910\n",
      "0.16207671165466309\n",
      "0.1612558811903\n",
      "0.15554997324943542\n",
      "0.15317478775978088\n",
      "0.1477155089378357\n",
      "0.14563331007957458\n",
      "0.1436101496219635\n",
      "0.1422356516122818\n",
      "0.13875055313110352\n",
      "0.13828939199447632\n",
      "test loss 0.227 and accuracy 0.912\n",
      "0.13667140901088715\n",
      "0.13471026718616486\n",
      "0.13348564505577087\n",
      "0.13348692655563354\n",
      "0.13241475820541382\n",
      "0.13527050614356995\n",
      "0.13649065792560577\n",
      "0.1337829828262329\n",
      "0.131186380982399\n",
      "0.1330241858959198\n",
      "test loss 0.227 and accuracy 0.912\n",
      "0.13376034796237946\n",
      "0.13049305975437164\n",
      "0.13003572821617126\n",
      "0.1309463232755661\n",
      "0.13331720232963562\n",
      "0.12880109250545502\n",
      "0.1313980668783188\n",
      "0.13015106320381165\n",
      "0.12963147461414337\n",
      "0.1309942901134491\n",
      "test loss 0.227 and accuracy 0.912\n"
     ]
    }
   ],
   "source": [
    "for lr in [0.01, 0.01, 0.001,0.001, 0.0001, 0.0001]:\n",
    "    train_epocs(model, x_tr_enc, y_tr, x_vl, y_vl, epochs=10, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33279, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 40)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the encoding dictionary from the language model\n",
    "with open('dict17.pkl','rb') as f:\n",
    "    word2idx_lm, idx2word_lm = pickle.load(f)\n",
    "    \n",
    "with open('lm_emb_np_trained.pkl','rb') as f:\n",
    "    learned_emb_np = pickle.load(f)\n",
    "    \n",
    "print(learned_emb_np.shape)\n",
    "\n",
    "x_tr_enc_lm = np.vstack([encode_sentence(x, word2idx_lm) for x in X_tr])\n",
    "x_tr_enc_lm.shape\n",
    "\n",
    "x_vl_lm = np.vstack([encode_sentence(x, word2idx_lm) for x in X_vl])\n",
    "x_vl_lm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 301, 5011, 7292,  362, 6357,  311, 2194,  362,   28,  579, 9429,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr_enc_lm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'will god let her fall or give her a new path \\\\?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    44,   1534,    887,     72,    808,     47,    456,     72,\n",
       "            8,     51,   2819, 400001,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr_enc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400007, 300)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33279, 300)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_emb_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of word vec terms\n",
    "V = learned_emb_np.shape[0]\n",
    "\n",
    "# dimension of the embedding vector\n",
    "D = 300\n",
    "\n",
    "# \n",
    "N = 40\n",
    "model_lm = SentenceCNN(V, D, glove_weights=learned_emb_np).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08356434106826782\n",
      "0.13237479329109192\n",
      "0.8512473702430725\n",
      "0.09559396654367447\n",
      "0.38252323865890503\n",
      "0.480146199464798\n",
      "0.2070934921503067\n",
      "0.10103637725114822\n",
      "0.15601587295532227\n",
      "0.2621101140975952\n",
      "test loss 0.722 and accuracy 0.779\n",
      "0.25699707865715027\n",
      "0.5025117993354797\n",
      "0.18561911582946777\n",
      "0.11384200304746628\n",
      "0.2420462816953659\n",
      "0.2445473074913025\n",
      "0.1383257508277893\n",
      "0.10515493154525757\n",
      "0.13474813103675842\n",
      "0.1756541132926941\n",
      "test loss 0.522 and accuracy 0.821\n",
      "0.1668291687965393\n",
      "0.1251341998577118\n",
      "0.1057162657380104\n",
      "0.09714513272047043\n",
      "0.09732341766357422\n",
      "0.09979969263076782\n",
      "0.10536742210388184\n",
      "0.10613448917865753\n",
      "0.1069868728518486\n",
      "0.09919300675392151\n",
      "test loss 0.467 and accuracy 0.841\n",
      "0.09513245522975922\n",
      "0.08595099300146103\n",
      "0.08557414263486862\n",
      "0.08511672914028168\n",
      "0.08385423570871353\n",
      "0.07995212823152542\n",
      "0.07866086065769196\n",
      "0.0755494087934494\n",
      "0.07421944290399551\n",
      "0.07373911142349243\n",
      "test loss 0.448 and accuracy 0.843\n",
      "0.07112368196249008\n",
      "0.07097770273685455\n",
      "0.07182207703590393\n",
      "0.07174369692802429\n",
      "0.07061135023832321\n",
      "0.06919632852077484\n",
      "0.07227293401956558\n",
      "0.07019276171922684\n",
      "0.0691404938697815\n",
      "0.06997411698102951\n",
      "test loss 0.443 and accuracy 0.852\n",
      "0.06929126381874084\n",
      "0.06926960498094559\n",
      "0.06993242353200912\n",
      "0.06708008050918579\n",
      "0.07076678425073624\n",
      "0.06636309623718262\n",
      "0.06794807314872742\n",
      "0.06794733554124832\n",
      "0.06936639547348022\n",
      "0.06736163049936295\n",
      "test loss 0.446 and accuracy 0.847\n"
     ]
    }
   ],
   "source": [
    "for lr in [0.01, 0.01, 0.001,0.001, 0.0001, 0.0001]:\n",
    "    train_epocs(model_lm, x_tr_enc_lm, y_tr, x_vl_lm, y_vl, epochs=10, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
